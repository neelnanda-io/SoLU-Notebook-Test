<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>SoLU-Notebook-Test â€“ solu_transformer_v3-(1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">SoLU-Notebook-Test</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/neelnanda-io/SoLU-Notebook-Test"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">(Untitled)</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">SoLU-Notebook-Test</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./core.html" class="sidebar-item-text sidebar-link">core</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./SoLU_Transformer_v3 (1).html" class="sidebar-item-text sidebar-link active">SoLU_Transformer_v3 (1).html</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<div class="cell" data-outputid="b2b61efd-53b1-4355-be71-ab11ebce3fec" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>nvidia<span class="op">-</span>smi</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tue Sep 13 20:44:27 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A6000    On   | 00000000:04:00.0 Off |                  Off |
| 30%   44C    P5    78W / 300W |      5MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A6000    On   | 00000000:06:00.0 Off |                  Off |
| 30%   45C    P3    87W / 300W |      5MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A6000    On   | 00000000:07:00.0 Off |                  Off |
| 30%   40C    P5    76W / 300W |      5MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A6000    On   | 00000000:08:00.0 Off |                  Off |
| 30%   45C    P5    95W / 300W |      5MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA RTX A6000    On   | 00000000:0C:00.0 Off |                  Off |
| 30%   45C    P5    57W / 300W |      5MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA RTX A6000    On   | 00000000:0D:00.0 Off |                  Off |
| 30%   41C    P5    74W / 300W |      5MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA RTX A6000    On   | 00000000:0E:00.0 Off |                  Off |
| 30%   48C    P5    72W / 300W |      5MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA RTX A6000    On   | 00000000:0F:00.0 Off |                  Off |
| 30%   46C    P5    65W / 300W |      5MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2918      G                                       4MiB |
|    1   N/A  N/A      2918      G                                       4MiB |
|    2   N/A  N/A      2918      G                                       4MiB |
|    3   N/A  N/A      2918      G                                       4MiB |
|    4   N/A  N/A      2918      G                                       4MiB |
|    5   N/A  N/A      2918      G                                       4MiB |
|    6   N/A  N/A      2918      G                                       4MiB |
|    7   N/A  N/A      2918      G                                       4MiB |
+-----------------------------------------------------------------------------+</code></pre>
</div>
</div>
<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # !pip install wandb</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># # !apt-get update</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># # !su -</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># # !apt-get install sudo -y</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># # !apt-get install tmux -y</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install einops</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install pyyaml==5.4.1</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install transformers</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install datasets</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install matplotlib</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install plotly</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install zstandard</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># # import wandb</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># # wandb.login()</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># # # !wandb login</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import stuff</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> einops</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tqdm.notebook <span class="im">as</span> tqdm</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># from google.colab import drive</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.io <span class="im">as</span> pio</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>pio.renderers.default <span class="op">=</span> <span class="st">"colab"</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> <span class="op">*</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gc</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> collections</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># import comet_ml</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoConfig, AutoTokenizer</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datasets</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> wandb</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="a9c06e00-6e77-450e-89af-3b55c2443cdd" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>cfg <span class="op">=</span> {</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'d_model'</span>:<span class="dv">1280</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_layers'</span>:<span class="dv">10</span>,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'lr'</span>:<span class="fl">5e-4</span>,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'batch_size'</span>:<span class="dv">13</span> <span class="op">*</span> torch.cuda.device_count(),</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'batches_per_step'</span>:<span class="dv">3</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'seed'</span>:<span class="dv">14916</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 'checkpoint_every_tokens':5*10**7,</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_checkpoint_schedule'</span>:<span class="va">True</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'debug'</span>:<span class="va">False</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'debug_batch'</span>:<span class="va">False</span>,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'debug_overfit'</span>:<span class="va">False</span>,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'normalization'</span>:<span class="st">'LN'</span>, <span class="co"># 'LN' 'RMS' or None</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_tokens'</span>:<span class="dv">15</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">9</span>,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'version'</span>:<span class="dv">22</span>,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_bfloat16'</span>:<span class="va">False</span>,</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'save_checkpoints_to_bfloat16'</span>:<span class="va">True</span>,</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_bfloat16_matmul'</span>:<span class="va">True</span>,</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'right_multiply_matrices'</span>:<span class="va">True</span>,</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 'n_heads':8,</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'d_head'</span>:<span class="dv">64</span>,</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_ctx'</span>:<span class="dv">1024</span>,</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'d_vocab'</span>:<span class="dv">50278</span>,</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 'factor_size':256,</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'betas'</span>:(<span class="fl">0.9</span>, <span class="fl">0.99</span>),</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weight_decay'</span>:<span class="fl">0.01</span>,</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">'dataset_name'</span>:<span class="st">'the_pile'</span>,</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">'grad_norm_clip'</span>:<span class="fl">1.0</span>,</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_attn_result'</span>:<span class="va">False</span>,</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_devices'</span>:torch.cuda.device_count(),</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'act_fn'</span>:<span class="st">'SoLU'</span>,</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">'use_pos_resid'</span>:<span class="va">True</span>,</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">'attn_only'</span>:<span class="va">False</span>,</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ln_eps'</span>:<span class="fl">1e-5</span>,</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">'lr_schedule'</span>: <span class="st">'cosine_warmup'</span>,</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">'warmup_tokens'</span>:<span class="dv">25</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">7</span>,</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">'factored_embed'</span>:<span class="va">False</span>,</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'train_loss_ewma_beta'</span>:<span class="fl">0.99</span>,</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'shuffled_data'</span>:<span class="va">True</span>,</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 'W_O_init_scale':True,</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Old'</span>)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>pprint(cfg)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>cfg[<span class="st">'n_heads'</span>] <span class="op">=</span> cfg[<span class="st">'d_model'</span>]<span class="op">//</span>cfg[<span class="st">'d_head'</span>]</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>cfg[<span class="st">'d_mlp'</span>] <span class="op">=</span> <span class="dv">4</span> <span class="op">*</span> cfg[<span class="st">'d_model'</span>]</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>cfg[<span class="st">'tokens_per_step'</span>] <span class="op">=</span> (cfg[<span class="st">'batch_size'</span>]<span class="op">*</span>cfg[<span class="st">'n_ctx'</span>]<span class="op">*</span>cfg[<span class="st">'batches_per_step'</span>])</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>cfg[<span class="st">'max_steps'</span>] <span class="op">=</span> cfg[<span class="st">'max_tokens'</span>]<span class="op">//</span>cfg[<span class="st">'tokens_per_step'</span>]</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>cfg[<span class="st">'warmup_steps'</span>] <span class="op">=</span> cfg[<span class="st">'warmup_tokens'</span>]<span class="op">//</span>cfg[<span class="st">'tokens_per_step'</span>]</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a><span class="co"># cfg['checkpoint_every'] = cfg['checkpoint_every_tokens']//cfg['tokens_per_step']</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> cfg[<span class="st">'debug'</span>] <span class="kw">and</span> <span class="kw">not</span> cfg[<span class="st">'debug_overfit'</span>]:</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Old max steps:'</span>, cfg[<span class="st">'max_steps'</span>])</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    cfg[<span class="st">'max_steps'</span>]<span class="op">=</span><span class="dv">20</span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a><span class="co"># cfg['warmup_steps']=cfg['warmup_tokens']//cfg['tokens_per_step']</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>pprint(cfg)</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(cfg[<span class="st">'seed'</span>])</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>np.random.seed(cfg[<span class="st">'seed'</span>])</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>random.seed(cfg[<span class="st">'seed'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Old
{'act_fn': 'SoLU',
 'attn_only': False,
 'batch_size': 104,
 'batches_per_step': 3,
 'betas': (0.9, 0.99),
 'd_head': 64,
 'd_model': 1280,
 'd_vocab': 50278,
 'dataset_name': 'the_pile',
 'debug': False,
 'debug_batch': False,
 'debug_overfit': False,
 'factored_embed': False,
 'grad_norm_clip': 1.0,
 'ln_eps': 1e-05,
 'lr': 0.0005,
 'lr_schedule': 'cosine_warmup',
 'max_tokens': 15000000000,
 'n_ctx': 1024,
 'n_devices': 8,
 'n_layers': 10,
 'normalization': 'LN',
 'right_multiply_matrices': True,
 'save_checkpoints_to_bfloat16': True,
 'seed': 14916,
 'shuffled_data': True,
 'train_loss_ewma_beta': 0.99,
 'use_attn_result': False,
 'use_bfloat16': False,
 'use_bfloat16_matmul': True,
 'use_checkpoint_schedule': True,
 'use_pos_resid': True,
 'version': 22,
 'warmup_tokens': 250000000,
 'weight_decay': 0.01}

{'act_fn': 'SoLU',
 'attn_only': False,
 'batch_size': 104,
 'batches_per_step': 3,
 'betas': (0.9, 0.99),
 'd_head': 64,
 'd_mlp': 5120,
 'd_model': 1280,
 'd_vocab': 50278,
 'dataset_name': 'the_pile',
 'debug': False,
 'debug_batch': False,
 'debug_overfit': False,
 'factored_embed': False,
 'grad_norm_clip': 1.0,
 'ln_eps': 1e-05,
 'lr': 0.0005,
 'lr_schedule': 'cosine_warmup',
 'max_steps': 46950,
 'max_tokens': 15000000000,
 'n_ctx': 1024,
 'n_devices': 8,
 'n_heads': 20,
 'n_layers': 10,
 'normalization': 'LN',
 'right_multiply_matrices': True,
 'save_checkpoints_to_bfloat16': True,
 'seed': 14916,
 'shuffled_data': True,
 'tokens_per_step': 319488,
 'train_loss_ewma_beta': 0.99,
 'use_attn_result': False,
 'use_bfloat16': False,
 'use_bfloat16_matmul': True,
 'use_checkpoint_schedule': True,
 'use_pos_resid': True,
 'version': 22,
 'warmup_steps': 782,
 'warmup_tokens': 250000000,
 'weight_decay': 0.01}</code></pre>
</div>
</div>
<div class="cell" data-outputid="15a8f5ae-b759-48b6-c46a-f8d6d644f9dd" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Num params: </span><span class="sc">{</span><span class="dv">12</span><span class="op">*</span>cfg[<span class="st">'n_layers'</span>]<span class="op">*</span>cfg[<span class="st">'d_model'</span>]<span class="op">**</span><span class="dv">2</span><span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Num params: 196,608,000</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cuda_memory():</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>([torch.cuda.memory_allocated(<span class="ss">f"cuda:</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)<span class="op">/</span><span class="fl">1e9</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(torch.cuda.device_count())])</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_corner(tensor, n<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prints the top left corner of the tensor</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(tensor.shape)<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tensor</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">len</span>(tensor.shape)<span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tensor[:n]</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">len</span>(tensor.shape)<span class="op">==</span><span class="dv">2</span>:</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tensor[:n, :n]</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">len</span>(tensor.shape)<span class="op">==</span><span class="dv">3</span>:</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tensor[:n, :n, :n]</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">len</span>(tensor.shape)<span class="op">==</span><span class="dv">4</span>:</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tensor[:n, :n, :n, :n]</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">len</span>(tensor.shape)<span class="op">==</span><span class="dv">5</span>:</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tensor[:n, :n, :n, :n, :n]</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">len</span>(tensor.shape)<span class="op">==</span><span class="dv">6</span>:</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tensor[:n, :n, :n, :n, :n, :n]</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># I never need tensors of rank &gt; 6</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f'Tensor of shape </span><span class="sc">{</span>tensor<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> is too big'</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_numpy(tensor, flat<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="bu">type</span>(tensor)<span class="op">!=</span>torch.Tensor) <span class="kw">and</span> (<span class="bu">type</span>(tensor)<span class="op">!=</span>torch.nn.parameter.Parameter):</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tensor</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> flat:</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tensor.flatten().detach().cpu().numpy()</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tensor.detach().cpu().numpy()</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_to_bfloat16(model, file_name):</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    sd <span class="op">=</span> model.state_dict()</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    torch.save({k:v.to(torch.bfloat16) <span class="cf">for</span> k, v <span class="kw">in</span> sd.items()}, file_name)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Saved model as bfloat16 to"</span>, file_name)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="co"># save_to_bfloat16(model, 'SoLU_3L_testing.pth')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A helper class to get access to intermediate activations (inspired by Garcon)</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># It's a dummy module that is the identity function by default</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># I can wrap any intermediate activation in a HookPoint and get a convenient </span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># way to add PyTorch hooks</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HookPoint(nn.Module):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fwd_hooks <span class="op">=</span> []</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bwd_hooks <span class="op">=</span> []</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ctx <span class="op">=</span> {}</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A variable giving the hook's name (from the perspective of the root </span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># module) - this is set by the root module at setup.</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.name <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add_hook(<span class="va">self</span>, hook, <span class="bu">dir</span><span class="op">=</span><span class="st">'fwd'</span>):</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hook format is fn(activation, hook_name)</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Change it into PyTorch hook format (this includes input and output, </span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># which are the same for a HookPoint)</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> full_hook(module, module_input, module_output):</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> hook(module_output, hook<span class="op">=</span><span class="va">self</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">dir</span><span class="op">==</span><span class="st">'fwd'</span>:</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>            handle <span class="op">=</span> <span class="va">self</span>.register_forward_hook(full_hook)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.fwd_hooks.append(handle)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">dir</span><span class="op">==</span><span class="st">'bwd'</span>:</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>            handle <span class="op">=</span> <span class="va">self</span>.register_full_backward_hook(full_hook)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.bwd_hooks.append(handle)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Invalid direction </span><span class="sc">{</span><span class="bu">dir</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remove_hooks(<span class="va">self</span>, <span class="bu">dir</span><span class="op">=</span><span class="st">'fwd'</span>):</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (<span class="bu">dir</span><span class="op">==</span><span class="st">'fwd'</span>) <span class="kw">or</span> (<span class="bu">dir</span><span class="op">==</span><span class="st">'both'</span>):</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.fwd_hooks:</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>                hook.remove()</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.fwd_hooks <span class="op">=</span> []</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (<span class="bu">dir</span><span class="op">==</span><span class="st">'bwd'</span>) <span class="kw">or</span> (<span class="bu">dir</span><span class="op">==</span><span class="st">'both'</span>):</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.bwd_hooks:</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>                hook.remove()</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.bwd_hooks <span class="op">=</span> []</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">dir</span> <span class="kw">not</span> <span class="kw">in</span> [<span class="st">'fwd'</span>, <span class="st">'bwd'</span>, <span class="st">'both'</span>]:</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Invalid direction </span><span class="sc">{</span><span class="bu">dir</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> clear_context(<span class="va">self</span>):</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>        <span class="kw">del</span> <span class="va">self</span>.ctx</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ctx <span class="op">=</span> {}</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> layer(<span class="va">self</span>):</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns the layer index if the name has the form 'blocks.{layer}.{...}'</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Helper function that's mainly useful on EasyTransformer</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If it doesn't have this form, raises an error - </span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>        split_name <span class="op">=</span> <span class="va">self</span>.name.split(<span class="st">'.'</span>)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">int</span>(split_name[<span class="dv">1</span>])</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HookedRootModule(nn.Module):</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># A class building on nn.Module to interface nicely with HookPoints</span></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Allows you to name each hook, remove hooks, cache every activation/gradient, etc</span></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args):</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> setup_hooks(<span class="va">self</span>):</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Setup function - this needs to be run in __init__ AFTER defining all </span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># layers</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add a parameter to each module giving its name</span></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Build a dictionary mapping a module name to the module</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mod_dict <span class="op">=</span> {}</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_dict <span class="op">=</span> {}</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.named_modules():</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>            module.name <span class="op">=</span> name</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mod_dict[name] <span class="op">=</span> module</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">type</span>(module)<span class="op">==</span>HookPoint:</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hook_dict[name] <span class="op">=</span> module</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hook_points(<span class="va">self</span>):</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="va">self</span>.hook_dict.values())</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remove_all_hook_fns(<span class="va">self</span>, direction<span class="op">=</span><span class="st">'both'</span>):</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hp <span class="kw">in</span> <span class="va">self</span>.hook_points():</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>            hp.remove_hooks(direction)</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> clear_contexts(<span class="va">self</span>):</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hp <span class="kw">in</span> <span class="va">self</span>.hook_points():</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>            hp.clear_context()</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset_hooks(<span class="va">self</span>, clear_contexts<span class="op">=</span><span class="va">True</span>, direction<span class="op">=</span><span class="st">'both'</span>):</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> clear_contexts: <span class="va">self</span>.clear_contexts()</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.remove_all_hook_fns(direction)</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> cache_all(<span class="va">self</span>, cache, incl_bwd<span class="op">=</span><span class="va">False</span>, device<span class="op">=</span><span class="st">'cuda'</span>):</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Caches all activations wrapped in a HookPoint</span></span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> save_hook(tensor, hook):</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>            cache[hook.name] <span class="op">=</span> tensor.detach().to(device)</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> save_hook_back(tensor, hook):</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>            cache[hook.name<span class="op">+</span><span class="st">'_grad'</span>] <span class="op">=</span> tensor[<span class="dv">0</span>].detach().to(device)</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hp <span class="kw">in</span> <span class="va">self</span>.hook_points():</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>            hp.add_hook(save_hook, <span class="st">'fwd'</span>)</span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> incl_bwd:</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a>                hp.add_hook(save_hook_back, <span class="st">'bwd'</span>)</span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run_with_hooks(<span class="va">self</span>, </span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a>                       <span class="op">*</span>args, </span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a>                       fwd_hooks<span class="op">=</span>[], </span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>                       bwd_hooks<span class="op">=</span>[], </span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a>                       reset_hooks_start<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a>                       reset_hooks_end<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a>                       clear_contexts<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a><span class="co">        fwd_hooks: A list of (name, hook), where name is either the name of </span></span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a><span class="co">        a hook point or a Boolean function on hook names and hook is the </span></span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a><span class="co">        function to add to that hook point, or the hook whose names evaluate </span></span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a><span class="co">        to True respectively. Ditto bwd_hooks</span></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a><span class="co">        reset_hooks_start (bool): If True, all prior hooks are removed at the start</span></span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a><span class="co">        reset_hooks_end (bool): If True, all hooks are removed at the end (ie, </span></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a><span class="co">        including those added in this run)</span></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a><span class="co">        clear_contexts (bool): If True, clears hook contexts whenever hooks are reset</span></span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a><span class="co">        Note that if we want to use backward hooks, we need to set </span></span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a><span class="co">        reset_hooks_end to be False, so the backward hooks are still there - this function only runs a forward pass.</span></span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> reset_hooks_start:</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.reset_hooks(clear_contexts)</span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, hook <span class="kw">in</span> fwd_hooks:</span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">type</span>(name)<span class="op">==</span><span class="bu">str</span>:</span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.mod_dict[name].add_hook(hook, <span class="bu">dir</span><span class="op">=</span><span class="st">'fwd'</span>)</span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Otherwise, name is a Boolean function on names</span></span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> hook_name, hp <span class="kw">in</span> <span class="va">self</span>.hook_dict.items():</span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> name(hook_name):</span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a>                        hp.add_hook(hook, <span class="bu">dir</span><span class="op">=</span><span class="st">'fwd'</span>)</span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, hook <span class="kw">in</span> bwd_hooks:</span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">type</span>(name)<span class="op">==</span><span class="bu">str</span>:</span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.mod_dict[name].add_hook(hook, <span class="bu">dir</span><span class="op">=</span><span class="st">'fwd'</span>)</span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Otherwise, name is a Boolean function on names</span></span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> hook_name, hp <span class="kw">in</span> <span class="va">self</span>.hook_dict:</span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> name(hook_name):</span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a>                        hp.add_hook(hook, <span class="bu">dir</span><span class="op">=</span><span class="st">'bwd'</span>)</span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.forward(<span class="op">*</span>args)</span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> reset_hooks_end:</span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(bwd_hooks)<span class="op">&gt;</span><span class="dv">0</span>:</span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">"WARNING: Hooks were reset at the end of run_with_hooks while backward hooks were set."</span>)</span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">"This removes the backward hooks before a backward pass can occur"</span>)</span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.reset_hooks(clear_contexts)</span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss_fn(logits, batch):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    log_probs <span class="op">=</span> F.log_softmax(logits[:, :<span class="op">-</span><span class="dv">1</span>], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    pred_log_probs <span class="op">=</span> torch.gather(log_probs, <span class="op">-</span><span class="dv">1</span>, batch[:, <span class="dv">1</span>:, <span class="va">None</span>])[..., <span class="dv">0</span>]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>pred_log_probs.mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> amp_einsum(einsum_str, mat1, mat2):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return torch.einsum(einsum_str, mat1, mat2)</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return torch.einsum(einsum_str, mat1.to(torch.bfloat16), mat2.to(torch.bfloat16)).to(torch.float32)</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cfg[<span class="st">'use_bfloat16_matmul'</span>]:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.einsum(einsum_str, mat1.to(torch.bfloat16), mat2.to(torch.bfloat16)).to(torch.float32)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.einsum(einsum_str, mat1, mat2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define network architecture</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Embed &amp; Unembed</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Embed(nn.Module):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_E <span class="op">=</span> nn.Parameter(torch.empty(<span class="va">self</span>.cfg[<span class="st">'d_vocab'</span>], <span class="va">self</span>.cfg[<span class="st">'d_model'</span>]))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.W_E, a<span class="op">=</span>np.sqrt(<span class="dv">5</span>), mode<span class="op">=</span><span class="st">'fan_out'</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, tokens):</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B acts as a tensor of indices into the second dimension (so &gt;=0 and &lt;b)</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return einops.rearrange(self.W_E[tokens, :], 'd_model batch pos -&gt; batch pos d_model')</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.W_E[tokens, :]</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co"># class FactoredEmbed(nn.Module):</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg):</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_E = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_vocab']))</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_E_factor = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['factor_size']))</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5), mode='fan_out')</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_E_factor, a=np.sqrt(5), mode='fan_out')</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, tokens):</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co">#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="co">#         # B acts as a tensor of indices into the second dimension (so &gt;=0 and &lt;b)</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="co">#         return einops.rearrange(self.W_E[:, tokens], 'factor batch pos -&gt; batch pos factor') @ self.W_E_factor.T</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Unembed(nn.Module):</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg):</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_U <span class="op">=</span> nn.Parameter(torch.empty(<span class="va">self</span>.cfg[<span class="st">'d_model'</span>], <span class="va">self</span>.cfg[<span class="st">'d_vocab'</span>]))</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.W_U, a<span class="op">=</span>np.sqrt(<span class="dv">5</span>), mode<span class="op">=</span><span class="st">'fan_out'</span>)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, residual):</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> amp_einsum(<span class="st">'bpm,mv-&gt;bpv'</span>, residual, <span class="va">self</span>.W_U) <span class="co"># [batch, pos, d_vocab]</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="co"># class FactoredUnembed(nn.Module):</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg):</span></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_U = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['factor_size']))</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_U_factor = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_model']))</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5), mode='fan_out')</span></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_U_factor, a=np.sqrt(5), mode='fan_out')</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, residual):</span></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="co">#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]</span></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="co">#         # B acts as a tensor of indices into the second dimension (so &gt;=0 and &lt;b)</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a><span class="co">#         return amp_einsum('fm,vf,bpm-&gt;bpv', self.W_U_factor, self.W_U, residual) # [batch, pos, d_vocab]</span></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Positional Embeddings</span></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PosEmbed(nn.Module):</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg):</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_pos <span class="op">=</span> nn.Parameter(torch.empty(<span class="va">self</span>.cfg[<span class="st">'n_ctx'</span>], <span class="va">self</span>.cfg[<span class="st">'d_model'</span>])) </span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.W_pos, a<span class="op">=</span>np.sqrt(<span class="dv">5</span>), mode<span class="op">=</span><span class="st">'fan_out'</span>)</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output shape [pos, d_model] - will be broadcast along batch dim</span></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.W_pos[:x.size(<span class="op">-</span><span class="dv">1</span>), :] <span class="co"># [pos, d_model]</span></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNormPre(nn.Module):</span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg):</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> <span class="va">self</span>.cfg[<span class="st">'ln_eps'</span>]</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adds a hook point for the normalization scale factor</span></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_scale <span class="op">=</span> HookPoint() <span class="co"># [batch, pos]</span></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">-</span> x.mean(axis<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>        scale <span class="op">=</span> <span class="va">self</span>.hook_scale((x.<span class="bu">pow</span>(<span class="dv">2</span>).mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> </span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>                                 <span class="va">self</span>.eps).sqrt()) <span class="co"># [batch, pos, 1]</span></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">/</span> scale</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg, length):</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> <span class="va">self</span>.cfg[<span class="st">'ln_eps'</span>]</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.length <span class="op">=</span> length</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> nn.Parameter(torch.ones(length))</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> nn.Parameter(torch.zeros(length))</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adds a hook point for the normalization scale factor</span></span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_scale <span class="op">=</span> HookPoint() <span class="co"># [batch, pos]</span></span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">-</span> x.mean(axis<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>        scale <span class="op">=</span> <span class="va">self</span>.hook_scale((x.<span class="bu">pow</span>(<span class="dv">2</span>).mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> </span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a>                                 <span class="va">self</span>.eps).sqrt()) <span class="co"># [batch, pos, 1]</span></span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> (x <span class="op">/</span> scale) <span class="op">*</span> <span class="va">self</span>.w <span class="op">+</span> <span class="va">self</span>.b</span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RMSNorm(nn.Module):</span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg, length):</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> <span class="va">self</span>.cfg[<span class="st">'ln_eps'</span>]</span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.length <span class="op">=</span> length</span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> nn.Parameter(torch.ones(length))</span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adds a hook point for the normalization scale factor</span></span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_scale <span class="op">=</span> HookPoint() <span class="co"># [batch, pos]</span></span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a>        scale <span class="op">=</span> <span class="va">self</span>.hook_scale((x.<span class="bu">pow</span>(<span class="dv">2</span>).mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> </span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a>                                 <span class="va">self</span>.eps).sqrt()) <span class="co"># [batch, pos, 1]</span></span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> (x <span class="op">/</span> scale) <span class="op">*</span> <span class="va">self</span>.w</span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a><span class="co"># Attention</span></span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Attention(nn.Module):</span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg, attn_type<span class="op">=</span><span class="st">'global'</span>):</span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_Q <span class="op">=</span> nn.Parameter(torch.empty(<span class="va">self</span>.cfg[<span class="st">'n_heads'</span>], <span class="va">self</span>.cfg[<span class="st">'d_model'</span>], <span class="va">self</span>.cfg[<span class="st">'d_head'</span>]))</span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_Q <span class="op">=</span> nn.Parameter(torch.zeros(<span class="va">self</span>.cfg[<span class="st">'n_heads'</span>], <span class="va">self</span>.cfg[<span class="st">'d_head'</span>]))</span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.W_Q, a<span class="op">=</span>np.sqrt(<span class="dv">5</span>), mode<span class="op">=</span><span class="st">'fan_out'</span>)</span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_K <span class="op">=</span> nn.Parameter(torch.empty(<span class="va">self</span>.cfg[<span class="st">'n_heads'</span>], <span class="va">self</span>.cfg[<span class="st">'d_model'</span>], <span class="va">self</span>.cfg[<span class="st">'d_head'</span>]))</span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_K <span class="op">=</span> nn.Parameter(torch.zeros(<span class="va">self</span>.cfg[<span class="st">'n_heads'</span>], <span class="va">self</span>.cfg[<span class="st">'d_head'</span>]))</span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.W_K, a<span class="op">=</span>np.sqrt(<span class="dv">5</span>), mode<span class="op">=</span><span class="st">'fan_out'</span>)</span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_V <span class="op">=</span> nn.Parameter(torch.empty(<span class="va">self</span>.cfg[<span class="st">'n_heads'</span>], <span class="va">self</span>.cfg[<span class="st">'d_model'</span>], <span class="va">self</span>.cfg[<span class="st">'d_head'</span>]))</span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_V <span class="op">=</span> nn.Parameter(torch.zeros(<span class="va">self</span>.cfg[<span class="st">'n_heads'</span>], <span class="va">self</span>.cfg[<span class="st">'d_head'</span>]))</span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.W_V, a<span class="op">=</span>np.sqrt(<span class="dv">5</span>), mode<span class="op">=</span><span class="st">'fan_out'</span>)</span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_O <span class="op">=</span> nn.Parameter(torch.empty(<span class="va">self</span>.cfg[<span class="st">'n_heads'</span>], <span class="va">self</span>.cfg[<span class="st">'d_head'</span>], <span class="va">self</span>.cfg[<span class="st">'d_model'</span>]))</span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_O <span class="op">=</span> nn.Parameter(torch.zeros(<span class="va">self</span>.cfg[<span class="st">'d_model'</span>]))</span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.W_O, a<span class="op">=</span>np.sqrt(<span class="dv">5</span>), mode<span class="op">=</span><span class="st">'fan_out'</span>)</span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if cfg['W_O_init_scale']:</span></span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     self.W_O/=np.sqrt(2*self.cfg['n_layers'])</span></span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-139"><a href="#cb13-139" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_type <span class="op">=</span> attn_type</span>
<span id="cb13-140"><a href="#cb13-140" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a query_pos x key_pos mask, with True iff that query position </span></span>
<span id="cb13-141"><a href="#cb13-141" aria-hidden="true" tabindex="-1"></a>        <span class="co"># can attend to that key position</span></span>
<span id="cb13-142"><a href="#cb13-142" aria-hidden="true" tabindex="-1"></a>        causal_mask <span class="op">=</span> torch.tril(torch.ones((<span class="va">self</span>.cfg[<span class="st">'n_ctx'</span>], <span class="va">self</span>.cfg[<span class="st">'n_ctx'</span>])).<span class="bu">bool</span>())</span>
<span id="cb13-143"><a href="#cb13-143" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'mask'</span>, causal_mask)</span>
<span id="cb13-144"><a href="#cb13-144" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-145"><a href="#cb13-145" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'IGNORE'</span>, torch.tensor(<span class="op">-</span><span class="fl">1e5</span>))</span>
<span id="cb13-146"><a href="#cb13-146" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_scale <span class="op">=</span> np.sqrt(<span class="va">self</span>.cfg[<span class="st">'d_head'</span>])</span>
<span id="cb13-147"><a href="#cb13-147" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-148"><a href="#cb13-148" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_k <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, head_index, d_head]</span></span>
<span id="cb13-149"><a href="#cb13-149" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_q <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, head_index, d_head]</span></span>
<span id="cb13-150"><a href="#cb13-150" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_v <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, head_index, d_head]</span></span>
<span id="cb13-151"><a href="#cb13-151" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_z <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, head_index, d_head]</span></span>
<span id="cb13-152"><a href="#cb13-152" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_attn_scores <span class="op">=</span> HookPoint() <span class="co"># [batch, head_index, query_pos, key_pos]</span></span>
<span id="cb13-153"><a href="#cb13-153" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_attn <span class="op">=</span> HookPoint() <span class="co"># [batch, head_index, query_pos, key_pos]</span></span>
<span id="cb13-154"><a href="#cb13-154" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_result <span class="op">=</span> HookPoint() <span class="co"># [batch, head_index, head_index, d_model]</span></span>
<span id="cb13-155"><a href="#cb13-155" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> cfg[<span class="st">'use_pos_resid'</span>]:</span>
<span id="cb13-156"><a href="#cb13-156" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hook_attn_input <span class="op">=</span> HookPoint()</span>
<span id="cb13-157"><a href="#cb13-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-158"><a href="#cb13-158" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, pos_embed):</span>
<span id="cb13-159"><a href="#cb13-159" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> cfg[<span class="st">'use_pos_resid'</span>]:</span>
<span id="cb13-160"><a href="#cb13-160" aria-hidden="true" tabindex="-1"></a>            attn_input <span class="op">=</span> <span class="va">self</span>.hook_attn_input(x<span class="op">+</span>pos_embed)</span>
<span id="cb13-161"><a href="#cb13-161" aria-hidden="true" tabindex="-1"></a>            q <span class="op">=</span> <span class="va">self</span>.hook_q(amp_einsum(<span class="st">'bpm,imh-&gt;bpih'</span>, attn_input, <span class="va">self</span>.W_Q)<span class="op">+</span><span class="va">self</span>.b_Q) <span class="co"># [batch, pos, head_index, d_head]</span></span>
<span id="cb13-162"><a href="#cb13-162" aria-hidden="true" tabindex="-1"></a>            k <span class="op">=</span> <span class="va">self</span>.hook_k(amp_einsum(<span class="st">'bpm,imh-&gt;bpih'</span>, attn_input, <span class="va">self</span>.W_K)<span class="op">+</span><span class="va">self</span>.b_K) <span class="co"># [batch, pos, head_index, d_head]</span></span>
<span id="cb13-163"><a href="#cb13-163" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-164"><a href="#cb13-164" aria-hidden="true" tabindex="-1"></a>            q <span class="op">=</span> <span class="va">self</span>.hook_q(amp_einsum(<span class="st">'bpm,imh-&gt;bpih'</span>, x, <span class="va">self</span>.W_Q)<span class="op">+</span><span class="va">self</span>.b_Q) <span class="co"># [batch, pos, head_index, d_head]</span></span>
<span id="cb13-165"><a href="#cb13-165" aria-hidden="true" tabindex="-1"></a>            k <span class="op">=</span> <span class="va">self</span>.hook_k(amp_einsum(<span class="st">'bpm,imh-&gt;bpih'</span>, x, <span class="va">self</span>.W_K)<span class="op">+</span><span class="va">self</span>.b_K) <span class="co"># [batch, pos, head_index, d_head]</span></span>
<span id="cb13-166"><a href="#cb13-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-167"><a href="#cb13-167" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.hook_v(amp_einsum(<span class="st">'bpm,imh-&gt;bpih'</span>, x, <span class="va">self</span>.W_V)<span class="op">+</span><span class="va">self</span>.b_V) <span class="co"># [batch, pos, head_index, d_head]</span></span>
<span id="cb13-168"><a href="#cb13-168" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> amp_einsum(<span class="st">'bpih,bqih-&gt;bipq'</span>, q, k)<span class="op">/</span><span class="va">self</span>.attn_scale <span class="co"># [batch, head_index, query_pos, key_pos]</span></span>
<span id="cb13-169"><a href="#cb13-169" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> <span class="va">self</span>.hook_attn_scores(<span class="va">self</span>.apply_causal_mask(attn_scores)) <span class="co"># [batch, head_index, query_pos, key_pos]</span></span>
<span id="cb13-170"><a href="#cb13-170" aria-hidden="true" tabindex="-1"></a>        attn_matrix <span class="op">=</span> <span class="va">self</span>.hook_attn(F.softmax(attn_scores, dim<span class="op">=-</span><span class="dv">1</span>)) <span class="co"># [batch, head_index, query_pos, key_pos]</span></span>
<span id="cb13-171"><a href="#cb13-171" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.hook_z(amp_einsum(<span class="st">'bpih,biqp-&gt;bqih'</span>, v, attn_matrix)) <span class="co"># [batch, pos, head_index, d_head]</span></span>
<span id="cb13-172"><a href="#cb13-172" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-173"><a href="#cb13-173" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cfg[<span class="st">'use_attn_result'</span>]:</span>
<span id="cb13-174"><a href="#cb13-174" aria-hidden="true" tabindex="-1"></a>            result <span class="op">=</span> <span class="va">self</span>.hook_result(amp_einsum(<span class="st">'bqih,ihm-&gt;bqim'</span>, z, <span class="va">self</span>.W_O)) <span class="co"># [batch, pos, head_index, d_model]</span></span>
<span id="cb13-175"><a href="#cb13-175" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> einops.<span class="bu">reduce</span>(result, </span>
<span id="cb13-176"><a href="#cb13-176" aria-hidden="true" tabindex="-1"></a>                            <span class="st">'batch position index model-&gt;batch position model'</span>, </span>
<span id="cb13-177"><a href="#cb13-177" aria-hidden="true" tabindex="-1"></a>                            <span class="st">'sum'</span>)<span class="op">+</span><span class="va">self</span>.b_O  <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-178"><a href="#cb13-178" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-179"><a href="#cb13-179" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> (amp_einsum(<span class="st">'bqih,ihm-&gt;bqm'</span>, z, <span class="va">self</span>.W_O)<span class="op">+</span><span class="va">self</span>.b_O) <span class="co"># [batch, pos, head_index, d_model]</span></span>
<span id="cb13-180"><a href="#cb13-180" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb13-181"><a href="#cb13-181" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-182"><a href="#cb13-182" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> apply_causal_mask(<span class="va">self</span>, attn_scores):</span>
<span id="cb13-183"><a href="#cb13-183" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.where(<span class="va">self</span>.mask[:attn_scores.size(<span class="op">-</span><span class="dv">2</span>), :attn_scores.size(<span class="op">-</span><span class="dv">1</span>)], attn_scores, <span class="va">self</span>.IGNORE)</span>
<span id="cb13-184"><a href="#cb13-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-185"><a href="#cb13-185" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb13-186"><a href="#cb13-186" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg):</span>
<span id="cb13-187"><a href="#cb13-187" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-188"><a href="#cb13-188" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb13-189"><a href="#cb13-189" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_in <span class="op">=</span> nn.Parameter(torch.empty(<span class="va">self</span>.cfg[<span class="st">'d_model'</span>], <span class="va">self</span>.cfg[<span class="st">'d_mlp'</span>]))</span>
<span id="cb13-190"><a href="#cb13-190" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.W_in, a<span class="op">=</span>np.sqrt(<span class="dv">5</span>), mode<span class="op">=</span><span class="st">'fan_out'</span>)</span>
<span id="cb13-191"><a href="#cb13-191" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_in <span class="op">=</span> nn.Parameter(torch.zeros(<span class="va">self</span>.cfg[<span class="st">'d_mlp'</span>]))</span>
<span id="cb13-192"><a href="#cb13-192" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_out <span class="op">=</span> nn.Parameter(torch.empty(<span class="va">self</span>.cfg[<span class="st">'d_mlp'</span>], <span class="va">self</span>.cfg[<span class="st">'d_model'</span>]))</span>
<span id="cb13-193"><a href="#cb13-193" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.W_out, a<span class="op">=</span>np.sqrt(<span class="dv">5</span>), mode<span class="op">=</span><span class="st">'fan_out'</span>)</span>
<span id="cb13-194"><a href="#cb13-194" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_out <span class="op">=</span> nn.Parameter(torch.zeros(<span class="va">self</span>.cfg[<span class="st">'d_model'</span>]))</span>
<span id="cb13-195"><a href="#cb13-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-196"><a href="#cb13-196" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_pre <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_mlp]</span></span>
<span id="cb13-197"><a href="#cb13-197" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_post <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_mlp]</span></span>
<span id="cb13-198"><a href="#cb13-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-199"><a href="#cb13-199" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cfg[<span class="st">'act_fn'</span>].lower()<span class="op">==</span><span class="st">'relu'</span>:</span>
<span id="cb13-200"><a href="#cb13-200" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.act_fn <span class="op">=</span> F.relu</span>
<span id="cb13-201"><a href="#cb13-201" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.cfg[<span class="st">'act_fn'</span>].lower()<span class="op">==</span><span class="st">'gelu_new'</span>:</span>
<span id="cb13-202"><a href="#cb13-202" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.act_fn <span class="op">=</span> gelu_new</span>
<span id="cb13-203"><a href="#cb13-203" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.cfg[<span class="st">'act_fn'</span>].lower()<span class="op">==</span><span class="st">'solu'</span>:</span>
<span id="cb13-204"><a href="#cb13-204" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.act_fn <span class="op">=</span> <span class="kw">lambda</span> x: F.softmax(x, dim<span class="op">=-</span><span class="dv">1</span>)<span class="op">*</span>x</span>
<span id="cb13-205"><a href="#cb13-205" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hook_post_ln <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_mlp]</span></span>
<span id="cb13-206"><a href="#cb13-206" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.ln <span class="op">=</span> LayerNorm(<span class="va">self</span>.cfg, <span class="va">self</span>.cfg[<span class="st">'d_mlp'</span>])</span>
<span id="cb13-207"><a href="#cb13-207" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-208"><a href="#cb13-208" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Invalid activation function name: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>cfg[<span class="st">'act_fn'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-209"><a href="#cb13-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-210"><a href="#cb13-210" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-211"><a href="#cb13-211" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hook_pre(amp_einsum(<span class="st">'bpd,dm-&gt;bpm'</span>, x, <span class="va">self</span>.W_in) <span class="op">+</span> <span class="va">self</span>.b_in) <span class="co"># [batch, pos, d_mlp]</span></span>
<span id="cb13-212"><a href="#cb13-212" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hook_post(<span class="va">self</span>.act_fn(x)) <span class="co"># [batch, pos, d_mlp]</span></span>
<span id="cb13-213"><a href="#cb13-213" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cfg[<span class="st">'act_fn'</span>].lower()<span class="op">==</span><span class="st">'solu'</span>:</span>
<span id="cb13-214"><a href="#cb13-214" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.hook_post_ln(<span class="va">self</span>.ln(x))</span>
<span id="cb13-215"><a href="#cb13-215" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> amp_einsum(<span class="st">'bpm,md-&gt;bpd'</span>, x, <span class="va">self</span>.W_out) <span class="op">+</span> <span class="va">self</span>.b_out <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-216"><a href="#cb13-216" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb13-217"><a href="#cb13-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-218"><a href="#cb13-218" aria-hidden="true" tabindex="-1"></a><span class="co"># Transformer Block</span></span>
<span id="cb13-219"><a href="#cb13-219" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb13-220"><a href="#cb13-220" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg, block_index):</span>
<span id="cb13-221"><a href="#cb13-221" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-222"><a href="#cb13-222" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb13-223"><a href="#cb13-223" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cfg[<span class="st">'normalization'</span>]<span class="op">==</span><span class="st">'RMS'</span>:</span>
<span id="cb13-224"><a href="#cb13-224" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.norm1 <span class="op">=</span> LayerNorm(<span class="va">self</span>.cfg, <span class="va">self</span>.cfg[<span class="st">'d_model'</span>])</span>
<span id="cb13-225"><a href="#cb13-225" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.norm2 <span class="op">=</span> LayerNorm(<span class="va">self</span>.cfg, <span class="va">self</span>.cfg[<span class="st">'d_model'</span>])</span>
<span id="cb13-226"><a href="#cb13-226" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.cfg[<span class="st">'normalization'</span>]<span class="op">==</span><span class="st">'LN'</span>:</span>
<span id="cb13-227"><a href="#cb13-227" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.norm1 <span class="op">=</span> LayerNorm(<span class="va">self</span>.cfg, <span class="va">self</span>.cfg[<span class="st">'d_model'</span>])</span>
<span id="cb13-228"><a href="#cb13-228" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.norm2 <span class="op">=</span> LayerNorm(<span class="va">self</span>.cfg, <span class="va">self</span>.cfg[<span class="st">'d_model'</span>])</span>
<span id="cb13-229"><a href="#cb13-229" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> Attention(<span class="va">self</span>.cfg)</span>
<span id="cb13-230"><a href="#cb13-230" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> MLP(<span class="va">self</span>.cfg)</span>
<span id="cb13-231"><a href="#cb13-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-232"><a href="#cb13-232" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_attn_out <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-233"><a href="#cb13-233" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_mlp_out <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-234"><a href="#cb13-234" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note that resid_pre of layer k+1 is resid_post of layer k - given for convenience</span></span>
<span id="cb13-235"><a href="#cb13-235" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_resid_pre <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-236"><a href="#cb13-236" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_resid_mid <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-237"><a href="#cb13-237" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_resid_post <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-238"><a href="#cb13-238" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-239"><a href="#cb13-239" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, pos_embed):</span>
<span id="cb13-240"><a href="#cb13-240" aria-hidden="true" tabindex="-1"></a>        resid_pre <span class="op">=</span> <span class="va">self</span>.hook_resid_pre(x) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-241"><a href="#cb13-241" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cfg[<span class="st">'normalization'</span>] <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb13-242"><a href="#cb13-242" aria-hidden="true" tabindex="-1"></a>            attn_out <span class="op">=</span> <span class="va">self</span>.hook_attn_out(<span class="va">self</span>.attn(<span class="va">self</span>.norm1(resid_pre), pos_embed)) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-243"><a href="#cb13-243" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-244"><a href="#cb13-244" aria-hidden="true" tabindex="-1"></a>            attn_out <span class="op">=</span> <span class="va">self</span>.hook_attn_out(<span class="va">self</span>.attn(resid_pre, pos_embed)) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-245"><a href="#cb13-245" aria-hidden="true" tabindex="-1"></a>        resid_mid <span class="op">=</span> <span class="va">self</span>.hook_resid_mid(resid_pre <span class="op">+</span> attn_out) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-246"><a href="#cb13-246" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cfg[<span class="st">'normalization'</span>] <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb13-247"><a href="#cb13-247" aria-hidden="true" tabindex="-1"></a>            mlp_out <span class="op">=</span> <span class="va">self</span>.hook_mlp_out(<span class="va">self</span>.mlp(<span class="va">self</span>.norm2(resid_mid))) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-248"><a href="#cb13-248" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-249"><a href="#cb13-249" aria-hidden="true" tabindex="-1"></a>            mlp_out <span class="op">=</span> <span class="va">self</span>.hook_mlp_out(<span class="va">self</span>.mlp(resid_mid)) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-250"><a href="#cb13-250" aria-hidden="true" tabindex="-1"></a>        resid_post <span class="op">=</span> <span class="va">self</span>.hook_resid_post(resid_mid <span class="op">+</span> mlp_out) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-251"><a href="#cb13-251" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> resid_post</span>
<span id="cb13-252"><a href="#cb13-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-253"><a href="#cb13-253" aria-hidden="true" tabindex="-1"></a><span class="co"># Full transformer</span></span>
<span id="cb13-254"><a href="#cb13-254" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(HookedRootModule):</span>
<span id="cb13-255"><a href="#cb13-255" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg, tokenizer):</span>
<span id="cb13-256"><a href="#cb13-256" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-257"><a href="#cb13-257" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-258"><a href="#cb13-258" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb13-259"><a href="#cb13-259" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb13-260"><a href="#cb13-260" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-261"><a href="#cb13-261" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cfg[<span class="st">'factored_embed'</span>]:</span>
<span id="cb13-262"><a href="#cb13-262" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.embed <span class="op">=</span> FactoredEmbed(<span class="va">self</span>.cfg)</span>
<span id="cb13-263"><a href="#cb13-263" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-264"><a href="#cb13-264" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.embed <span class="op">=</span> Embed(<span class="va">self</span>.cfg)</span>
<span id="cb13-265"><a href="#cb13-265" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_embed <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-266"><a href="#cb13-266" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-267"><a href="#cb13-267" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> PosEmbed(<span class="va">self</span>.cfg)</span>
<span id="cb13-268"><a href="#cb13-268" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_pos_embed <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-269"><a href="#cb13-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-270"><a href="#cb13-270" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cfg[<span class="st">'normalization'</span>]<span class="op">==</span><span class="st">'RMS'</span>:</span>
<span id="cb13-271"><a href="#cb13-271" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.norm <span class="op">=</span> RMSNorm(<span class="va">self</span>.cfg, <span class="va">self</span>.cfg[<span class="st">'d_model'</span>])</span>
<span id="cb13-272"><a href="#cb13-272" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> cfg[<span class="st">'normalization'</span>]<span class="op">==</span><span class="st">'LN'</span>:</span>
<span id="cb13-273"><a href="#cb13-273" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.norm <span class="op">=</span> LayerNorm(<span class="va">self</span>.cfg, <span class="va">self</span>.cfg[<span class="st">'d_model'</span>])</span>
<span id="cb13-274"><a href="#cb13-274" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb13-275"><a href="#cb13-275" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.ModuleList([TransformerBlock(<span class="va">self</span>.cfg, block_index) <span class="cf">for</span> block_index <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.cfg[<span class="st">'n_layers'</span>])])</span>
<span id="cb13-276"><a href="#cb13-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-277"><a href="#cb13-277" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cfg[<span class="st">'factored_embed'</span>]:</span>
<span id="cb13-278"><a href="#cb13-278" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.unembed <span class="op">=</span> FactoredUnembed(<span class="va">self</span>.cfg)</span>
<span id="cb13-279"><a href="#cb13-279" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-280"><a href="#cb13-280" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.unembed <span class="op">=</span> Unembed(<span class="va">self</span>.cfg)</span>
<span id="cb13-281"><a href="#cb13-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-282"><a href="#cb13-282" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gives each module a parameter with its name (relative to this root module)</span></span>
<span id="cb13-283"><a href="#cb13-283" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Needed for HookPoints to work</span></span>
<span id="cb13-284"><a href="#cb13-284" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.setup_hooks()</span>
<span id="cb13-285"><a href="#cb13-285" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb13-286"><a href="#cb13-286" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, tokens, return_loss<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb13-287"><a href="#cb13-287" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input x is either a batch of tokens ([batch, pos]) or a text string</span></span>
<span id="cb13-288"><a href="#cb13-288" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if type(x)==str:</span></span>
<span id="cb13-289"><a href="#cb13-289" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     # If text, convert to tokens (batch_size=1)</span></span>
<span id="cb13-290"><a href="#cb13-290" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     x = self.to_tokens(x)</span></span>
<span id="cb13-291"><a href="#cb13-291" aria-hidden="true" tabindex="-1"></a>        embed <span class="op">=</span> <span class="va">self</span>.hook_embed(<span class="va">self</span>.embed(tokens)) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-292"><a href="#cb13-292" aria-hidden="true" tabindex="-1"></a>        pos_embed <span class="op">=</span> <span class="va">self</span>.hook_pos_embed(<span class="va">self</span>.pos_embed(tokens)) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-293"><a href="#cb13-293" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cfg[<span class="st">'use_pos_resid'</span>]:</span>
<span id="cb13-294"><a href="#cb13-294" aria-hidden="true" tabindex="-1"></a>            residual <span class="op">=</span> embed <span class="op">+</span> pos_embed <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-295"><a href="#cb13-295" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-296"><a href="#cb13-296" aria-hidden="true" tabindex="-1"></a>            residual <span class="op">=</span> embed <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-297"><a href="#cb13-297" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.blocks:</span>
<span id="cb13-298"><a href="#cb13-298" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Note that each block includes skip connections, so we don't need</span></span>
<span id="cb13-299"><a href="#cb13-299" aria-hidden="true" tabindex="-1"></a>            <span class="co"># residual + block(residual)</span></span>
<span id="cb13-300"><a href="#cb13-300" aria-hidden="true" tabindex="-1"></a>            residual <span class="op">=</span> block(residual, pos_embed) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb13-301"><a href="#cb13-301" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cfg[<span class="st">'normalization'</span>] <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb13-302"><a href="#cb13-302" aria-hidden="true" tabindex="-1"></a>            residual <span class="op">=</span> <span class="va">self</span>.norm(residual)</span>
<span id="cb13-303"><a href="#cb13-303" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.unembed(residual) <span class="co"># [batch, pos, d_vocab]</span></span>
<span id="cb13-304"><a href="#cb13-304" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_loss:</span>
<span id="cb13-305"><a href="#cb13-305" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> loss_fn(logits, tokens)</span>
<span id="cb13-306"><a href="#cb13-306" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-307"><a href="#cb13-307" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> logits</span>
<span id="cb13-308"><a href="#cb13-308" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-309"><a href="#cb13-309" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> to_tokens(<span class="va">self</span>, text):</span>
<span id="cb13-310"><a href="#cb13-310" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)[<span class="st">'input_ids'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Transformer Block</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttnOnlyBlock(nn.Module):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg, block_index):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> Attention(cfg)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_attn_out <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note that resid_pre of layer k+1 is resid_post of layer k - given for convenience</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_resid_pre <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_resid_post <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, pos_embed):</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        resid_pre <span class="op">=</span> <span class="va">self</span>.hook_resid_pre(x) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        attn_out <span class="op">=</span> <span class="va">self</span>.hook_attn_out(<span class="va">self</span>.attn(x, pos_embed)) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        resid_post <span class="op">=</span> <span class="va">self</span>.hook_resid_post(resid_pre <span class="op">+</span> attn_out) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> resid_post</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Full transformer</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttnOnlyTransformer(HookedRootModule):</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg, tokenizer):</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">"Need to add LN support etc"</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cfg <span class="op">=</span> cfg</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embed <span class="op">=</span> Embed(<span class="va">self</span>.cfg)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_embed <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> PosEmbed(<span class="va">self</span>.cfg)</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hook_pos_embed <span class="op">=</span> HookPoint() <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.ModuleList([AttnOnlyBlock(<span class="va">self</span>.cfg, block_index) <span class="cf">for</span> block_index <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.cfg[<span class="st">'n_layers'</span>])])</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.unembed <span class="op">=</span> Unembed(<span class="va">self</span>.cfg)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gives each module a parameter with its name (relative to this root module)</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Needed for HookPoints to work</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.setup_hooks()</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, tokens, return_loss<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input x is either a batch of tokens ([batch, pos]) or a text string</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if type(x)==str:</span></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     # If text, convert to tokens (batch_size=1)</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     x = self.to_tokens(x)</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        embed <span class="op">=</span> <span class="va">self</span>.hook_embed(<span class="va">self</span>.embed(tokens)) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>        pos_embed <span class="op">=</span> <span class="va">self</span>.hook_pos_embed(<span class="va">self</span>.pos_embed(tokens)) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>        residual <span class="op">=</span> embed <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.blocks:</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Note that each block includes skip connections, so we don't need</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>            <span class="co"># residual + block(residual)</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>            residual <span class="op">=</span> block(residual, pos_embed) <span class="co"># [batch, pos, d_model]</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.unembed(residual) <span class="co"># [batch, pos, d_vocab]</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_loss:</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> loss_fn(logits, tokens)</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> logits</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> to_tokens(<span class="va">self</span>, text):</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)[<span class="st">'input_ids'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="93f82d37-9209-4180-d300-f5aebdd35fed" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">'EleutherAI/gpt-neox-20b'</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>pad_token <span class="op">=</span> <span class="st">'&lt;PAD&gt;'</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>tokenizer.add_special_tokens({<span class="st">'pad_token'</span>:pad_token})</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-neox-20b', vocab_size=50254, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;|endoftext|&gt;', 'eos_token': '&lt;|endoftext|&gt;', 'unk_token': '&lt;|endoftext|&gt;', 'pad_token': '&lt;PAD&gt;'})</code></pre>
</div>
</div>
<div class="cell" data-outputid="ddf64897-14cb-4a0b-88f6-fabb92e1bb84" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> cfg[<span class="st">'attn_only'</span>]:</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AttnOnlyTransformer(cfg, tokenizer)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Transformer(cfg, tokenizer)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>model.to(<span class="st">'cuda'</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> cfg[<span class="st">'use_bfloat16'</span>]:</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    model.to(torch.bfloat16)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), </span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>                              lr<span class="op">=</span>cfg[<span class="st">'lr'</span>], </span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>                              betas<span class="op">=</span>cfg[<span class="st">'betas'</span>], </span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>                              weight_decay<span class="op">=</span>cfg[<span class="st">'weight_decay'</span>])</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> cfg[<span class="st">'lr_schedule'</span>] <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> lr_schedule(step):</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step<span class="op">&lt;</span>cfg[<span class="st">'warmup_steps'</span>]:</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (<span class="fl">1e-7</span><span class="op">+</span>(cfg[<span class="st">'lr'</span>]<span class="op">-</span><span class="fl">1e-7</span>)<span class="op">*</span>step<span class="op">/</span>cfg[<span class="st">'warmup_steps'</span>])<span class="op">/</span>cfg[<span class="st">'lr'</span>]</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (<span class="fl">0.55</span> <span class="op">+</span> <span class="fl">0.9</span><span class="op">*</span><span class="fl">0.5</span><span class="op">*</span>np.cos(np.pi<span class="op">*</span>(step<span class="op">-</span>cfg[<span class="st">'warmup_steps'</span>])<span class="op">/</span>(cfg[<span class="st">'max_steps'</span>] <span class="op">-</span> cfg[<span class="st">'warmup_steps'</span>])))</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    param_groups <span class="op">=</span> {<span class="st">'decay'</span>:[], <span class="st">'no_decay'</span>:[]}</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(name)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'W_'</span> <span class="kw">in</span> name <span class="kw">and</span> name <span class="kw">not</span> <span class="kw">in</span> [<span class="st">'W_E'</span>, <span class="st">'W_U'</span>]:</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>            param_groups[<span class="st">'decay'</span>].append(param)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>            param_groups[<span class="st">'no_decay'</span>].append(param)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    optim_groups <span class="op">=</span> [</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>                {<span class="st">"params"</span>: param_groups[<span class="st">'decay'</span>], <span class="st">"weight_decay"</span>: cfg[<span class="st">'weight_decay'</span>]},</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>                {<span class="st">"params"</span>: param_groups[<span class="st">'no_decay'</span>], <span class="st">"weight_decay"</span>: <span class="fl">0.0</span>},</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.AdamW(optim_groups, lr<span class="op">=</span>cfg[<span class="st">'lr'</span>])</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(optimizer)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    scheduler <span class="op">=</span> torch.optim.lr_scheduler.LambdaLR(optimizer, lr_schedule)</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># px.line(y=[lr_schedule(i) for i in range(cfg['max_steps'])]).show()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>embed.W_E
pos_embed.W_pos
norm.w
norm.b
blocks.0.norm1.w
blocks.0.norm1.b
blocks.0.norm2.w
blocks.0.norm2.b
blocks.0.attn.W_Q
blocks.0.attn.b_Q
blocks.0.attn.W_K
blocks.0.attn.b_K
blocks.0.attn.W_V
blocks.0.attn.b_V
blocks.0.attn.W_O
blocks.0.attn.b_O
blocks.0.mlp.W_in
blocks.0.mlp.b_in
blocks.0.mlp.W_out
blocks.0.mlp.b_out
blocks.0.mlp.ln.w
blocks.0.mlp.ln.b
blocks.1.norm1.w
blocks.1.norm1.b
blocks.1.norm2.w
blocks.1.norm2.b
blocks.1.attn.W_Q
blocks.1.attn.b_Q
blocks.1.attn.W_K
blocks.1.attn.b_K
blocks.1.attn.W_V
blocks.1.attn.b_V
blocks.1.attn.W_O
blocks.1.attn.b_O
blocks.1.mlp.W_in
blocks.1.mlp.b_in
blocks.1.mlp.W_out
blocks.1.mlp.b_out
blocks.1.mlp.ln.w
blocks.1.mlp.ln.b
blocks.2.norm1.w
blocks.2.norm1.b
blocks.2.norm2.w
blocks.2.norm2.b
blocks.2.attn.W_Q
blocks.2.attn.b_Q
blocks.2.attn.W_K
blocks.2.attn.b_K
blocks.2.attn.W_V
blocks.2.attn.b_V
blocks.2.attn.W_O
blocks.2.attn.b_O
blocks.2.mlp.W_in
blocks.2.mlp.b_in
blocks.2.mlp.W_out
blocks.2.mlp.b_out
blocks.2.mlp.ln.w
blocks.2.mlp.ln.b
blocks.3.norm1.w
blocks.3.norm1.b
blocks.3.norm2.w
blocks.3.norm2.b
blocks.3.attn.W_Q
blocks.3.attn.b_Q
blocks.3.attn.W_K
blocks.3.attn.b_K
blocks.3.attn.W_V
blocks.3.attn.b_V
blocks.3.attn.W_O
blocks.3.attn.b_O
blocks.3.mlp.W_in
blocks.3.mlp.b_in
blocks.3.mlp.W_out
blocks.3.mlp.b_out
blocks.3.mlp.ln.w
blocks.3.mlp.ln.b
blocks.4.norm1.w
blocks.4.norm1.b
blocks.4.norm2.w
blocks.4.norm2.b
blocks.4.attn.W_Q
blocks.4.attn.b_Q
blocks.4.attn.W_K
blocks.4.attn.b_K
blocks.4.attn.W_V
blocks.4.attn.b_V
blocks.4.attn.W_O
blocks.4.attn.b_O
blocks.4.mlp.W_in
blocks.4.mlp.b_in
blocks.4.mlp.W_out
blocks.4.mlp.b_out
blocks.4.mlp.ln.w
blocks.4.mlp.ln.b
blocks.5.norm1.w
blocks.5.norm1.b
blocks.5.norm2.w
blocks.5.norm2.b
blocks.5.attn.W_Q
blocks.5.attn.b_Q
blocks.5.attn.W_K
blocks.5.attn.b_K
blocks.5.attn.W_V
blocks.5.attn.b_V
blocks.5.attn.W_O
blocks.5.attn.b_O
blocks.5.mlp.W_in
blocks.5.mlp.b_in
blocks.5.mlp.W_out
blocks.5.mlp.b_out
blocks.5.mlp.ln.w
blocks.5.mlp.ln.b
blocks.6.norm1.w
blocks.6.norm1.b
blocks.6.norm2.w
blocks.6.norm2.b
blocks.6.attn.W_Q
blocks.6.attn.b_Q
blocks.6.attn.W_K
blocks.6.attn.b_K
blocks.6.attn.W_V
blocks.6.attn.b_V
blocks.6.attn.W_O
blocks.6.attn.b_O
blocks.6.mlp.W_in
blocks.6.mlp.b_in
blocks.6.mlp.W_out
blocks.6.mlp.b_out
blocks.6.mlp.ln.w
blocks.6.mlp.ln.b
blocks.7.norm1.w
blocks.7.norm1.b
blocks.7.norm2.w
blocks.7.norm2.b
blocks.7.attn.W_Q
blocks.7.attn.b_Q
blocks.7.attn.W_K
blocks.7.attn.b_K
blocks.7.attn.W_V
blocks.7.attn.b_V
blocks.7.attn.W_O
blocks.7.attn.b_O
blocks.7.mlp.W_in
blocks.7.mlp.b_in
blocks.7.mlp.W_out
blocks.7.mlp.b_out
blocks.7.mlp.ln.w
blocks.7.mlp.ln.b
blocks.8.norm1.w
blocks.8.norm1.b
blocks.8.norm2.w
blocks.8.norm2.b
blocks.8.attn.W_Q
blocks.8.attn.b_Q
blocks.8.attn.W_K
blocks.8.attn.b_K
blocks.8.attn.W_V
blocks.8.attn.b_V
blocks.8.attn.W_O
blocks.8.attn.b_O
blocks.8.mlp.W_in
blocks.8.mlp.b_in
blocks.8.mlp.W_out
blocks.8.mlp.b_out
blocks.8.mlp.ln.w
blocks.8.mlp.ln.b
blocks.9.norm1.w
blocks.9.norm1.b
blocks.9.norm2.w
blocks.9.norm2.b
blocks.9.attn.W_Q
blocks.9.attn.b_Q
blocks.9.attn.W_K
blocks.9.attn.b_K
blocks.9.attn.W_V
blocks.9.attn.b_V
blocks.9.attn.W_O
blocks.9.attn.b_O
blocks.9.mlp.W_in
blocks.9.mlp.b_in
blocks.9.mlp.W_out
blocks.9.mlp.b_out
blocks.9.mlp.ln.w
blocks.9.mlp.ln.b
unembed.W_U
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.01

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0
)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># if cfg['attn_only']:</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co">#     model = AttnOnlyTransformer(cfg, tokenizer)</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co"># else:</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">#     model = Transformer(cfg, tokenizer)</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># model.to('cuda')</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = torch.optim.AdamW(model.parameters(), </span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">#                               lr=cfg['lr'], </span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">#                               betas=cfg['betas'], </span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co">#                               weight_decay=cfg['weight_decay'])</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># if cfg['lr_schedule'] is not None:</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">#     # print("Using scheduler:" scheduler)</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">#     scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=cfg['warmup_steps'], num_training_steps=cfg['max_steps'])</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">#     print("Using scheduler:", scheduler)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> cfg[<span class="st">'debug'</span>] <span class="kw">and</span> cfg[<span class="st">'debug_batch'</span>]:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    parallel_model <span class="op">=</span> torch.nn.DataParallel(model, </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>                        device_ids<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(torch.cuda.device_count())))</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_size <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">12</span>, <span class="dv">128</span>, <span class="dv">2</span>):</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for batch_size in range(48, 128, 4):</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for batch_size in [64, 64, 64, 64, 64, 64]:</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        start_time<span class="op">=</span>time.time()</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'New Batch!'</span>, batch_size)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        cuda_memory()</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>            batch <span class="op">=</span> torch.randint(<span class="dv">100</span>, <span class="dv">2000</span>, (<span class="dv">6</span><span class="op">*</span>batch_size, cfg[<span class="st">'n_ctx'</span>]))</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> parallel_model(batch).mean()</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'Finished run'</span>, i, batch_size, batch.shape)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        cuda_memory()</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>            <span class="kw">del</span> loss</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'Deleting loss failed'</span>)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        torch.cuda.empty_cache()</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        cuda_memory()</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Time:'</span>, time.time() <span class="op">-</span> start_time)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">raise</span> <span class="pp">ValueError</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="60ef3c09-855e-4b47-f595-b15872fdf7b3" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> cfg[<span class="st">'n_ctx'</span>]</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(examples):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    texts <span class="op">=</span> examples[<span class="st">'text'</span>]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    full_text <span class="op">=</span> tokenizer.eos_token.join(texts)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    div <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    length <span class="op">=</span> <span class="bu">len</span>(full_text)<span class="op">//</span>div</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    text_list <span class="op">=</span> [full_text[i<span class="op">*</span>length:(i<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>length] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(div)]</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenizer(text_list, return_tensors<span class="op">=</span><span class="st">'np'</span>, padding<span class="op">=</span><span class="va">True</span>)[<span class="st">'input_ids'</span>].flatten()</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokens[tokens<span class="op">!=</span>tokenizer.pad_token_id]</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(len(text_list), len(text_list[0]))</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(tokens.shape)</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(tokens)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    curr_batch_size <span class="op">=</span> n<span class="op">//</span>(seq_len<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokens[:(seq_len<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>curr_batch_size]</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> einops.rearrange(tokens, <span class="st">'(batch_size seq) -&gt; batch_size seq'</span>, batch_size<span class="op">=</span>curr_batch_size, seq<span class="op">=</span>seq_len<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    prefix <span class="op">=</span> np.ones((curr_batch_size, <span class="dv">1</span>), dtype<span class="op">=</span>np.int64)<span class="op">*</span>tokenizer.bos_token_id</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(tokens.shape, n, curr_batch_size, seq_len)</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">'text'</span>: np.concatenate([prefix, tokens], axis<span class="op">=</span><span class="dv">1</span>)}<span class="co"># tiny_owt_orig_2 = load_dataset('stas/openwebtext-10k', cache_dir='./cache', split='train', download_config=datasets.DownloadConfig(resume_download=True, num_proc=4))</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> cfg[<span class="st">'debug'</span>]:</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cfg[<span class="st">'shuffled_data'</span>]:</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>        randperm <span class="op">=</span> np.random.permutation(<span class="dv">28</span>)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Permutation of PILE URLs'</span>, randperm)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>        pile_urls <span class="op">=</span> [<span class="ss">f"https://mystic.the-eye.eu/public/AI/pile/train/</span><span class="sc">{</span>i<span class="sc">:0&gt;2}</span><span class="ss">.jsonl.zst"</span> <span class="cf">for</span> i <span class="kw">in</span> randperm]</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>        dataset <span class="op">=</span> load_dataset(<span class="st">'json'</span>, data_files<span class="op">=</span>pile_urls, streaming<span class="op">=</span><span class="va">True</span>, split<span class="op">=</span><span class="st">'train'</span>)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>        dataset <span class="op">=</span> load_dataset(cfg[<span class="st">'dataset_name'</span>], streaming<span class="op">=</span><span class="va">True</span>, split<span class="op">=</span><span class="st">'train'</span>)</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Loaded!'</span>, time.time()<span class="op">-</span>start_time)</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>        dataset <span class="op">=</span> dataset.remove_columns(<span class="st">'meta'</span>)</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span>:</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Meta not in dataset'</span>)</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Loaded!'</span>, time.time()<span class="op">-</span>start_time)</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'dataset.map'</span>, time.time()<span class="op">-</span>start_time)</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.with_format(<span class="bu">type</span><span class="op">=</span><span class="st">'torch'</span>)</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'dataset.set_format'</span>, time.time()<span class="op">-</span>start_time)</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.shuffle(seed<span class="op">=</span>cfg[<span class="st">'seed'</span>], buffer_size<span class="op">=</span><span class="dv">30000</span>)</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'dataset.shuffle'</span>, time.time()<span class="op">-</span>start_time)</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>    train_data_loader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>cfg[<span class="st">'batch_size'</span>])</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'train_data_loader ='</span>, time.time()<span class="op">-</span>start_time)</span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>    streaming_owt <span class="op">=</span> load_dataset(<span class="st">'stas/openwebtext-10k'</span>, split<span class="op">=</span><span class="st">'train'</span>, cache_dir<span class="op">=</span><span class="st">'cache'</span>)</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>    streaming_owt <span class="op">=</span> streaming_owt.<span class="bu">map</span>(tokenize, batched<span class="op">=</span><span class="va">True</span>, num_proc<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>    streaming_owt <span class="op">=</span> streaming_owt.with_format(<span class="bu">type</span><span class="op">=</span><span class="st">'torch'</span>)</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>    train_data_loader <span class="op">=</span> DataLoader(streaming_owt, batch_size<span class="op">=</span>cfg[<span class="st">'batch_size'</span>], shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c, i <span class="kw">in</span> tqdm.tqdm(<span class="bu">enumerate</span>(train_data_loader)):</span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> c <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Loaded Initial stream!"</span>)</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(c, time.time() <span class="op">-</span> start_time)</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>            start_time <span class="op">=</span> time.time()</span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> c<span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'Time for next batch:'</span>, time.time() <span class="op">-</span> start_time)</span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true" tabindex="-1"></a>data_iter <span class="op">=</span> <span class="bu">iter</span>(train_data_loader)</span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true" tabindex="-1"></a><span class="co"># tiny_owt_orig_2 = load_dataset('stas/openwebtext-10k', cache_dir='./cache', split='train', download_config=datasets.DownloadConfig(resume_download=True, num_proc=4))</span></span>
<span id="cb21-66"><a href="#cb21-66" aria-hidden="true" tabindex="-1"></a><span class="co"># print('Loaded!')</span></span>
<span id="cb21-67"><a href="#cb21-67" aria-hidden="true" tabindex="-1"></a><span class="co"># # tokenizer.add_special_tokens({'pad_token':'&lt;PAD&gt;'})</span></span>
<span id="cb21-68"><a href="#cb21-68" aria-hidden="true" tabindex="-1"></a><span class="co"># tiny_owt = tiny_owt_orig.map(tokenize, batched=True)</span></span>
<span id="cb21-69"><a href="#cb21-69" aria-hidden="true" tabindex="-1"></a><span class="co"># print('Tokenized!')</span></span>
<span id="cb21-70"><a href="#cb21-70" aria-hidden="true" tabindex="-1"></a><span class="co"># tiny_owt_2 = tiny_owt_orig_2.map(tokenize, batched=True)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Permutation of PILE URLs [17  5  6  8  9 25 18 13 14 27 26 20  2 24 10  0  7 12  4  3  1 19 16 23
 15 22 11 21]</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
[{"model_id":"3d727eff30df4a41822cd353ec0403c5","version_major":2,"version_minor":0}]
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Using custom data configuration default-5e370f4de25e8bde</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loaded! 0.853412389755249
Loaded! 0.0002193450927734375
dataset.map 0.00016236305236816406
dataset.set_format 0.0033745765686035156
dataset.shuffle 0.0008072853088378906
train_data_loader = 0.0004334449768066406</code></pre>
</div>
</div>
<div class="cell" data-outputid="9e096a14-7a38-4cc9-acc0-1410061bebb5" data-execution_count="17">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache()</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="ss">f'SoLU_</span><span class="sc">{</span>cfg[<span class="st">"n_layers"</span>]<span class="sc">}</span><span class="ss">L_v</span><span class="sc">{</span>cfg[<span class="st">"version"</span>]<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Transformer(
  (embed): Embed()
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (norm): LayerNorm(
    (hook_scale): HookPoint()
  )
  (blocks): ModuleList(
    (0): TransformerBlock(
      (norm1): LayerNorm(
        (hook_scale): HookPoint()
      )
      (norm2): LayerNorm(
        (hook_scale): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_attn): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
        (hook_post_ln): HookPoint()
        (ln): LayerNorm(
          (hook_scale): HookPoint()
        )
      )
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
    (1): TransformerBlock(
      (norm1): LayerNorm(
        (hook_scale): HookPoint()
      )
      (norm2): LayerNorm(
        (hook_scale): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_attn): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
        (hook_post_ln): HookPoint()
        (ln): LayerNorm(
          (hook_scale): HookPoint()
        )
      )
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
    (2): TransformerBlock(
      (norm1): LayerNorm(
        (hook_scale): HookPoint()
      )
      (norm2): LayerNorm(
        (hook_scale): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_attn): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
        (hook_post_ln): HookPoint()
        (ln): LayerNorm(
          (hook_scale): HookPoint()
        )
      )
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
    (3): TransformerBlock(
      (norm1): LayerNorm(
        (hook_scale): HookPoint()
      )
      (norm2): LayerNorm(
        (hook_scale): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_attn): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
        (hook_post_ln): HookPoint()
        (ln): LayerNorm(
          (hook_scale): HookPoint()
        )
      )
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
    (4): TransformerBlock(
      (norm1): LayerNorm(
        (hook_scale): HookPoint()
      )
      (norm2): LayerNorm(
        (hook_scale): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_attn): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
        (hook_post_ln): HookPoint()
        (ln): LayerNorm(
          (hook_scale): HookPoint()
        )
      )
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
    (5): TransformerBlock(
      (norm1): LayerNorm(
        (hook_scale): HookPoint()
      )
      (norm2): LayerNorm(
        (hook_scale): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_attn): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
        (hook_post_ln): HookPoint()
        (ln): LayerNorm(
          (hook_scale): HookPoint()
        )
      )
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
    (6): TransformerBlock(
      (norm1): LayerNorm(
        (hook_scale): HookPoint()
      )
      (norm2): LayerNorm(
        (hook_scale): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_attn): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
        (hook_post_ln): HookPoint()
        (ln): LayerNorm(
          (hook_scale): HookPoint()
        )
      )
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
    (7): TransformerBlock(
      (norm1): LayerNorm(
        (hook_scale): HookPoint()
      )
      (norm2): LayerNorm(
        (hook_scale): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_attn): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
        (hook_post_ln): HookPoint()
        (ln): LayerNorm(
          (hook_scale): HookPoint()
        )
      )
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
    (8): TransformerBlock(
      (norm1): LayerNorm(
        (hook_scale): HookPoint()
      )
      (norm2): LayerNorm(
        (hook_scale): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_attn): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
        (hook_post_ln): HookPoint()
        (ln): LayerNorm(
          (hook_scale): HookPoint()
        )
      )
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
    (9): TransformerBlock(
      (norm1): LayerNorm(
        (hook_scale): HookPoint()
      )
      (norm2): LayerNorm(
        (hook_scale): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_attn): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
        (hook_post_ln): HookPoint()
        (ln): LayerNorm(
          (hook_scale): HookPoint()
        )
      )
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (unembed): Unembed()
)
SoLU_10L_v22</code></pre>
</div>
</div>
<div class="cell" data-outputid="73900005-2dbc-4a6e-bd5e-456282f6a731" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>parallel_model <span class="op">=</span> torch.nn.DataParallel(model, device_ids<span class="op">=</span><span class="bu">list</span>(<span class="bu">range</span>(torch.cuda.device_count())))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>cuda_memory()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1.340122112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</code></pre>
</div>
</div>
<div class="cell" data-outputid="8f86cf69-b9d5-41af-972a-599fa727845a" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SaveSchedule():</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, max_tokens, tokens_per_step, schedule<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> schedule <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.schedule <span class="op">=</span> np.concatenate([np.arange(<span class="dv">10</span>)<span class="op">/</span><span class="dv">10</span><span class="op">*</span><span class="fl">1e-3</span>, np.arange(<span class="dv">2</span>, <span class="dv">20</span>)<span class="op">/</span><span class="dv">20</span><span class="op">*</span><span class="fl">1e-2</span>, np.arange(<span class="dv">5</span>, <span class="dv">50</span>)<span class="op">/</span><span class="dv">50</span><span class="op">*</span><span class="fl">1e-1</span>, np.arange(<span class="dv">10</span>, <span class="dv">101</span>)<span class="op">/</span><span class="dv">100</span>])</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.schedule <span class="op">=</span> schedule</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_tokens <span class="op">=</span> max_tokens</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokens_per_step <span class="op">=</span> tokens_per_step</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.next_save_point <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>        px.line(<span class="va">self</span>.schedule <span class="op">*</span> max_tokens, log_y<span class="op">=</span><span class="va">True</span>, title<span class="op">=</span><span class="st">'Save Schedule'</span>, labels<span class="op">=</span>{<span class="st">"y"</span>:<span class="st">"Tokens"</span>, <span class="st">"x"</span>:<span class="st">"Checkpoint Index"</span>}).show()</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.counter <span class="op">*</span> <span class="va">self</span>.tokens_per_step <span class="op">/</span> <span class="va">self</span>.max_tokens</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>        threshold <span class="op">=</span> <span class="va">self</span>.schedule[<span class="va">self</span>.next_save_point]</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> value <span class="op">&gt;=</span> threshold:</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.next_save_point<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.counter<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.counter<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>schedule <span class="op">=</span> SaveSchedule(cfg[<span class="st">'max_tokens'</span>], cfg[<span class="st">'tokens_per_step'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">


<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.14.0.min.js"></script>                <div id="bc9565a3-8bd8-45b2-8d53-337f3c982a8a" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("bc9565a3-8bd8-45b2-8d53-337f3c982a8a")) {                    Plotly.newPlot(                        "bc9565a3-8bd8-45b2-8d53-337f3c982a8a",                        [{"hovertemplate":"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163],"xaxis":"x","y":[0.0,1500000.0,3000000.0,4500000.0,6000000.0,7500000.0,9000000.0,10500000.0,12000000.0,13500000.000000002,15000000.0,22500000.0,30000000.0,37500000.0,45000000.0,52499999.99999999,60000000.0,67500000.00000001,75000000.0,82500000.00000001,90000000.0,97500000.00000001,104999999.99999999,112500000.0,120000000.0,127500000.00000001,135000000.00000003,142500000.0,150000000.00000003,180000000.0,210000000.00000003,240000000.0,270000000.0,300000000.00000006,330000000.00000006,360000000.0,390000000.00000006,420000000.00000006,450000000.0,480000000.0,510000000.00000006,540000000.0,570000000.0000001,600000000.0000001,630000000.0,660000000.0000001,690000000.0000001,720000000.0,750000000.0,780000000.0000001,810000000.0000001,840000000.0000001,869999999.9999999,900000000.0,930000000.0,960000000.0,990000000.0,1020000000.0000001,1049999999.9999999,1080000000.0,1110000000.0,1140000000.0000002,1170000000.0000002,1200000000.0000002,1230000000.0,1260000000.0,1290000000.0,1320000000.0000002,1350000000.0000002,1380000000.0000002,1410000000.0,1440000000.0,1470000000.0,1500000000.0,1650000000.0,1800000000.0,1950000000.0,2100000000.0000002,2250000000.0,2400000000.0,2550000000.0,2700000000.0,2850000000.0,3000000000.0,3150000000.0,3300000000.0,3450000000.0,3600000000.0,3750000000.0,3900000000.0,4050000000.0000005,4200000000.0000005,4350000000.0,4500000000.0,4650000000.0,4800000000.0,4950000000.0,5100000000.0,5250000000.0,5400000000.0,5550000000.0,5700000000.0,5850000000.0,6000000000.0,6150000000.0,6300000000.0,6450000000.0,6600000000.0,6750000000.0,6900000000.0,7050000000.0,7200000000.0,7350000000.0,7500000000.0,7650000000.0,7800000000.0,7950000000.0,8100000000.000001,8250000000.000001,8400000000.000001,8549999999.999999,8700000000.0,8850000000.0,9000000000.0,9150000000.0,9300000000.0,9450000000.0,9600000000.0,9750000000.0,9900000000.0,10050000000.0,10200000000.0,10350000000.0,10500000000.0,10650000000.0,10800000000.0,10950000000.0,11100000000.0,11250000000.0,11400000000.0,11550000000.0,11700000000.0,11850000000.0,12000000000.0,12150000000.0,12300000000.0,12450000000.0,12600000000.0,12750000000.0,12900000000.0,13050000000.0,13200000000.0,13350000000.0,13500000000.0,13650000000.0,13800000000.0,13950000000.0,14100000000.0,14250000000.0,14400000000.0,14550000000.0,14700000000.0,14850000000.0,15000000000.0],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"},"type":"log"},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Save Schedule"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('bc9565a3-8bd8-45b2-8d53-337f3c982a8a');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>
<div class="cell" data-outputid="dce3cfea-7bc5-4ec0-a716-d5a4723f8268" data-execution_count="20">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>wandb.init(project<span class="op">=</span><span class="st">"solu"</span>, entity<span class="op">=</span><span class="st">"mechanistic-interpretability"</span>, config<span class="op">=</span>cfg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>wandb: Currently logged in as: neelnanda-io (mechanistic-interpretability). Use `wandb login --relogin` to force relogin</code></pre>
</div>
<div class="cell-output cell-output-display">
wandb version 0.13.3 is available!  To upgrade, please run:
 $ pip install wandb --upgrade
</div>
<div class="cell-output cell-output-display">
Tracking run with wandb version 0.13.2
</div>
<div class="cell-output cell-output-display">
Run data is saved locally in <code>/workspace/wandb/run-20220913_204442-ve2e5pjs</code>
</div>
<div class="cell-output cell-output-display">
Syncing run <strong><a href="https://wandb.ai/mechanistic-interpretability/solu/runs/ve2e5pjs" target="_blank">jumping-sky-91</a></strong> to <a href="https://wandb.ai/mechanistic-interpretability/solu" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/run" target="_blank">docs</a>)<br>
</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<button onclick="this.nextSibling.style.display='block';this.style.display='none';">Display W&amp;B run</button><iframe src="https://wandb.ai/mechanistic-interpretability/solu/runs/ve2e5pjs?jupyter=true" style="border:none;width:100%;height:420px;display:none;"></iframe>
</div>
</div>
<div class="cell" data-outputid="df947bbf-77eb-470f-df5d-8487a6f1f4e3">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>pprint(cfg)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="co"># DataLoader(full_owt_test['text'], batch_size=cfg['batch_size'], shuffle=False, pin_memory=False)</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training begins!'</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>loss_ewmas<span class="op">=</span>[]</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>step <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>loss_ewma <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co"># loss_beta = 0.95</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>total_tokens <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>running_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>prev_time<span class="op">=</span>time.time()</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>epoch<span class="op">=</span><span class="dv">0</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="co"># for epoch in range(100):</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c, batch <span class="kw">in</span> tqdm.tqdm(<span class="bu">enumerate</span>(data_iter)):</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> batch[<span class="st">'text'</span>]</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cfg[<span class="st">'debug'</span>] <span class="kw">and</span> epoch<span class="op">==</span><span class="dv">0</span> <span class="kw">and</span> c<span class="op">&lt;</span><span class="dv">3</span>:</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(batch[<span class="dv">0</span>])</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(tokenizer.decode(batch[<span class="dv">0</span>]))</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> batch.cuda()</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> parallel_model(batch).mean()</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    running_loss<span class="op">+=</span>loss.item()</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    total_tokens <span class="op">+=</span> batch.numel()</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (c<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span>cfg[<span class="st">'batches_per_step'</span>] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[<span class="st">'grad_norm_clip'</span>])</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cfg[<span class="st">'lr_schedule'</span>] <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>            scheduler.step()</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>            wandb.log({<span class="st">'scheduled_lr'</span>:scheduler.get_last_lr()[<span class="dv">0</span>]}, step<span class="op">=</span>step)</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> schedule.step() <span class="kw">and</span> cfg[<span class="st">'use_checkpoint_schedule'</span>]:</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'Saved the model! Step: </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">. Frac of way through training: </span><span class="sc">{</span>schedule<span class="sc">.</span>schedule[schedule.next_save_point<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> cfg[<span class="st">'debug'</span>]:</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> cfg[<span class="st">'save_checkpoints_to_bfloat16'</span>]:</span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>                    save_to_bfloat16(model, <span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>step<span class="sc">:0&gt;6}</span><span class="ss">.pth'</span>)</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>                    torch.save(model.state_dict(), <span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>step<span class="sc">:0&gt;6}</span><span class="ss">.pth'</span>)</span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>                torch.save(optimizer.state_dict(), <span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">_opt_checkpoint.pth'</span>)</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> cfg[<span class="st">'lr_schedule'</span>] <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>                    torch.save(scheduler.state_dict(), <span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">_scheduler_checkpoint.pth'</span>)</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>                wandb.save(<span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>step<span class="sc">:0&gt;6}</span><span class="ss">.pth'</span>)</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">=</span> running_loss <span class="op">/</span> cfg[<span class="st">'batches_per_step'</span>]</span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>        losses.append(running_loss)</span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>        loss_ewma <span class="op">=</span> loss_ewma <span class="op">*</span> cfg[<span class="st">'train_loss_ewma_beta'</span>] <span class="op">+</span> running_loss <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> cfg[<span class="st">'train_loss_ewma_beta'</span>])</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>        loss_ewmas.append(loss_ewma)</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>        wandb.log({<span class="st">"loss"</span>: loss.item(), <span class="st">'loss_ewma'</span>:loss_ewma, <span class="st">'elapsed'</span>:time.time()<span class="op">-</span>start_time, <span class="st">'total_tokens'</span>:total_tokens, <span class="st">'c'</span>:c}, step<span class="op">=</span>step)</span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print('Just logged')</span></span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print({"loss": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c})</span></span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">30</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(c, step, total_tokens, losses[<span class="op">-</span><span class="dv">1</span>], loss_ewmas[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>        step<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step<span class="op">&gt;=</span>cfg[<span class="st">'max_steps'</span>]:</span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> c<span class="op">&lt;=</span><span class="dv">12</span> <span class="kw">and</span> epoch<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>        cuda_memory()</span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Early iteration complete!'</span>, c, time.time()<span class="op">-</span>prev_time)</span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a>        prev_time<span class="op">=</span>time.time()</span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> loss</span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(batch.shape, logits.shape, running_loss, loss, step, total_tokens)</span></span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if not cfg['debug_overfit']:</span></span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a>    <span class="co">#     break</span></span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Finished training! Train Loss EWMA: </span><span class="sc">{</span>loss_ewma<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> cfg[<span class="st">'debug'</span>]:</span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a>    torch.save(model.state_dict(), <span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">_final.pth'</span>)</span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>    wandb.save(<span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">_final.pth'</span>)</span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>wandb.finish()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'act_fn': 'SoLU',
 'attn_only': False,
 'batch_size': 104,
 'batches_per_step': 3,
 'betas': (0.9, 0.99),
 'd_head': 64,
 'd_mlp': 5120,
 'd_model': 1280,
 'd_vocab': 50278,
 'dataset_name': 'the_pile',
 'debug': False,
 'debug_batch': False,
 'debug_overfit': False,
 'factored_embed': False,
 'grad_norm_clip': 1.0,
 'ln_eps': 1e-05,
 'lr': 0.0005,
 'lr_schedule': 'cosine_warmup',
 'max_steps': 46950,
 'max_tokens': 15000000000,
 'n_ctx': 1024,
 'n_devices': 8,
 'n_heads': 20,
 'n_layers': 10,
 'normalization': 'LN',
 'right_multiply_matrices': True,
 'save_checkpoints_to_bfloat16': True,
 'seed': 14916,
 'shuffled_data': True,
 'tokens_per_step': 319488,
 'train_loss_ewma_beta': 0.99,
 'use_attn_result': False,
 'use_bfloat16': False,
 'use_bfloat16_matmul': True,
 'use_checkpoint_schedule': True,
 'use_pos_resid': True,
 'version': 22,
 'warmup_steps': 782,
 'warmup_tokens': 250000000,
 'weight_decay': 0.01}
Training begins!</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
[{"model_id":"be21190537f344af8445642cde0dce84","version_major":2,"version_minor":0}]
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:

Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[2.649573888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 0 90.26232886314392
[2.649573888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 1 2.3085339069366455
Saved the model! Step: 0. Frac of way through training: 0.0
Saved model as bfloat16 to SoLU_10L_v22_000000.pth
2 0 319488 10.982126871744791 9.019821268717449
[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 2 8.920480012893677
[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 3 2.3054752349853516
[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 4 2.3048253059387207
[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 5 2.3655030727386475
[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 6 2.2996890544891357
[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 7 2.301769256591797
[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 8 2.362489938735962
[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 9 2.303229808807373
[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 10 2.3018174171447754
[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Early iteration complete! 11 9.724807262420654</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pprint(cfg)</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co"># # DataLoader(full_owt_test['text'], batch_size=cfg['batch_size'], shuffle=False, pin_memory=False)</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print('Training begins!')</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># losses = []</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co"># loss_ewmas=[]</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co"># step = 0</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co"># start_time = time.time()</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># loss_ewma = 9</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="co"># loss_beta = 0.95</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="co"># total_tokens = 0</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="co"># running_loss = 0</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="co"># prev_time=time.time()</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="co"># epoch=0</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="co"># # for epoch in range(100):</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="co"># for epoch in range(100):</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="co">#     data_iter = iter(train_data_loader)</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="co">#     for c, batch in tqdm.tqdm(enumerate(data_iter)):</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="co">#         batch = batch['text']</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="co">#         if cfg['debug'] and epoch==0 and c&lt;3:</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="co">#             print(batch[0])</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="co">#             print(tokenizer.decode(batch[0]))</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a><span class="co">#         batch = batch.cuda()</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="co">#         loss = parallel_model(batch).mean()</span></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a><span class="co">#         loss.backward()</span></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="co">#         running_loss+=loss.item()</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a><span class="co">#         total_tokens += batch.numel()</span></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="co">#         if (c+1)%cfg['batches_per_step'] == 0:</span></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a><span class="co">#             torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['grad_norm_clip'])</span></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a><span class="co">#             optimizer.step()</span></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a><span class="co">#             if cfg['lr_schedule'] is not None:</span></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a><span class="co">#                 scheduler.step()</span></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a><span class="co">#                 wandb.log({'scheduled_lr':scheduler.get_last_lr()[0]}, step=step)</span></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a><span class="co">#             optimizer.zero_grad()</span></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a><span class="co">#             if step % cfg['checkpoint_every'] == 0:</span></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a><span class="co">#                 print(f'Saved the model! Step: {step}')</span></span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a><span class="co">#                 if not cfg['debug']:</span></span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a><span class="co">#                     torch.save(model.state_dict(), f'{model_name}_{step}.pth')</span></span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a><span class="co">#                     torch.save(optimizer.state_dict(), f'{model_name}_opt_checkpoint.pth')</span></span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a><span class="co">#                     if cfg['lr_schedule'] is not None:</span></span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a><span class="co">#                         torch.save(scheduler.state_dict(), f'{model_name}_scheduler_checkpoint.pth')</span></span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a><span class="co">#                     wandb.save(f'{model_name}_{step}.pth')</span></span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a><span class="co">#             running_loss = running_loss / cfg['batches_per_step']</span></span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a><span class="co">#             losses.append(running_loss)</span></span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a><span class="co">#             loss_ewma = loss_ewma * loss_beta + running_loss * (1 - loss_beta)</span></span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a><span class="co">#             loss_ewmas.append(loss_ewma)</span></span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a><span class="co">#             wandb.log({"loss": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c}, step=step)</span></span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a><span class="co">#             # print('Just logged')</span></span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a><span class="co">#             # print({"loss": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c})</span></span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a><span class="co">#             running_loss = 0</span></span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a><span class="co">#             if step % 30 == 0:</span></span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a><span class="co">#                 print(c, step, total_tokens, losses[-1], loss_ewmas[-1])</span></span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a><span class="co">#             step+=1</span></span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a><span class="co">#             # if step&gt;=cfg['max_steps']:</span></span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a><span class="co">#             #     break</span></span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a><span class="co">#         if c&lt;=12 and epoch==0:</span></span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a><span class="co">#             cuda_memory()</span></span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a><span class="co">#             print('Early iteration complete!', c, time.time()-prev_time)</span></span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a><span class="co">#             prev_time=time.time()</span></span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a><span class="co">#         # print(batch.shape, logits.shape, running_loss, loss, step, total_tokens)</span></span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a><span class="co">#         # if not cfg['debug_overfit']:</span></span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a><span class="co">#         #     break</span></span>
<span id="cb36-63"><a href="#cb36-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-64"><a href="#cb36-64" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f'Finished training! Train Loss EWMA: {loss_ewma}')</span></span>
<span id="cb36-65"><a href="#cb36-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-66"><a href="#cb36-66" aria-hidden="true" tabindex="-1"></a><span class="co"># if not cfg['debug']:</span></span>
<span id="cb36-67"><a href="#cb36-67" aria-hidden="true" tabindex="-1"></a><span class="co">#     torch.save(model.state_dict(), f'{model_name}_final.pth')</span></span>
<span id="cb36-68"><a href="#cb36-68" aria-hidden="true" tabindex="-1"></a><span class="co">#     wandb.save(f'{model_name}_final.pth')</span></span>
<span id="cb36-69"><a href="#cb36-69" aria-hidden="true" tabindex="-1"></a><span class="co"># wandb.finish()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print(model)</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="co"># print(parallel_model)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer_2 = torch.optim.AdamW(model.parameters(), </span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co">#                               lr=3e-4, </span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co">#                               betas=cfg['betas'], </span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co">#                               weight_decay=cfg['weight_decay'])</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer_2.load_state_dict(optimizer.state_dict())</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print(optimizer_2)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.save(optimizer_2.state_dict(), "_"+model_name+"_opt_midflight_checkpoint.pth")</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.save(model.state_dict(), "_"+model_name+"_model_midflight_checkpoint.pth")</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># wandb.save("_"+model_name+"_model_midflight_checkpoint.pth")</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># wandb.save("_"+model_name+"_opt_midflight_checkpoint.pth")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sd = optimizer_2.state_dict()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sd['param_groups'][0]['lr']=2e-4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># _ = optimizer_2.load_state_dict(sd)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print(optimizer_2)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import datasets</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># files = '29.jsonl.zst'</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co"># pile_dataset = datasets.load_dataset('json', data_files=files, split='train', cache_dir='cache')</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="co"># print(pile_dataset)</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="co"># pile_dataset = pile_dataset.remove_columns('meta')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pile_dataset = pile_dataset.map(tokenize, batched=True, num_proc=30)</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="co"># pile_dataset = pile_dataset.with_format(type='torch')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train_data_loader = DataLoader(pile_dataset, batch_size=cfg['batch_size'], shuffle=True, num_workers=10)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model_old = Transformer(cfg, tokenizer)</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model_old.to('cuda')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Old transformer, left mult</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="co"># # Define network architecture</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co"># # Embed &amp; Unembed</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="co"># class Embed(nn.Module):</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg):</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_E = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_vocab']))</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5))</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, tokens):</span></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="co">#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a><span class="co">#         # B acts as a tensor of indices into the second dimension (so &gt;=0 and &lt;b)</span></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a><span class="co">#         return einops.rearrange(self.W_E[:, tokens], 'd_model batch pos -&gt; batch pos d_model')</span></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a><span class="co"># class FactoredEmbed(nn.Module):</span></span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg):</span></span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_E = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_vocab']))</span></span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_E_factor = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['factor_size']))</span></span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5))</span></span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_E_factor, a=np.sqrt(5))</span></span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, tokens):</span></span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a><span class="co">#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]</span></span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a><span class="co">#         # B acts as a tensor of indices into the second dimension (so &gt;=0 and &lt;b)</span></span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a><span class="co">#         return einops.rearrange(self.W_E[:, tokens], 'factor batch pos -&gt; batch pos factor') @ self.W_E_factor.T</span></span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a><span class="co"># class Unembed(nn.Module):</span></span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg):</span></span>
<span id="cb49-34"><a href="#cb49-34" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-35"><a href="#cb49-35" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-36"><a href="#cb49-36" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_U = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['d_model']))</span></span>
<span id="cb49-37"><a href="#cb49-37" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5))</span></span>
<span id="cb49-38"><a href="#cb49-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-39"><a href="#cb49-39" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, residual):</span></span>
<span id="cb49-40"><a href="#cb49-40" aria-hidden="true" tabindex="-1"></a><span class="co">#         return amp_einsum('vm,bpm-&gt;bpv', self.W_U, residual) # [batch, pos, d_vocab]</span></span>
<span id="cb49-41"><a href="#cb49-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-42"><a href="#cb49-42" aria-hidden="true" tabindex="-1"></a><span class="co"># class FactoredUnembed(nn.Module):</span></span>
<span id="cb49-43"><a href="#cb49-43" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg):</span></span>
<span id="cb49-44"><a href="#cb49-44" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-45"><a href="#cb49-45" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-46"><a href="#cb49-46" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_U = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['factor_size']))</span></span>
<span id="cb49-47"><a href="#cb49-47" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_U_factor = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_model']))</span></span>
<span id="cb49-48"><a href="#cb49-48" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5))</span></span>
<span id="cb49-49"><a href="#cb49-49" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_U_factor, a=np.sqrt(5))</span></span>
<span id="cb49-50"><a href="#cb49-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-51"><a href="#cb49-51" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, residual):</span></span>
<span id="cb49-52"><a href="#cb49-52" aria-hidden="true" tabindex="-1"></a><span class="co">#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]</span></span>
<span id="cb49-53"><a href="#cb49-53" aria-hidden="true" tabindex="-1"></a><span class="co">#         # B acts as a tensor of indices into the second dimension (so &gt;=0 and &lt;b)</span></span>
<span id="cb49-54"><a href="#cb49-54" aria-hidden="true" tabindex="-1"></a><span class="co">#         return amp_einsum('fm,vf,bpm-&gt;bpv', self.W_U_factor, self.W_U, residual) # [batch, pos, d_vocab]</span></span>
<span id="cb49-55"><a href="#cb49-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-56"><a href="#cb49-56" aria-hidden="true" tabindex="-1"></a><span class="co"># # Positional Embeddings</span></span>
<span id="cb49-57"><a href="#cb49-57" aria-hidden="true" tabindex="-1"></a><span class="co"># class PosEmbed(nn.Module):</span></span>
<span id="cb49-58"><a href="#cb49-58" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg):</span></span>
<span id="cb49-59"><a href="#cb49-59" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-60"><a href="#cb49-60" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-61"><a href="#cb49-61" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_pos = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['n_ctx'])) </span></span>
<span id="cb49-62"><a href="#cb49-62" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_pos, a=np.sqrt(5))</span></span>
<span id="cb49-63"><a href="#cb49-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-64"><a href="#cb49-64" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, x):</span></span>
<span id="cb49-65"><a href="#cb49-65" aria-hidden="true" tabindex="-1"></a><span class="co">#         # Output shape [pos, d_model] - will be broadcast along batch dim</span></span>
<span id="cb49-66"><a href="#cb49-66" aria-hidden="true" tabindex="-1"></a><span class="co">#         return self.W_pos[:, :x.size(-1)].T # [pos, d_model]</span></span>
<span id="cb49-67"><a href="#cb49-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-68"><a href="#cb49-68" aria-hidden="true" tabindex="-1"></a><span class="co"># class LayerNormPre(nn.Module):</span></span>
<span id="cb49-69"><a href="#cb49-69" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg):</span></span>
<span id="cb49-70"><a href="#cb49-70" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-71"><a href="#cb49-71" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-72"><a href="#cb49-72" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.eps = self.cfg['ln_eps']</span></span>
<span id="cb49-73"><a href="#cb49-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-74"><a href="#cb49-74" aria-hidden="true" tabindex="-1"></a><span class="co">#         # Adds a hook point for the normalization scale factor</span></span>
<span id="cb49-75"><a href="#cb49-75" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_scale = HookPoint() # [batch, pos]</span></span>
<span id="cb49-76"><a href="#cb49-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-77"><a href="#cb49-77" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, x):</span></span>
<span id="cb49-78"><a href="#cb49-78" aria-hidden="true" tabindex="-1"></a><span class="co">#         x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]</span></span>
<span id="cb49-79"><a href="#cb49-79" aria-hidden="true" tabindex="-1"></a><span class="co">#         scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + </span></span>
<span id="cb49-80"><a href="#cb49-80" aria-hidden="true" tabindex="-1"></a><span class="co">#                                  self.eps).sqrt()) # [batch, pos, 1]</span></span>
<span id="cb49-81"><a href="#cb49-81" aria-hidden="true" tabindex="-1"></a><span class="co">#         return x / scale</span></span>
<span id="cb49-82"><a href="#cb49-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-83"><a href="#cb49-83" aria-hidden="true" tabindex="-1"></a><span class="co"># class LayerNorm(nn.Module):</span></span>
<span id="cb49-84"><a href="#cb49-84" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg, length):</span></span>
<span id="cb49-85"><a href="#cb49-85" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-86"><a href="#cb49-86" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-87"><a href="#cb49-87" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.eps = self.cfg['ln_eps']</span></span>
<span id="cb49-88"><a href="#cb49-88" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.length = length</span></span>
<span id="cb49-89"><a href="#cb49-89" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.w = nn.Parameter(torch.ones(length))</span></span>
<span id="cb49-90"><a href="#cb49-90" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.b = nn.Parameter(torch.zeros(length))</span></span>
<span id="cb49-91"><a href="#cb49-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-92"><a href="#cb49-92" aria-hidden="true" tabindex="-1"></a><span class="co">#         # Adds a hook point for the normalization scale factor</span></span>
<span id="cb49-93"><a href="#cb49-93" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_scale = HookPoint() # [batch, pos]</span></span>
<span id="cb49-94"><a href="#cb49-94" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-95"><a href="#cb49-95" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, x):</span></span>
<span id="cb49-96"><a href="#cb49-96" aria-hidden="true" tabindex="-1"></a><span class="co">#         x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]</span></span>
<span id="cb49-97"><a href="#cb49-97" aria-hidden="true" tabindex="-1"></a><span class="co">#         scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + </span></span>
<span id="cb49-98"><a href="#cb49-98" aria-hidden="true" tabindex="-1"></a><span class="co">#                                  self.eps).sqrt()) # [batch, pos, 1]</span></span>
<span id="cb49-99"><a href="#cb49-99" aria-hidden="true" tabindex="-1"></a><span class="co">#         out = (x / scale) * self.w + self.b</span></span>
<span id="cb49-100"><a href="#cb49-100" aria-hidden="true" tabindex="-1"></a><span class="co">#         return out</span></span>
<span id="cb49-101"><a href="#cb49-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-102"><a href="#cb49-102" aria-hidden="true" tabindex="-1"></a><span class="co"># class RMSNorm(nn.Module):</span></span>
<span id="cb49-103"><a href="#cb49-103" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg, length):</span></span>
<span id="cb49-104"><a href="#cb49-104" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-105"><a href="#cb49-105" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-106"><a href="#cb49-106" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.eps = self.cfg['ln_eps']</span></span>
<span id="cb49-107"><a href="#cb49-107" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.length = length</span></span>
<span id="cb49-108"><a href="#cb49-108" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.w = nn.Parameter(torch.ones(length))</span></span>
<span id="cb49-109"><a href="#cb49-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-110"><a href="#cb49-110" aria-hidden="true" tabindex="-1"></a><span class="co">#         # Adds a hook point for the normalization scale factor</span></span>
<span id="cb49-111"><a href="#cb49-111" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_scale = HookPoint() # [batch, pos]</span></span>
<span id="cb49-112"><a href="#cb49-112" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-113"><a href="#cb49-113" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, x):</span></span>
<span id="cb49-114"><a href="#cb49-114" aria-hidden="true" tabindex="-1"></a><span class="co">#         scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + </span></span>
<span id="cb49-115"><a href="#cb49-115" aria-hidden="true" tabindex="-1"></a><span class="co">#                                  self.eps).sqrt()) # [batch, pos, 1]</span></span>
<span id="cb49-116"><a href="#cb49-116" aria-hidden="true" tabindex="-1"></a><span class="co">#         out = (x / scale) * self.w</span></span>
<span id="cb49-117"><a href="#cb49-117" aria-hidden="true" tabindex="-1"></a><span class="co">#         return out</span></span>
<span id="cb49-118"><a href="#cb49-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-119"><a href="#cb49-119" aria-hidden="true" tabindex="-1"></a><span class="co"># # Attention</span></span>
<span id="cb49-120"><a href="#cb49-120" aria-hidden="true" tabindex="-1"></a><span class="co"># class Attention(nn.Module):</span></span>
<span id="cb49-121"><a href="#cb49-121" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg, attn_type='global'):</span></span>
<span id="cb49-122"><a href="#cb49-122" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-123"><a href="#cb49-123" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-124"><a href="#cb49-124" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_Q = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))</span></span>
<span id="cb49-125"><a href="#cb49-125" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.b_Q = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))</span></span>
<span id="cb49-126"><a href="#cb49-126" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_Q, a=np.sqrt(5))</span></span>
<span id="cb49-127"><a href="#cb49-127" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_K = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))</span></span>
<span id="cb49-128"><a href="#cb49-128" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.b_K = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))</span></span>
<span id="cb49-129"><a href="#cb49-129" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_K, a=np.sqrt(5))</span></span>
<span id="cb49-130"><a href="#cb49-130" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_V = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))</span></span>
<span id="cb49-131"><a href="#cb49-131" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.b_V = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))</span></span>
<span id="cb49-132"><a href="#cb49-132" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_V, a=np.sqrt(5))</span></span>
<span id="cb49-133"><a href="#cb49-133" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_O = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_model'], self.cfg['d_head']))</span></span>
<span id="cb49-134"><a href="#cb49-134" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.b_O = nn.Parameter(torch.zeros(self.cfg['d_model']))</span></span>
<span id="cb49-135"><a href="#cb49-135" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_O, a=np.sqrt(5))</span></span>
<span id="cb49-136"><a href="#cb49-136" aria-hidden="true" tabindex="-1"></a><span class="co">#         # if cfg['W_O_init_scale']:</span></span>
<span id="cb49-137"><a href="#cb49-137" aria-hidden="true" tabindex="-1"></a><span class="co">#         #     self.W_O/=np.sqrt(2*self.cfg['n_layers'])</span></span>
<span id="cb49-138"><a href="#cb49-138" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb49-139"><a href="#cb49-139" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.attn_type = attn_type</span></span>
<span id="cb49-140"><a href="#cb49-140" aria-hidden="true" tabindex="-1"></a><span class="co">#         # Create a query_pos x key_pos mask, with True iff that query position </span></span>
<span id="cb49-141"><a href="#cb49-141" aria-hidden="true" tabindex="-1"></a><span class="co">#         # can attend to that key position</span></span>
<span id="cb49-142"><a href="#cb49-142" aria-hidden="true" tabindex="-1"></a><span class="co">#         causal_mask = torch.tril(torch.ones((self.cfg['n_ctx'], self.cfg['n_ctx'])).bool())</span></span>
<span id="cb49-143"><a href="#cb49-143" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.register_buffer('mask', causal_mask)</span></span>
<span id="cb49-144"><a href="#cb49-144" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb49-145"><a href="#cb49-145" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.register_buffer('IGNORE', torch.tensor(-1e5))</span></span>
<span id="cb49-146"><a href="#cb49-146" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.attn_scale = np.sqrt(self.cfg['d_head'])</span></span>
<span id="cb49-147"><a href="#cb49-147" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb49-148"><a href="#cb49-148" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_k = HookPoint() # [batch, pos, head_index, d_head]</span></span>
<span id="cb49-149"><a href="#cb49-149" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_q = HookPoint() # [batch, pos, head_index, d_head]</span></span>
<span id="cb49-150"><a href="#cb49-150" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_v = HookPoint() # [batch, pos, head_index, d_head]</span></span>
<span id="cb49-151"><a href="#cb49-151" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_z = HookPoint() # [batch, pos, head_index, d_head]</span></span>
<span id="cb49-152"><a href="#cb49-152" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_attn_scores = HookPoint() # [batch, head_index, query_pos, key_pos]</span></span>
<span id="cb49-153"><a href="#cb49-153" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_attn = HookPoint() # [batch, head_index, query_pos, key_pos]</span></span>
<span id="cb49-154"><a href="#cb49-154" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_result = HookPoint() # [batch, head_index, head_index, d_model]</span></span>
<span id="cb49-155"><a href="#cb49-155" aria-hidden="true" tabindex="-1"></a><span class="co">#         if not cfg['use_pos_resid']:</span></span>
<span id="cb49-156"><a href="#cb49-156" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.hook_attn_input = HookPoint()</span></span>
<span id="cb49-157"><a href="#cb49-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-158"><a href="#cb49-158" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, x, pos_embed):</span></span>
<span id="cb49-159"><a href="#cb49-159" aria-hidden="true" tabindex="-1"></a><span class="co">#         if not cfg['use_pos_resid']:</span></span>
<span id="cb49-160"><a href="#cb49-160" aria-hidden="true" tabindex="-1"></a><span class="co">#             attn_input = self.hook_attn_input(x+pos_embed)</span></span>
<span id="cb49-161"><a href="#cb49-161" aria-hidden="true" tabindex="-1"></a><span class="co">#             q = self.hook_q(amp_einsum('ihm,bpm-&gt;bpih', self.W_Q, attn_input)+self.b_Q) # [batch, pos, head_index, d_head]</span></span>
<span id="cb49-162"><a href="#cb49-162" aria-hidden="true" tabindex="-1"></a><span class="co">#             k = self.hook_k(amp_einsum('ihm,bpm-&gt;bpih', self.W_K, attn_input)+self.b_K) # [batch, pos, head_index, d_head]</span></span>
<span id="cb49-163"><a href="#cb49-163" aria-hidden="true" tabindex="-1"></a><span class="co">#         else:</span></span>
<span id="cb49-164"><a href="#cb49-164" aria-hidden="true" tabindex="-1"></a><span class="co">#             q = self.hook_q(amp_einsum('ihm,bpm-&gt;bpih', self.W_Q, x)+self.b_Q) # [batch, pos, head_index, d_head]</span></span>
<span id="cb49-165"><a href="#cb49-165" aria-hidden="true" tabindex="-1"></a><span class="co">#             k = self.hook_k(amp_einsum('ihm,bpm-&gt;bpih', self.W_K, x)+self.b_K) # [batch, pos, head_index, d_head]</span></span>
<span id="cb49-166"><a href="#cb49-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-167"><a href="#cb49-167" aria-hidden="true" tabindex="-1"></a><span class="co">#         v = self.hook_v(amp_einsum('ihm,bpm-&gt;bpih', self.W_V, x)+self.b_V) # [batch, pos, head_index, d_head]</span></span>
<span id="cb49-168"><a href="#cb49-168" aria-hidden="true" tabindex="-1"></a><span class="co">#         attn_scores = amp_einsum('bpih,bqih-&gt;bipq', q, k)/self.attn_scale # [batch, head_index, query_pos, key_pos]</span></span>
<span id="cb49-169"><a href="#cb49-169" aria-hidden="true" tabindex="-1"></a><span class="co">#         attn_scores = self.hook_attn_scores(self.apply_causal_mask(attn_scores)) # [batch, head_index, query_pos, key_pos]</span></span>
<span id="cb49-170"><a href="#cb49-170" aria-hidden="true" tabindex="-1"></a><span class="co">#         attn_matrix = self.hook_attn(F.softmax(attn_scores, dim=-1)) # [batch, head_index, query_pos, key_pos]</span></span>
<span id="cb49-171"><a href="#cb49-171" aria-hidden="true" tabindex="-1"></a><span class="co">#         z = self.hook_z(amp_einsum('bpih,biqp-&gt;bqih', v, attn_matrix)) # [batch, pos, head_index, d_head]</span></span>
<span id="cb49-172"><a href="#cb49-172" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb49-173"><a href="#cb49-173" aria-hidden="true" tabindex="-1"></a><span class="co">#         if cfg['use_attn_result']:</span></span>
<span id="cb49-174"><a href="#cb49-174" aria-hidden="true" tabindex="-1"></a><span class="co">#             result = self.hook_result(amp_einsum('imh,bqih-&gt;bqim', self.W_O, z)) # [batch, pos, head_index, d_model]</span></span>
<span id="cb49-175"><a href="#cb49-175" aria-hidden="true" tabindex="-1"></a><span class="co">#             out = einops.reduce(result, </span></span>
<span id="cb49-176"><a href="#cb49-176" aria-hidden="true" tabindex="-1"></a><span class="co">#                             'batch position index model-&gt;batch position model', </span></span>
<span id="cb49-177"><a href="#cb49-177" aria-hidden="true" tabindex="-1"></a><span class="co">#                             'sum')+self.b_O  # [batch, pos, d_model]</span></span>
<span id="cb49-178"><a href="#cb49-178" aria-hidden="true" tabindex="-1"></a><span class="co">#         else:</span></span>
<span id="cb49-179"><a href="#cb49-179" aria-hidden="true" tabindex="-1"></a><span class="co">#             out = (amp_einsum('imh,bqih-&gt;bqm', self.W_O, z)+self.b_O) # [batch, pos, head_index, d_model]</span></span>
<span id="cb49-180"><a href="#cb49-180" aria-hidden="true" tabindex="-1"></a><span class="co">#         return out</span></span>
<span id="cb49-181"><a href="#cb49-181" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-182"><a href="#cb49-182" aria-hidden="true" tabindex="-1"></a><span class="co">#     def apply_causal_mask(self, attn_scores):</span></span>
<span id="cb49-183"><a href="#cb49-183" aria-hidden="true" tabindex="-1"></a><span class="co">#         return torch.where(self.mask[:attn_scores.size(-2), :attn_scores.size(-1)], attn_scores, self.IGNORE)</span></span>
<span id="cb49-184"><a href="#cb49-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-185"><a href="#cb49-185" aria-hidden="true" tabindex="-1"></a><span class="co"># class MLP(nn.Module):</span></span>
<span id="cb49-186"><a href="#cb49-186" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg):</span></span>
<span id="cb49-187"><a href="#cb49-187" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-188"><a href="#cb49-188" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-189"><a href="#cb49-189" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_in = nn.Parameter(torch.empty(self.cfg['d_mlp'], self.cfg['d_model']))</span></span>
<span id="cb49-190"><a href="#cb49-190" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_in, a=np.sqrt(5))</span></span>
<span id="cb49-191"><a href="#cb49-191" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.b_in = nn.Parameter(torch.zeros(self.cfg['d_mlp']))</span></span>
<span id="cb49-192"><a href="#cb49-192" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.W_out = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_mlp']))</span></span>
<span id="cb49-193"><a href="#cb49-193" aria-hidden="true" tabindex="-1"></a><span class="co">#         nn.init.kaiming_uniform_(self.W_out, a=np.sqrt(5))</span></span>
<span id="cb49-194"><a href="#cb49-194" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.b_out = nn.Parameter(torch.zeros(self.cfg['d_model']))</span></span>
<span id="cb49-195"><a href="#cb49-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-196"><a href="#cb49-196" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_pre = HookPoint() # [batch, pos, d_mlp]</span></span>
<span id="cb49-197"><a href="#cb49-197" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_post = HookPoint() # [batch, pos, d_mlp]</span></span>
<span id="cb49-198"><a href="#cb49-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-199"><a href="#cb49-199" aria-hidden="true" tabindex="-1"></a><span class="co">#         if self.cfg['act_fn'].lower()=='relu':</span></span>
<span id="cb49-200"><a href="#cb49-200" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.act_fn = F.relu</span></span>
<span id="cb49-201"><a href="#cb49-201" aria-hidden="true" tabindex="-1"></a><span class="co">#         elif self.cfg['act_fn'].lower()=='gelu_new':</span></span>
<span id="cb49-202"><a href="#cb49-202" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.act_fn = gelu_new</span></span>
<span id="cb49-203"><a href="#cb49-203" aria-hidden="true" tabindex="-1"></a><span class="co">#         elif self.cfg['act_fn'].lower()=='solu':</span></span>
<span id="cb49-204"><a href="#cb49-204" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.act_fn = lambda x: F.softmax(x, dim=-1)*x</span></span>
<span id="cb49-205"><a href="#cb49-205" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.hook_post_ln = HookPoint() # [batch, pos, d_mlp]</span></span>
<span id="cb49-206"><a href="#cb49-206" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.ln = LayerNorm(self.cfg, self.cfg['d_mlp'])</span></span>
<span id="cb49-207"><a href="#cb49-207" aria-hidden="true" tabindex="-1"></a><span class="co">#         else:</span></span>
<span id="cb49-208"><a href="#cb49-208" aria-hidden="true" tabindex="-1"></a><span class="co">#             raise ValueError(f"Invalid activation function name: {self.cfg['act_fn']}")</span></span>
<span id="cb49-209"><a href="#cb49-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-210"><a href="#cb49-210" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, x):</span></span>
<span id="cb49-211"><a href="#cb49-211" aria-hidden="true" tabindex="-1"></a><span class="co">#         x = self.hook_pre(amp_einsum('md,bpd-&gt;bpm', self.W_in, x) + self.b_in) # [batch, pos, d_mlp]</span></span>
<span id="cb49-212"><a href="#cb49-212" aria-hidden="true" tabindex="-1"></a><span class="co">#         x = self.hook_post(self.act_fn(x)) # [batch, pos, d_mlp]</span></span>
<span id="cb49-213"><a href="#cb49-213" aria-hidden="true" tabindex="-1"></a><span class="co">#         if self.cfg['act_fn'].lower()=='solu':</span></span>
<span id="cb49-214"><a href="#cb49-214" aria-hidden="true" tabindex="-1"></a><span class="co">#             x = self.hook_post_ln(self.ln(x))</span></span>
<span id="cb49-215"><a href="#cb49-215" aria-hidden="true" tabindex="-1"></a><span class="co">#         x = amp_einsum('dm,bpm-&gt;bpd', self.W_out, x) + self.b_out # [batch, pos, d_model]</span></span>
<span id="cb49-216"><a href="#cb49-216" aria-hidden="true" tabindex="-1"></a><span class="co">#         return x</span></span>
<span id="cb49-217"><a href="#cb49-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-218"><a href="#cb49-218" aria-hidden="true" tabindex="-1"></a><span class="co"># # Transformer Block</span></span>
<span id="cb49-219"><a href="#cb49-219" aria-hidden="true" tabindex="-1"></a><span class="co"># class TransformerBlock(nn.Module):</span></span>
<span id="cb49-220"><a href="#cb49-220" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg, block_index):</span></span>
<span id="cb49-221"><a href="#cb49-221" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-222"><a href="#cb49-222" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-223"><a href="#cb49-223" aria-hidden="true" tabindex="-1"></a><span class="co">#         if self.cfg['normalization']=='RMS':</span></span>
<span id="cb49-224"><a href="#cb49-224" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.norm1 = LayerNorm(self.cfg, self.cfg['d_model'])</span></span>
<span id="cb49-225"><a href="#cb49-225" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.norm2 = LayerNorm(self.cfg, self.cfg['d_model'])</span></span>
<span id="cb49-226"><a href="#cb49-226" aria-hidden="true" tabindex="-1"></a><span class="co">#         elif self.cfg['normalization']=='LN':</span></span>
<span id="cb49-227"><a href="#cb49-227" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.norm1 = LayerNorm(self.cfg, self.cfg['d_model'])</span></span>
<span id="cb49-228"><a href="#cb49-228" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.norm2 = LayerNorm(self.cfg, self.cfg['d_model'])</span></span>
<span id="cb49-229"><a href="#cb49-229" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.attn = Attention(self.cfg)</span></span>
<span id="cb49-230"><a href="#cb49-230" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.mlp = MLP(self.cfg)</span></span>
<span id="cb49-231"><a href="#cb49-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-232"><a href="#cb49-232" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_attn_out = HookPoint() # [batch, pos, d_model]</span></span>
<span id="cb49-233"><a href="#cb49-233" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_mlp_out = HookPoint() # [batch, pos, d_model]</span></span>
<span id="cb49-234"><a href="#cb49-234" aria-hidden="true" tabindex="-1"></a><span class="co">#         # Note that resid_pre of layer k+1 is resid_post of layer k - given for convenience</span></span>
<span id="cb49-235"><a href="#cb49-235" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_resid_pre = HookPoint() # [batch, pos, d_model]</span></span>
<span id="cb49-236"><a href="#cb49-236" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_resid_mid = HookPoint() # [batch, pos, d_model]</span></span>
<span id="cb49-237"><a href="#cb49-237" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_resid_post = HookPoint() # [batch, pos, d_model]</span></span>
<span id="cb49-238"><a href="#cb49-238" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-239"><a href="#cb49-239" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, x, pos_embed):</span></span>
<span id="cb49-240"><a href="#cb49-240" aria-hidden="true" tabindex="-1"></a><span class="co">#         resid_pre = self.hook_resid_pre(x) # [batch, pos, d_model]</span></span>
<span id="cb49-241"><a href="#cb49-241" aria-hidden="true" tabindex="-1"></a><span class="co">#         if self.cfg['normalization'] is not None:</span></span>
<span id="cb49-242"><a href="#cb49-242" aria-hidden="true" tabindex="-1"></a><span class="co">#             attn_out = self.hook_attn_out(self.attn(self.norm1(resid_pre), pos_embed)) # [batch, pos, d_model]</span></span>
<span id="cb49-243"><a href="#cb49-243" aria-hidden="true" tabindex="-1"></a><span class="co">#         else:</span></span>
<span id="cb49-244"><a href="#cb49-244" aria-hidden="true" tabindex="-1"></a><span class="co">#             attn_out = self.hook_attn_out(self.attn(resid_pre, pos_embed)) # [batch, pos, d_model]</span></span>
<span id="cb49-245"><a href="#cb49-245" aria-hidden="true" tabindex="-1"></a><span class="co">#         resid_mid = self.hook_resid_mid(resid_pre + attn_out) # [batch, pos, d_model]</span></span>
<span id="cb49-246"><a href="#cb49-246" aria-hidden="true" tabindex="-1"></a><span class="co">#         if self.cfg['normalization'] is not None:</span></span>
<span id="cb49-247"><a href="#cb49-247" aria-hidden="true" tabindex="-1"></a><span class="co">#             mlp_out = self.hook_mlp_out(self.mlp(self.norm2(resid_mid))) # [batch, pos, d_model]</span></span>
<span id="cb49-248"><a href="#cb49-248" aria-hidden="true" tabindex="-1"></a><span class="co">#         else:</span></span>
<span id="cb49-249"><a href="#cb49-249" aria-hidden="true" tabindex="-1"></a><span class="co">#             mlp_out = self.hook_mlp_out(self.mlp(resid_mid)) # [batch, pos, d_model]</span></span>
<span id="cb49-250"><a href="#cb49-250" aria-hidden="true" tabindex="-1"></a><span class="co">#         resid_post = self.hook_resid_post(resid_mid + mlp_out) # [batch, pos, d_model]</span></span>
<span id="cb49-251"><a href="#cb49-251" aria-hidden="true" tabindex="-1"></a><span class="co">#         return resid_post</span></span>
<span id="cb49-252"><a href="#cb49-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-253"><a href="#cb49-253" aria-hidden="true" tabindex="-1"></a><span class="co"># # Full transformer</span></span>
<span id="cb49-254"><a href="#cb49-254" aria-hidden="true" tabindex="-1"></a><span class="co"># class Transformer(HookedRootModule):</span></span>
<span id="cb49-255"><a href="#cb49-255" aria-hidden="true" tabindex="-1"></a><span class="co">#     def __init__(self, cfg, tokenizer):</span></span>
<span id="cb49-256"><a href="#cb49-256" aria-hidden="true" tabindex="-1"></a><span class="co">#         super().__init__()</span></span>
<span id="cb49-257"><a href="#cb49-257" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb49-258"><a href="#cb49-258" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.cfg = cfg</span></span>
<span id="cb49-259"><a href="#cb49-259" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.tokenizer = tokenizer</span></span>
<span id="cb49-260"><a href="#cb49-260" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb49-261"><a href="#cb49-261" aria-hidden="true" tabindex="-1"></a><span class="co">#         if self.cfg['factored_embed']:</span></span>
<span id="cb49-262"><a href="#cb49-262" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.embed = FactoredEmbed(self.cfg)</span></span>
<span id="cb49-263"><a href="#cb49-263" aria-hidden="true" tabindex="-1"></a><span class="co">#         else:</span></span>
<span id="cb49-264"><a href="#cb49-264" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.embed = Embed(self.cfg)</span></span>
<span id="cb49-265"><a href="#cb49-265" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_embed = HookPoint() # [batch, pos, d_model]</span></span>
<span id="cb49-266"><a href="#cb49-266" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb49-267"><a href="#cb49-267" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.pos_embed = PosEmbed(self.cfg)</span></span>
<span id="cb49-268"><a href="#cb49-268" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.hook_pos_embed = HookPoint() # [batch, pos, d_model]</span></span>
<span id="cb49-269"><a href="#cb49-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-270"><a href="#cb49-270" aria-hidden="true" tabindex="-1"></a><span class="co">#         if cfg['normalization']=='RMS':</span></span>
<span id="cb49-271"><a href="#cb49-271" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.norm = RMSNorm(self.cfg, self.cfg['d_model'])</span></span>
<span id="cb49-272"><a href="#cb49-272" aria-hidden="true" tabindex="-1"></a><span class="co">#         elif cfg['normalization']=='LN':</span></span>
<span id="cb49-273"><a href="#cb49-273" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.norm = LayerNorm(self.cfg, self.cfg['d_model'])</span></span>
<span id="cb49-274"><a href="#cb49-274" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb49-275"><a href="#cb49-275" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.blocks = nn.ModuleList([TransformerBlock(self.cfg, block_index) for block_index in range(self.cfg['n_layers'])])</span></span>
<span id="cb49-276"><a href="#cb49-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-277"><a href="#cb49-277" aria-hidden="true" tabindex="-1"></a><span class="co">#         if self.cfg['factored_embed']:</span></span>
<span id="cb49-278"><a href="#cb49-278" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.unembed = FactoredUnembed(self.cfg)</span></span>
<span id="cb49-279"><a href="#cb49-279" aria-hidden="true" tabindex="-1"></a><span class="co">#         else:</span></span>
<span id="cb49-280"><a href="#cb49-280" aria-hidden="true" tabindex="-1"></a><span class="co">#             self.unembed = Unembed(self.cfg)</span></span>
<span id="cb49-281"><a href="#cb49-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-282"><a href="#cb49-282" aria-hidden="true" tabindex="-1"></a><span class="co">#         # Gives each module a parameter with its name (relative to this root module)</span></span>
<span id="cb49-283"><a href="#cb49-283" aria-hidden="true" tabindex="-1"></a><span class="co">#         # Needed for HookPoints to work</span></span>
<span id="cb49-284"><a href="#cb49-284" aria-hidden="true" tabindex="-1"></a><span class="co">#         self.setup_hooks()</span></span>
<span id="cb49-285"><a href="#cb49-285" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb49-286"><a href="#cb49-286" aria-hidden="true" tabindex="-1"></a><span class="co">#     def forward(self, tokens, return_loss=True):</span></span>
<span id="cb49-287"><a href="#cb49-287" aria-hidden="true" tabindex="-1"></a><span class="co">#         # Input x is either a batch of tokens ([batch, pos]) or a text string</span></span>
<span id="cb49-288"><a href="#cb49-288" aria-hidden="true" tabindex="-1"></a><span class="co">#         # if type(x)==str:</span></span>
<span id="cb49-289"><a href="#cb49-289" aria-hidden="true" tabindex="-1"></a><span class="co">#         #     # If text, convert to tokens (batch_size=1)</span></span>
<span id="cb49-290"><a href="#cb49-290" aria-hidden="true" tabindex="-1"></a><span class="co">#         #     x = self.to_tokens(x)</span></span>
<span id="cb49-291"><a href="#cb49-291" aria-hidden="true" tabindex="-1"></a><span class="co">#         embed = self.hook_embed(self.embed(tokens)) # [batch, pos, d_model]</span></span>
<span id="cb49-292"><a href="#cb49-292" aria-hidden="true" tabindex="-1"></a><span class="co">#         pos_embed = self.hook_pos_embed(self.pos_embed(tokens)) # [batch, pos, d_model]</span></span>
<span id="cb49-293"><a href="#cb49-293" aria-hidden="true" tabindex="-1"></a><span class="co">#         if cfg['use_pos_resid']:</span></span>
<span id="cb49-294"><a href="#cb49-294" aria-hidden="true" tabindex="-1"></a><span class="co">#             residual = embed + pos_embed # [batch, pos, d_model]</span></span>
<span id="cb49-295"><a href="#cb49-295" aria-hidden="true" tabindex="-1"></a><span class="co">#         else:</span></span>
<span id="cb49-296"><a href="#cb49-296" aria-hidden="true" tabindex="-1"></a><span class="co">#             residual = embed # [batch, pos, d_model]</span></span>
<span id="cb49-297"><a href="#cb49-297" aria-hidden="true" tabindex="-1"></a><span class="co">#         for block in self.blocks:</span></span>
<span id="cb49-298"><a href="#cb49-298" aria-hidden="true" tabindex="-1"></a><span class="co">#             # Note that each block includes skip connections, so we don't need</span></span>
<span id="cb49-299"><a href="#cb49-299" aria-hidden="true" tabindex="-1"></a><span class="co">#             # residual + block(residual)</span></span>
<span id="cb49-300"><a href="#cb49-300" aria-hidden="true" tabindex="-1"></a><span class="co">#             residual = block(residual, pos_embed) # [batch, pos, d_model]</span></span>
<span id="cb49-301"><a href="#cb49-301" aria-hidden="true" tabindex="-1"></a><span class="co">#         if self.cfg['normalization'] is not None:</span></span>
<span id="cb49-302"><a href="#cb49-302" aria-hidden="true" tabindex="-1"></a><span class="co">#             residual = self.norm(residual)</span></span>
<span id="cb49-303"><a href="#cb49-303" aria-hidden="true" tabindex="-1"></a><span class="co">#         logits = self.unembed(residual) # [batch, pos, d_vocab]</span></span>
<span id="cb49-304"><a href="#cb49-304" aria-hidden="true" tabindex="-1"></a><span class="co">#         if return_loss:</span></span>
<span id="cb49-305"><a href="#cb49-305" aria-hidden="true" tabindex="-1"></a><span class="co">#             return loss_fn(logits, tokens)</span></span>
<span id="cb49-306"><a href="#cb49-306" aria-hidden="true" tabindex="-1"></a><span class="co">#         else:</span></span>
<span id="cb49-307"><a href="#cb49-307" aria-hidden="true" tabindex="-1"></a><span class="co">#             return logits</span></span>
<span id="cb49-308"><a href="#cb49-308" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-309"><a href="#cb49-309" aria-hidden="true" tabindex="-1"></a><span class="co">#     def to_tokens(self, text):</span></span>
<span id="cb49-310"><a href="#cb49-310" aria-hidden="true" tabindex="-1"></a><span class="co">#         return self.tokenizer(text, return_tensors='pt')['input_ids']</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model_new = NewTransformer(cfg, tokenizer)</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model_new.to('cuda:0')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model_old.to('cuda:7').to(torch.float32)</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model_new.to('cuda:7').to(torch.float32)</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="co"># tokens = torch.randint(100, 1000, (10, 1024)).to('cuda:7')</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co"># s = time.time()</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = model_old(tokens)</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co"># print(loss.item())</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="co"># print(time.time() - s)</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co"># s = time.time()</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = model_new(tokens)</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print(loss.item())</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="co"># print(time.time() - s)</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="co"># %timeit model_old(tokens)</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="co"># %timeit model_new(tokens)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># %timeit model_old(tokens)</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="co"># %timeit model_new(tokens)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model_old.to('cuda:5').to(torch.float16)</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model_new.to('cuda:5').to(torch.float16)</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co"># tokens = torch.randint(100, 1000, (10, 1024)).to('cuda:5')</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="co"># s = time.time()</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = model_old(tokens)</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="co"># print(loss.item())</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="co"># print(time.time() - s)</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="co"># s = time.time()</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = model_new(tokens)</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print(loss.item())</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="co"># print(time.time() - s)</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="co"># %timeit model_old(tokens)</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a><span class="co"># %timeit model_new(tokens)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model_old.to('cuda:7')</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model_new_2.to('cuda:7')</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co"># tokens = torch.randint(100, 1000, (10, 1024)).to('cuda:7')</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># s = time.time()</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = model_old(tokens)</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co"># print(loss.item())</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co"># print(time.time() - s)</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co"># s = time.time()</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co"># loss = model_new_2(tokens)</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print(loss.item())</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co"># print(time.time() - s)</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="co"># %timeit model_old(tokens)</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="co"># %timeit model_new_2(tokens)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">False</span>:</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Debugging code:</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    lis <span class="op">=</span> []</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(cfg[<span class="st">'max_steps'</span>]):</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> schedule.step():</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>            lis.append(i)</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>    px.line(lis, log_y<span class="op">=</span><span class="va">True</span>).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>



</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"08cd8519be0a40eea30318f13c6802fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0970794e733a4600bde99bbd47af6d86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"16afb996974d463b923d6843d883036f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fd9c8a71bf54bbab0501e117a8a7668":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"209b37e6fec246f4b27bc5cfa279301e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08cd8519be0a40eea30318f13c6802fa","placeholder":"â€‹","style":"IPY_MODEL_ac3bf66810dd4a448144bff89ef210ef","value":""}},"3d727eff30df4a41822cd353ec0403c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d36d21a6877b4d83ac04aa9ca7a6ef9f","IPY_MODEL_d9335316359049cf9610881fcd4c612d","IPY_MODEL_e57a960343e3432c95ac3f2bf022d6d8"],"layout":"IPY_MODEL_4010279517df474aa1c750ee1e7d3e98"}},"4010279517df474aa1c750ee1e7d3e98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47ee13fcd2b5450abacad9e38a43d407":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49e5e2dbd3844a649af6918fb727d2e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d8f1eaefe924bea971fba060f62dc55":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65d4268c8e1e4c00b6bc2915f368381d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"66c6db320d274bf5b9e626e44dc97d8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6db466e04b094f1ba2e44f090b89754e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7569faa473bc49c7babb8ab8532034d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7db02bba95ad46b9b6a145c88eb9bf74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49e5e2dbd3844a649af6918fb727d2e8","placeholder":"â€‹","style":"IPY_MODEL_47ee13fcd2b5450abacad9e38a43d407","value":" 12/? [02:09&lt;00:00,  5.18s/it]"}},"ac3bf66810dd4a448144bff89ef210ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be21190537f344af8445642cde0dce84":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_209b37e6fec246f4b27bc5cfa279301e","IPY_MODEL_d3ab6deb92674d02b58ce4035e321662","IPY_MODEL_7db02bba95ad46b9b6a145c88eb9bf74"],"layout":"IPY_MODEL_16afb996974d463b923d6843d883036f"}},"c32f287b40534880a800c859ca637412":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d36d21a6877b4d83ac04aa9ca7a6ef9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d8f1eaefe924bea971fba060f62dc55","placeholder":"â€‹","style":"IPY_MODEL_66c6db320d274bf5b9e626e44dc97d8b","value":"Resolving data files: 100%"}},"d3ab6deb92674d02b58ce4035e321662":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_65d4268c8e1e4c00b6bc2915f368381d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7569faa473bc49c7babb8ab8532034d3","value":1}},"d9335316359049cf9610881fcd4c612d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fd9c8a71bf54bbab0501e117a8a7668","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0970794e733a4600bde99bbd47af6d86","value":28}},"e57a960343e3432c95ac3f2bf022d6d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c32f287b40534880a800c859ca637412","placeholder":"â€‹","style":"IPY_MODEL_6db466e04b094f1ba2e44f090b89754e","value":" 28/28 [00:00&lt;00:00,  2.18it/s]"}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>