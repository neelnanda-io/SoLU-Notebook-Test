{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kOvreNmqwXC",
        "outputId": "b2b61efd-53b1-4355-be71-ab11ebce3fec"
      },
      "id": "4kOvreNmqwXC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 13 20:44:27 2022       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA RTX A6000    On   | 00000000:04:00.0 Off |                  Off |\n",
            "| 30%   44C    P5    78W / 300W |      5MiB / 49140MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA RTX A6000    On   | 00000000:06:00.0 Off |                  Off |\n",
            "| 30%   45C    P3    87W / 300W |      5MiB / 49140MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   2  NVIDIA RTX A6000    On   | 00000000:07:00.0 Off |                  Off |\n",
            "| 30%   40C    P5    76W / 300W |      5MiB / 49140MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   3  NVIDIA RTX A6000    On   | 00000000:08:00.0 Off |                  Off |\n",
            "| 30%   45C    P5    95W / 300W |      5MiB / 49140MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   4  NVIDIA RTX A6000    On   | 00000000:0C:00.0 Off |                  Off |\n",
            "| 30%   45C    P5    57W / 300W |      5MiB / 49140MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   5  NVIDIA RTX A6000    On   | 00000000:0D:00.0 Off |                  Off |\n",
            "| 30%   41C    P5    74W / 300W |      5MiB / 49140MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   6  NVIDIA RTX A6000    On   | 00000000:0E:00.0 Off |                  Off |\n",
            "| 30%   48C    P5    72W / 300W |      5MiB / 49140MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   7  NVIDIA RTX A6000    On   | 00000000:0F:00.0 Off |                  Off |\n",
            "| 30%   46C    P5    65W / 300W |      5MiB / 49140MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      2918      G                                       4MiB |\n",
            "|    1   N/A  N/A      2918      G                                       4MiB |\n",
            "|    2   N/A  N/A      2918      G                                       4MiB |\n",
            "|    3   N/A  N/A      2918      G                                       4MiB |\n",
            "|    4   N/A  N/A      2918      G                                       4MiB |\n",
            "|    5   N/A  N/A      2918      G                                       4MiB |\n",
            "|    6   N/A  N/A      2918      G                                       4MiB |\n",
            "|    7   N/A  N/A      2918      G                                       4MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3a74a2b0-1d28-4cb2-bb75-cbb28b81dfa9",
      "metadata": {
        "id": "3a74a2b0-1d28-4cb2-bb75-cbb28b81dfa9"
      },
      "outputs": [],
      "source": [
        "# # !pip install wandb\n",
        "\n",
        "# # !apt-get update\n",
        "# # !su -\n",
        "# # !apt-get install sudo -y\n",
        "# # !apt-get install tmux -y\n",
        "# !pip install einops\n",
        "# !pip install pyyaml==5.4.1\n",
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install matplotlib\n",
        "# !pip install plotly\n",
        "# !pip install zstandard\n",
        "\n",
        "# # import wandb\n",
        "# # wandb.login()\n",
        "# # # !wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "14cbb24a-7b4b-464a-a21d-5fb6b875880e",
      "metadata": {
        "id": "14cbb24a-7b4b-464a-a21d-5fb6b875880e"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "# from google.colab import drive\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from functools import *\n",
        "import pandas as pd\n",
        "import gc\n",
        "import collections\n",
        "import copy\n",
        "\n",
        "# import comet_ml\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import datasets\n",
        "import time\n",
        "import wandb\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1db6b4fa-2530-450d-a5e6-4f9eff31bd48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1db6b4fa-2530-450d-a5e6-4f9eff31bd48",
        "outputId": "a9c06e00-6e77-450e-89af-3b55c2443cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old\n",
            "{'act_fn': 'SoLU',\n",
            " 'attn_only': False,\n",
            " 'batch_size': 104,\n",
            " 'batches_per_step': 3,\n",
            " 'betas': (0.9, 0.99),\n",
            " 'd_head': 64,\n",
            " 'd_model': 1280,\n",
            " 'd_vocab': 50278,\n",
            " 'dataset_name': 'the_pile',\n",
            " 'debug': False,\n",
            " 'debug_batch': False,\n",
            " 'debug_overfit': False,\n",
            " 'factored_embed': False,\n",
            " 'grad_norm_clip': 1.0,\n",
            " 'ln_eps': 1e-05,\n",
            " 'lr': 0.0005,\n",
            " 'lr_schedule': 'cosine_warmup',\n",
            " 'max_tokens': 15000000000,\n",
            " 'n_ctx': 1024,\n",
            " 'n_devices': 8,\n",
            " 'n_layers': 10,\n",
            " 'normalization': 'LN',\n",
            " 'right_multiply_matrices': True,\n",
            " 'save_checkpoints_to_bfloat16': True,\n",
            " 'seed': 14916,\n",
            " 'shuffled_data': True,\n",
            " 'train_loss_ewma_beta': 0.99,\n",
            " 'use_attn_result': False,\n",
            " 'use_bfloat16': False,\n",
            " 'use_bfloat16_matmul': True,\n",
            " 'use_checkpoint_schedule': True,\n",
            " 'use_pos_resid': True,\n",
            " 'version': 22,\n",
            " 'warmup_tokens': 250000000,\n",
            " 'weight_decay': 0.01}\n",
            "\n",
            "{'act_fn': 'SoLU',\n",
            " 'attn_only': False,\n",
            " 'batch_size': 104,\n",
            " 'batches_per_step': 3,\n",
            " 'betas': (0.9, 0.99),\n",
            " 'd_head': 64,\n",
            " 'd_mlp': 5120,\n",
            " 'd_model': 1280,\n",
            " 'd_vocab': 50278,\n",
            " 'dataset_name': 'the_pile',\n",
            " 'debug': False,\n",
            " 'debug_batch': False,\n",
            " 'debug_overfit': False,\n",
            " 'factored_embed': False,\n",
            " 'grad_norm_clip': 1.0,\n",
            " 'ln_eps': 1e-05,\n",
            " 'lr': 0.0005,\n",
            " 'lr_schedule': 'cosine_warmup',\n",
            " 'max_steps': 46950,\n",
            " 'max_tokens': 15000000000,\n",
            " 'n_ctx': 1024,\n",
            " 'n_devices': 8,\n",
            " 'n_heads': 20,\n",
            " 'n_layers': 10,\n",
            " 'normalization': 'LN',\n",
            " 'right_multiply_matrices': True,\n",
            " 'save_checkpoints_to_bfloat16': True,\n",
            " 'seed': 14916,\n",
            " 'shuffled_data': True,\n",
            " 'tokens_per_step': 319488,\n",
            " 'train_loss_ewma_beta': 0.99,\n",
            " 'use_attn_result': False,\n",
            " 'use_bfloat16': False,\n",
            " 'use_bfloat16_matmul': True,\n",
            " 'use_checkpoint_schedule': True,\n",
            " 'use_pos_resid': True,\n",
            " 'version': 22,\n",
            " 'warmup_steps': 782,\n",
            " 'warmup_tokens': 250000000,\n",
            " 'weight_decay': 0.01}\n"
          ]
        }
      ],
      "source": [
        "cfg = {\n",
        "    'd_model':1280,\n",
        "    'n_layers':10,\n",
        "    'lr':5e-4,\n",
        "    'batch_size':13 * torch.cuda.device_count(),\n",
        "    'batches_per_step':3,\n",
        "    'seed':14916,\n",
        "    # 'checkpoint_every_tokens':5*10**7,\n",
        "    'use_checkpoint_schedule':True,\n",
        "    'debug':False,\n",
        "    'debug_batch':False,\n",
        "    'debug_overfit':False,\n",
        "    'normalization':'LN', # 'LN' 'RMS' or None\n",
        "    'max_tokens':15*10**9,\n",
        "    'version':22,\n",
        "    'use_bfloat16':False,\n",
        "    'save_checkpoints_to_bfloat16':True,\n",
        "    'use_bfloat16_matmul':True,\n",
        "    'right_multiply_matrices':True,\n",
        "    # 'n_heads':8,\n",
        "    'd_head':64,\n",
        "    'n_ctx':1024,\n",
        "    'd_vocab':50278,\n",
        "    # 'factor_size':256,\n",
        "    'betas':(0.9, 0.99),\n",
        "    'weight_decay':0.01,\n",
        "    'dataset_name':'the_pile',\n",
        "    'grad_norm_clip':1.0,\n",
        "    'use_attn_result':False,\n",
        "    'n_devices':torch.cuda.device_count(),\n",
        "    'act_fn':'SoLU',\n",
        "    'use_pos_resid':True,\n",
        "    'attn_only':False,\n",
        "    'ln_eps':1e-5,\n",
        "    'lr_schedule': 'cosine_warmup',\n",
        "    'warmup_tokens':25*10**7,\n",
        "    'factored_embed':False,\n",
        "    'train_loss_ewma_beta':0.99,\n",
        "    'shuffled_data':True,\n",
        "    # 'W_O_init_scale':True,\n",
        "}\n",
        "print('Old')\n",
        "pprint(cfg)\n",
        "print()\n",
        "cfg['n_heads'] = cfg['d_model']//cfg['d_head']\n",
        "cfg['d_mlp'] = 4 * cfg['d_model']\n",
        "cfg['tokens_per_step'] = (cfg['batch_size']*cfg['n_ctx']*cfg['batches_per_step'])\n",
        "cfg['max_steps'] = cfg['max_tokens']//cfg['tokens_per_step']\n",
        "cfg['warmup_steps'] = cfg['warmup_tokens']//cfg['tokens_per_step']\n",
        "# cfg['checkpoint_every'] = cfg['checkpoint_every_tokens']//cfg['tokens_per_step']\n",
        "if cfg['debug'] and not cfg['debug_overfit']:\n",
        "    print('Old max steps:', cfg['max_steps'])\n",
        "    cfg['max_steps']=20\n",
        "# cfg['warmup_steps']=cfg['warmup_tokens']//cfg['tokens_per_step']\n",
        "pprint(cfg)\n",
        "torch.manual_seed(cfg['seed'])\n",
        "np.random.seed(cfg['seed'])\n",
        "random.seed(cfg['seed'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Num params: {12*cfg['n_layers']*cfg['d_model']**2:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "429yY2-2V2EG",
        "outputId": "15a8f5ae-b759-48b6-c46a-f8d6d644f9dd"
      },
      "id": "429yY2-2V2EG",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num params: 196,608,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4f3bdf6a-8e43-4bac-aab4-489edfa60b36",
      "metadata": {
        "id": "4f3bdf6a-8e43-4bac-aab4-489edfa60b36"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fbcb638d-8678-4adb-85d1-5ada67cd75c5",
      "metadata": {
        "id": "fbcb638d-8678-4adb-85d1-5ada67cd75c5"
      },
      "outputs": [],
      "source": [
        "def cuda_memory():\n",
        "    print([torch.cuda.memory_allocated(f\"cuda:{i}\")/1e9 for i in range(torch.cuda.device_count())])\n",
        "\n",
        "\n",
        "def get_corner(tensor, n=2):\n",
        "    # Prints the top left corner of the tensor\n",
        "    if len(tensor.shape)==0:\n",
        "        return tensor\n",
        "    elif len(tensor.shape)==1:\n",
        "        return tensor[:n]\n",
        "    elif len(tensor.shape)==2:\n",
        "        return tensor[:n, :n]\n",
        "    elif len(tensor.shape)==3:\n",
        "        return tensor[:n, :n, :n]\n",
        "    elif len(tensor.shape)==4:\n",
        "        return tensor[:n, :n, :n, :n]\n",
        "    elif len(tensor.shape)==5:\n",
        "        return tensor[:n, :n, :n, :n, :n]\n",
        "    elif len(tensor.shape)==6:\n",
        "        return tensor[:n, :n, :n, :n, :n, :n]\n",
        "    else:\n",
        "        # I never need tensors of rank > 6\n",
        "        raise ValueError(f'Tensor of shape {tensor.shape} is too big')\n",
        "\n",
        "def to_numpy(tensor, flat=False):\n",
        "    if (type(tensor)!=torch.Tensor) and (type(tensor)!=torch.nn.parameter.Parameter):\n",
        "        return tensor\n",
        "    if flat:\n",
        "        return tensor.flatten().detach().cpu().numpy()\n",
        "    else:\n",
        "        return tensor.detach().cpu().numpy()\n",
        "\n",
        "def save_to_bfloat16(model, file_name):\n",
        "    sd = model.state_dict()\n",
        "    torch.save({k:v.to(torch.bfloat16) for k, v in sd.items()}, file_name)\n",
        "    print(\"Saved model as bfloat16 to\", file_name)\n",
        "# save_to_bfloat16(model, 'SoLU_3L_testing.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3b38a466-ca0a-444a-9a88-cb8e820e4d29",
      "metadata": {
        "id": "3b38a466-ca0a-444a-9a88-cb8e820e4d29"
      },
      "outputs": [],
      "source": [
        "# A helper class to get access to intermediate activations (inspired by Garcon)\n",
        "# It's a dummy module that is the identity function by default\n",
        "# I can wrap any intermediate activation in a HookPoint and get a convenient \n",
        "# way to add PyTorch hooks\n",
        "class HookPoint(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fwd_hooks = []\n",
        "        self.bwd_hooks = []\n",
        "        self.ctx = {}\n",
        "        \n",
        "        # A variable giving the hook's name (from the perspective of the root \n",
        "        # module) - this is set by the root module at setup.\n",
        "        self.name = None\n",
        "    \n",
        "    def add_hook(self, hook, dir='fwd'):\n",
        "        # Hook format is fn(activation, hook_name)\n",
        "        # Change it into PyTorch hook format (this includes input and output, \n",
        "        # which are the same for a HookPoint)\n",
        "        def full_hook(module, module_input, module_output):\n",
        "            return hook(module_output, hook=self)\n",
        "        if dir=='fwd':\n",
        "            handle = self.register_forward_hook(full_hook)\n",
        "            self.fwd_hooks.append(handle)\n",
        "        elif dir=='bwd':\n",
        "            handle = self.register_full_backward_hook(full_hook)\n",
        "            self.bwd_hooks.append(handle)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "    \n",
        "    def remove_hooks(self, dir='fwd'):\n",
        "        if (dir=='fwd') or (dir=='both'):\n",
        "            for hook in self.fwd_hooks:\n",
        "                hook.remove()\n",
        "            self.fwd_hooks = []\n",
        "        if (dir=='bwd') or (dir=='both'):\n",
        "            for hook in self.bwd_hooks:\n",
        "                hook.remove()\n",
        "            self.bwd_hooks = []\n",
        "        if dir not in ['fwd', 'bwd', 'both']:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "    \n",
        "    def clear_context(self):\n",
        "        del self.ctx\n",
        "        self.ctx = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "    def layer(self):\n",
        "        # Returns the layer index if the name has the form 'blocks.{layer}.{...}'\n",
        "        # Helper function that's mainly useful on EasyTransformer\n",
        "        # If it doesn't have this form, raises an error - \n",
        "        split_name = self.name.split('.')\n",
        "        return int(split_name[1])\n",
        "\n",
        "class HookedRootModule(nn.Module):\n",
        "    # A class building on nn.Module to interface nicely with HookPoints\n",
        "    # Allows you to name each hook, remove hooks, cache every activation/gradient, etc\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "    \n",
        "    def setup_hooks(self):\n",
        "        # Setup function - this needs to be run in __init__ AFTER defining all \n",
        "        # layers\n",
        "        # Add a parameter to each module giving its name\n",
        "        # Build a dictionary mapping a module name to the module\n",
        "        self.mod_dict = {}\n",
        "        self.hook_dict = {}\n",
        "        for name, module in self.named_modules():\n",
        "            module.name = name\n",
        "            self.mod_dict[name] = module\n",
        "            if type(module)==HookPoint:\n",
        "                self.hook_dict[name] = module\n",
        "        \n",
        "    def hook_points(self):\n",
        "        return (self.hook_dict.values())\n",
        "\n",
        "    def remove_all_hook_fns(self, direction='both'):\n",
        "        for hp in self.hook_points():\n",
        "            hp.remove_hooks(direction)\n",
        "    \n",
        "    def clear_contexts(self):\n",
        "        for hp in self.hook_points():\n",
        "            hp.clear_context()\n",
        "    \n",
        "    def reset_hooks(self, clear_contexts=True, direction='both'):\n",
        "        if clear_contexts: self.clear_contexts()\n",
        "        self.remove_all_hook_fns(direction)\n",
        "    \n",
        "    def cache_all(self, cache, incl_bwd=False, device='cuda'):\n",
        "        # Caches all activations wrapped in a HookPoint\n",
        "        def save_hook(tensor, hook):\n",
        "            cache[hook.name] = tensor.detach().to(device)\n",
        "        def save_hook_back(tensor, hook):\n",
        "            cache[hook.name+'_grad'] = tensor[0].detach().to(device)\n",
        "        for hp in self.hook_points():\n",
        "            hp.add_hook(save_hook, 'fwd')\n",
        "            if incl_bwd:\n",
        "                hp.add_hook(save_hook_back, 'bwd')\n",
        "    \n",
        "    def run_with_hooks(self, \n",
        "                       *args, \n",
        "                       fwd_hooks=[], \n",
        "                       bwd_hooks=[], \n",
        "                       reset_hooks_start=True, \n",
        "                       reset_hooks_end=True, \n",
        "                       clear_contexts=False):\n",
        "        '''\n",
        "        fwd_hooks: A list of (name, hook), where name is either the name of \n",
        "        a hook point or a Boolean function on hook names and hook is the \n",
        "        function to add to that hook point, or the hook whose names evaluate \n",
        "        to True respectively. Ditto bwd_hooks\n",
        "        reset_hooks_start (bool): If True, all prior hooks are removed at the start\n",
        "        reset_hooks_end (bool): If True, all hooks are removed at the end (ie, \n",
        "        including those added in this run)\n",
        "        clear_contexts (bool): If True, clears hook contexts whenever hooks are reset\n",
        "        \n",
        "        Note that if we want to use backward hooks, we need to set \n",
        "        reset_hooks_end to be False, so the backward hooks are still there - this function only runs a forward pass.\n",
        "        '''\n",
        "        if reset_hooks_start:\n",
        "            self.reset_hooks(clear_contexts)\n",
        "        for name, hook in fwd_hooks:\n",
        "            if type(name)==str:\n",
        "                self.mod_dict[name].add_hook(hook, dir='fwd')\n",
        "            else:\n",
        "                # Otherwise, name is a Boolean function on names\n",
        "                for hook_name, hp in self.hook_dict.items():\n",
        "                    if name(hook_name):\n",
        "                        hp.add_hook(hook, dir='fwd')\n",
        "        for name, hook in bwd_hooks:\n",
        "            if type(name)==str:\n",
        "                self.mod_dict[name].add_hook(hook, dir='fwd')\n",
        "            else:\n",
        "                # Otherwise, name is a Boolean function on names\n",
        "                for hook_name, hp in self.hook_dict:\n",
        "                    if name(hook_name):\n",
        "                        hp.add_hook(hook, dir='bwd')\n",
        "        out = self.forward(*args)\n",
        "        if reset_hooks_end:\n",
        "            if len(bwd_hooks)>0:\n",
        "                print(\"WARNING: Hooks were reset at the end of run_with_hooks while backward hooks were set.\")\n",
        "                print(\"This removes the backward hooks before a backward pass can occur\")\n",
        "            self.reset_hooks(clear_contexts)\n",
        "        return out\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "aa5092eb-1fb6-40fd-9d50-cb8de287aad1",
      "metadata": {
        "id": "aa5092eb-1fb6-40fd-9d50-cb8de287aad1"
      },
      "outputs": [],
      "source": [
        "def loss_fn(logits, batch):\n",
        "    log_probs = F.log_softmax(logits[:, :-1], dim=-1)\n",
        "    pred_log_probs = torch.gather(log_probs, -1, batch[:, 1:, None])[..., 0]\n",
        "    return -pred_log_probs.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def amp_einsum(einsum_str, mat1, mat2):\n",
        "    # return torch.einsum(einsum_str, mat1, mat2)\n",
        "    # return torch.einsum(einsum_str, mat1.to(torch.bfloat16), mat2.to(torch.bfloat16)).to(torch.float32)\n",
        "    if cfg['use_bfloat16_matmul']:\n",
        "        return torch.einsum(einsum_str, mat1.to(torch.bfloat16), mat2.to(torch.bfloat16)).to(torch.float32)\n",
        "    else:\n",
        "        return torch.einsum(einsum_str, mat1, mat2)"
      ],
      "metadata": {
        "id": "xEJNmsy-rscP"
      },
      "id": "xEJNmsy-rscP",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "po8ZVKbB3NSN"
      },
      "id": "po8ZVKbB3NSN",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "999f9be9-faf4-47c8-afee-6e2afc14fb31",
      "metadata": {
        "id": "999f9be9-faf4-47c8-afee-6e2afc14fb31"
      },
      "outputs": [],
      "source": [
        "# Define network architecture\n",
        "\n",
        "# Embed & Unembed\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_E = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['d_model']))\n",
        "        nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5), mode='fan_out')\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n",
        "        # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n",
        "        # return einops.rearrange(self.W_E[tokens, :], 'd_model batch pos -> batch pos d_model')\n",
        "        return self.W_E[tokens, :]\n",
        "\n",
        "# class FactoredEmbed(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.W_E = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_vocab']))\n",
        "#         self.W_E_factor = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['factor_size']))\n",
        "#         nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5), mode='fan_out')\n",
        "#         nn.init.kaiming_uniform_(self.W_E_factor, a=np.sqrt(5), mode='fan_out')\n",
        "    \n",
        "#     def forward(self, tokens):\n",
        "#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n",
        "#         # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n",
        "#         return einops.rearrange(self.W_E[:, tokens], 'factor batch pos -> batch pos factor') @ self.W_E_factor.T\n",
        "\n",
        "\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_U = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_vocab']))\n",
        "        nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5), mode='fan_out')\n",
        "    \n",
        "    def forward(self, residual):\n",
        "        return amp_einsum('bpm,mv->bpv', residual, self.W_U) # [batch, pos, d_vocab]\n",
        "\n",
        "# class FactoredUnembed(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.W_U = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['factor_size']))\n",
        "#         self.W_U_factor = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_model']))\n",
        "#         nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5), mode='fan_out')\n",
        "#         nn.init.kaiming_uniform_(self.W_U_factor, a=np.sqrt(5), mode='fan_out')\n",
        "    \n",
        "#     def forward(self, residual):\n",
        "#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n",
        "#         # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n",
        "#         return amp_einsum('fm,vf,bpm->bpv', self.W_U_factor, self.W_U, residual) # [batch, pos, d_vocab]\n",
        "\n",
        "# Positional Embeddings\n",
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_pos = nn.Parameter(torch.empty(self.cfg['n_ctx'], self.cfg['d_model'])) \n",
        "        nn.init.kaiming_uniform_(self.W_pos, a=np.sqrt(5), mode='fan_out')\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Output shape [pos, d_model] - will be broadcast along batch dim\n",
        "        return self.W_pos[:x.size(-1), :] # [pos, d_model]\n",
        "\n",
        "class LayerNormPre(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.eps = self.cfg['ln_eps']\n",
        "\n",
        "        # Adds a hook point for the normalization scale factor\n",
        "        self.hook_scale = HookPoint() # [batch, pos]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]\n",
        "        scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n",
        "                                 self.eps).sqrt()) # [batch, pos, 1]\n",
        "        return x / scale\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg, length):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.eps = self.cfg['ln_eps']\n",
        "        self.length = length\n",
        "        self.w = nn.Parameter(torch.ones(length))\n",
        "        self.b = nn.Parameter(torch.zeros(length))\n",
        "\n",
        "        # Adds a hook point for the normalization scale factor\n",
        "        self.hook_scale = HookPoint() # [batch, pos]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]\n",
        "        scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n",
        "                                 self.eps).sqrt()) # [batch, pos, 1]\n",
        "        out = (x / scale) * self.w + self.b\n",
        "        return out\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, cfg, length):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.eps = self.cfg['ln_eps']\n",
        "        self.length = length\n",
        "        self.w = nn.Parameter(torch.ones(length))\n",
        "\n",
        "        # Adds a hook point for the normalization scale factor\n",
        "        self.hook_scale = HookPoint() # [batch, pos]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n",
        "                                 self.eps).sqrt()) # [batch, pos, 1]\n",
        "        out = (x / scale) * self.w\n",
        "        return out\n",
        "\n",
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, cfg, attn_type='global'):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_Q = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_model'], self.cfg['d_head']))\n",
        "        self.b_Q = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n",
        "        nn.init.kaiming_uniform_(self.W_Q, a=np.sqrt(5), mode='fan_out')\n",
        "        self.W_K = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_model'], self.cfg['d_head']))\n",
        "        self.b_K = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n",
        "        nn.init.kaiming_uniform_(self.W_K, a=np.sqrt(5), mode='fan_out')\n",
        "        self.W_V = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_model'], self.cfg['d_head']))\n",
        "        self.b_V = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n",
        "        nn.init.kaiming_uniform_(self.W_V, a=np.sqrt(5), mode='fan_out')\n",
        "        self.W_O = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n",
        "        self.b_O = nn.Parameter(torch.zeros(self.cfg['d_model']))\n",
        "        nn.init.kaiming_uniform_(self.W_O, a=np.sqrt(5), mode='fan_out')\n",
        "        # if cfg['W_O_init_scale']:\n",
        "        #     self.W_O/=np.sqrt(2*self.cfg['n_layers'])\n",
        "        \n",
        "        self.attn_type = attn_type\n",
        "        # Create a query_pos x key_pos mask, with True iff that query position \n",
        "        # can attend to that key position\n",
        "        causal_mask = torch.tril(torch.ones((self.cfg['n_ctx'], self.cfg['n_ctx'])).bool())\n",
        "        self.register_buffer('mask', causal_mask)\n",
        "        \n",
        "        self.register_buffer('IGNORE', torch.tensor(-1e5))\n",
        "        self.attn_scale = np.sqrt(self.cfg['d_head'])\n",
        "        \n",
        "        self.hook_k = HookPoint() # [batch, pos, head_index, d_head]\n",
        "        self.hook_q = HookPoint() # [batch, pos, head_index, d_head]\n",
        "        self.hook_v = HookPoint() # [batch, pos, head_index, d_head]\n",
        "        self.hook_z = HookPoint() # [batch, pos, head_index, d_head]\n",
        "        self.hook_attn_scores = HookPoint() # [batch, head_index, query_pos, key_pos]\n",
        "        self.hook_attn = HookPoint() # [batch, head_index, query_pos, key_pos]\n",
        "        self.hook_result = HookPoint() # [batch, head_index, head_index, d_model]\n",
        "        if not cfg['use_pos_resid']:\n",
        "            self.hook_attn_input = HookPoint()\n",
        "\n",
        "    def forward(self, x, pos_embed):\n",
        "        if not cfg['use_pos_resid']:\n",
        "            attn_input = self.hook_attn_input(x+pos_embed)\n",
        "            q = self.hook_q(amp_einsum('bpm,imh->bpih', attn_input, self.W_Q)+self.b_Q) # [batch, pos, head_index, d_head]\n",
        "            k = self.hook_k(amp_einsum('bpm,imh->bpih', attn_input, self.W_K)+self.b_K) # [batch, pos, head_index, d_head]\n",
        "        else:\n",
        "            q = self.hook_q(amp_einsum('bpm,imh->bpih', x, self.W_Q)+self.b_Q) # [batch, pos, head_index, d_head]\n",
        "            k = self.hook_k(amp_einsum('bpm,imh->bpih', x, self.W_K)+self.b_K) # [batch, pos, head_index, d_head]\n",
        "\n",
        "        v = self.hook_v(amp_einsum('bpm,imh->bpih', x, self.W_V)+self.b_V) # [batch, pos, head_index, d_head]\n",
        "        attn_scores = amp_einsum('bpih,bqih->bipq', q, k)/self.attn_scale # [batch, head_index, query_pos, key_pos]\n",
        "        attn_scores = self.hook_attn_scores(self.apply_causal_mask(attn_scores)) # [batch, head_index, query_pos, key_pos]\n",
        "        attn_matrix = self.hook_attn(F.softmax(attn_scores, dim=-1)) # [batch, head_index, query_pos, key_pos]\n",
        "        z = self.hook_z(amp_einsum('bpih,biqp->bqih', v, attn_matrix)) # [batch, pos, head_index, d_head]\n",
        "        \n",
        "        if cfg['use_attn_result']:\n",
        "            result = self.hook_result(amp_einsum('bqih,ihm->bqim', z, self.W_O)) # [batch, pos, head_index, d_model]\n",
        "            out = einops.reduce(result, \n",
        "                            'batch position index model->batch position model', \n",
        "                            'sum')+self.b_O  # [batch, pos, d_model]\n",
        "        else:\n",
        "            out = (amp_einsum('bqih,ihm->bqm', z, self.W_O)+self.b_O) # [batch, pos, head_index, d_model]\n",
        "        return out\n",
        "    \n",
        "    def apply_causal_mask(self, attn_scores):\n",
        "        return torch.where(self.mask[:attn_scores.size(-2), :attn_scores.size(-1)], attn_scores, self.IGNORE)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_in = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_mlp']))\n",
        "        nn.init.kaiming_uniform_(self.W_in, a=np.sqrt(5), mode='fan_out')\n",
        "        self.b_in = nn.Parameter(torch.zeros(self.cfg['d_mlp']))\n",
        "        self.W_out = nn.Parameter(torch.empty(self.cfg['d_mlp'], self.cfg['d_model']))\n",
        "        nn.init.kaiming_uniform_(self.W_out, a=np.sqrt(5), mode='fan_out')\n",
        "        self.b_out = nn.Parameter(torch.zeros(self.cfg['d_model']))\n",
        "\n",
        "        self.hook_pre = HookPoint() # [batch, pos, d_mlp]\n",
        "        self.hook_post = HookPoint() # [batch, pos, d_mlp]\n",
        "\n",
        "        if self.cfg['act_fn'].lower()=='relu':\n",
        "            self.act_fn = F.relu\n",
        "        elif self.cfg['act_fn'].lower()=='gelu_new':\n",
        "            self.act_fn = gelu_new\n",
        "        elif self.cfg['act_fn'].lower()=='solu':\n",
        "            self.act_fn = lambda x: F.softmax(x, dim=-1)*x\n",
        "            self.hook_post_ln = HookPoint() # [batch, pos, d_mlp]\n",
        "            self.ln = LayerNorm(self.cfg, self.cfg['d_mlp'])\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid activation function name: {self.cfg['act_fn']}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hook_pre(amp_einsum('bpd,dm->bpm', x, self.W_in) + self.b_in) # [batch, pos, d_mlp]\n",
        "        x = self.hook_post(self.act_fn(x)) # [batch, pos, d_mlp]\n",
        "        if self.cfg['act_fn'].lower()=='solu':\n",
        "            x = self.hook_post_ln(self.ln(x))\n",
        "        x = amp_einsum('bpm,md->bpd', x, self.W_out) + self.b_out # [batch, pos, d_model]\n",
        "        return x\n",
        "\n",
        "# Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg, block_index):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        if self.cfg['normalization']=='RMS':\n",
        "            self.norm1 = LayerNorm(self.cfg, self.cfg['d_model'])\n",
        "            self.norm2 = LayerNorm(self.cfg, self.cfg['d_model'])\n",
        "        elif self.cfg['normalization']=='LN':\n",
        "            self.norm1 = LayerNorm(self.cfg, self.cfg['d_model'])\n",
        "            self.norm2 = LayerNorm(self.cfg, self.cfg['d_model'])\n",
        "        self.attn = Attention(self.cfg)\n",
        "        self.mlp = MLP(self.cfg)\n",
        "\n",
        "        self.hook_attn_out = HookPoint() # [batch, pos, d_model]\n",
        "        self.hook_mlp_out = HookPoint() # [batch, pos, d_model]\n",
        "        # Note that resid_pre of layer k+1 is resid_post of layer k - given for convenience\n",
        "        self.hook_resid_pre = HookPoint() # [batch, pos, d_model]\n",
        "        self.hook_resid_mid = HookPoint() # [batch, pos, d_model]\n",
        "        self.hook_resid_post = HookPoint() # [batch, pos, d_model]\n",
        "    \n",
        "    def forward(self, x, pos_embed):\n",
        "        resid_pre = self.hook_resid_pre(x) # [batch, pos, d_model]\n",
        "        if self.cfg['normalization'] is not None:\n",
        "            attn_out = self.hook_attn_out(self.attn(self.norm1(resid_pre), pos_embed)) # [batch, pos, d_model]\n",
        "        else:\n",
        "            attn_out = self.hook_attn_out(self.attn(resid_pre, pos_embed)) # [batch, pos, d_model]\n",
        "        resid_mid = self.hook_resid_mid(resid_pre + attn_out) # [batch, pos, d_model]\n",
        "        if self.cfg['normalization'] is not None:\n",
        "            mlp_out = self.hook_mlp_out(self.mlp(self.norm2(resid_mid))) # [batch, pos, d_model]\n",
        "        else:\n",
        "            mlp_out = self.hook_mlp_out(self.mlp(resid_mid)) # [batch, pos, d_model]\n",
        "        resid_post = self.hook_resid_post(resid_mid + mlp_out) # [batch, pos, d_model]\n",
        "        return resid_post\n",
        "\n",
        "# Full transformer\n",
        "class Transformer(HookedRootModule):\n",
        "    def __init__(self, cfg, tokenizer):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.cfg = cfg\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "        if self.cfg['factored_embed']:\n",
        "            self.embed = FactoredEmbed(self.cfg)\n",
        "        else:\n",
        "            self.embed = Embed(self.cfg)\n",
        "        self.hook_embed = HookPoint() # [batch, pos, d_model]\n",
        "        \n",
        "        self.pos_embed = PosEmbed(self.cfg)\n",
        "        self.hook_pos_embed = HookPoint() # [batch, pos, d_model]\n",
        "\n",
        "        if cfg['normalization']=='RMS':\n",
        "            self.norm = RMSNorm(self.cfg, self.cfg['d_model'])\n",
        "        elif cfg['normalization']=='LN':\n",
        "            self.norm = LayerNorm(self.cfg, self.cfg['d_model'])\n",
        "            \n",
        "        self.blocks = nn.ModuleList([TransformerBlock(self.cfg, block_index) for block_index in range(self.cfg['n_layers'])])\n",
        "\n",
        "        if self.cfg['factored_embed']:\n",
        "            self.unembed = FactoredUnembed(self.cfg)\n",
        "        else:\n",
        "            self.unembed = Unembed(self.cfg)\n",
        "\n",
        "        # Gives each module a parameter with its name (relative to this root module)\n",
        "        # Needed for HookPoints to work\n",
        "        self.setup_hooks()\n",
        "            \n",
        "    def forward(self, tokens, return_loss=True):\n",
        "        # Input x is either a batch of tokens ([batch, pos]) or a text string\n",
        "        # if type(x)==str:\n",
        "        #     # If text, convert to tokens (batch_size=1)\n",
        "        #     x = self.to_tokens(x)\n",
        "        embed = self.hook_embed(self.embed(tokens)) # [batch, pos, d_model]\n",
        "        pos_embed = self.hook_pos_embed(self.pos_embed(tokens)) # [batch, pos, d_model]\n",
        "        if cfg['use_pos_resid']:\n",
        "            residual = embed + pos_embed # [batch, pos, d_model]\n",
        "        else:\n",
        "            residual = embed # [batch, pos, d_model]\n",
        "        for block in self.blocks:\n",
        "            # Note that each block includes skip connections, so we don't need\n",
        "            # residual + block(residual)\n",
        "            residual = block(residual, pos_embed) # [batch, pos, d_model]\n",
        "        if self.cfg['normalization'] is not None:\n",
        "            residual = self.norm(residual)\n",
        "        logits = self.unembed(residual) # [batch, pos, d_vocab]\n",
        "        if return_loss:\n",
        "            return loss_fn(logits, tokens)\n",
        "        else:\n",
        "            return logits\n",
        "    \n",
        "    def to_tokens(self, text):\n",
        "        return self.tokenizer(text, return_tensors='pt')['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Block\n",
        "class AttnOnlyBlock(nn.Module):\n",
        "    def __init__(self, cfg, block_index):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.attn = Attention(cfg)\n",
        "\n",
        "        self.hook_attn_out = HookPoint() # [batch, pos, d_model]\n",
        "        # Note that resid_pre of layer k+1 is resid_post of layer k - given for convenience\n",
        "        self.hook_resid_pre = HookPoint() # [batch, pos, d_model]\n",
        "        self.hook_resid_post = HookPoint() # [batch, pos, d_model]\n",
        "    \n",
        "    def forward(self, x, pos_embed):\n",
        "        resid_pre = self.hook_resid_pre(x) # [batch, pos, d_model]\n",
        "        attn_out = self.hook_attn_out(self.attn(x, pos_embed)) # [batch, pos, d_model]\n",
        "        resid_post = self.hook_resid_post(resid_pre + attn_out) # [batch, pos, d_model]\n",
        "        return resid_post\n",
        "        \n",
        "# Full transformer\n",
        "class AttnOnlyTransformer(HookedRootModule):\n",
        "    def __init__(self, cfg, tokenizer):\n",
        "        raise NotImplementedError(\"Need to add LN support etc\")\n",
        "        super().__init__()\n",
        "        \n",
        "        self.cfg = cfg\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "        self.embed = Embed(self.cfg)\n",
        "        self.hook_embed = HookPoint() # [batch, pos, d_model]\n",
        "        \n",
        "        self.pos_embed = PosEmbed(self.cfg)\n",
        "        self.hook_pos_embed = HookPoint() # [batch, pos, d_model]\n",
        "        \n",
        "        self.blocks = nn.ModuleList([AttnOnlyBlock(self.cfg, block_index) for block_index in range(self.cfg['n_layers'])])\n",
        "        self.unembed = Unembed(self.cfg)\n",
        "\n",
        "        # Gives each module a parameter with its name (relative to this root module)\n",
        "        # Needed for HookPoints to work\n",
        "        self.setup_hooks()\n",
        "            \n",
        "    def forward(self, tokens, return_loss=True):\n",
        "        # Input x is either a batch of tokens ([batch, pos]) or a text string\n",
        "        # if type(x)==str:\n",
        "        #     # If text, convert to tokens (batch_size=1)\n",
        "        #     x = self.to_tokens(x)\n",
        "        embed = self.hook_embed(self.embed(tokens)) # [batch, pos, d_model]\n",
        "        pos_embed = self.hook_pos_embed(self.pos_embed(tokens)) # [batch, pos, d_model]\n",
        "        residual = embed # [batch, pos, d_model]\n",
        "        for block in self.blocks:\n",
        "            # Note that each block includes skip connections, so we don't need\n",
        "            # residual + block(residual)\n",
        "            residual = block(residual, pos_embed) # [batch, pos, d_model]\n",
        "        logits = self.unembed(residual) # [batch, pos, d_vocab]\n",
        "        if return_loss:\n",
        "            return loss_fn(logits, tokens)\n",
        "        else:\n",
        "            return logits\n",
        "    \n",
        "    def to_tokens(self, text):\n",
        "        return self.tokenizer(text, return_tensors='pt')['input_ids']\n"
      ],
      "metadata": {
        "id": "2D6HinTIfotN"
      },
      "id": "2D6HinTIfotN",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f85fe1d5-6759-495f-b8c1-1b34ba451785",
      "metadata": {
        "id": "f85fe1d5-6759-495f-b8c1-1b34ba451785",
        "outputId": "93f82d37-9209-4180-d300-f5aebdd35fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-neox-20b', vocab_size=50254, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<PAD>'})\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\n",
        "pad_token = '<PAD>'\n",
        "tokenizer.add_special_tokens({'pad_token':pad_token})\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if cfg['attn_only']:\n",
        "    model = AttnOnlyTransformer(cfg, tokenizer)\n",
        "else:\n",
        "    model = Transformer(cfg, tokenizer)\n",
        "model.to('cuda')\n",
        "if cfg['use_bfloat16']:\n",
        "    model.to(torch.bfloat16)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr=cfg['lr'], \n",
        "                              betas=cfg['betas'], \n",
        "                              weight_decay=cfg['weight_decay'])\n",
        "if cfg['lr_schedule'] is not None:\n",
        "    def lr_schedule(step):\n",
        "        if step<cfg['warmup_steps']:\n",
        "            return (1e-7+(cfg['lr']-1e-7)*step/cfg['warmup_steps'])/cfg['lr']\n",
        "        else:\n",
        "            return (0.55 + 0.9*0.5*np.cos(np.pi*(step-cfg['warmup_steps'])/(cfg['max_steps'] - cfg['warmup_steps'])))\n",
        "    param_groups = {'decay':[], 'no_decay':[]}\n",
        "    for name, param in model.named_parameters():\n",
        "        print(name)\n",
        "        if 'W_' in name and name not in ['W_E', 'W_U']:\n",
        "            param_groups['decay'].append(param)\n",
        "        else:\n",
        "            param_groups['no_decay'].append(param)\n",
        "    optim_groups = [\n",
        "                {\"params\": param_groups['decay'], \"weight_decay\": cfg['weight_decay']},\n",
        "                {\"params\": param_groups['no_decay'], \"weight_decay\": 0.0},\n",
        "            ]\n",
        "    optimizer = torch.optim.AdamW(optim_groups, lr=cfg['lr'])\n",
        "    print(optimizer)\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_schedule)\n",
        "    # px.line(y=[lr_schedule(i) for i in range(cfg['max_steps'])]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5Blow0D2diJ",
        "outputId": "ddf64897-14cb-4a0b-88f6-fabb92e1bb84"
      },
      "id": "c5Blow0D2diJ",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed.W_E\n",
            "pos_embed.W_pos\n",
            "norm.w\n",
            "norm.b\n",
            "blocks.0.norm1.w\n",
            "blocks.0.norm1.b\n",
            "blocks.0.norm2.w\n",
            "blocks.0.norm2.b\n",
            "blocks.0.attn.W_Q\n",
            "blocks.0.attn.b_Q\n",
            "blocks.0.attn.W_K\n",
            "blocks.0.attn.b_K\n",
            "blocks.0.attn.W_V\n",
            "blocks.0.attn.b_V\n",
            "blocks.0.attn.W_O\n",
            "blocks.0.attn.b_O\n",
            "blocks.0.mlp.W_in\n",
            "blocks.0.mlp.b_in\n",
            "blocks.0.mlp.W_out\n",
            "blocks.0.mlp.b_out\n",
            "blocks.0.mlp.ln.w\n",
            "blocks.0.mlp.ln.b\n",
            "blocks.1.norm1.w\n",
            "blocks.1.norm1.b\n",
            "blocks.1.norm2.w\n",
            "blocks.1.norm2.b\n",
            "blocks.1.attn.W_Q\n",
            "blocks.1.attn.b_Q\n",
            "blocks.1.attn.W_K\n",
            "blocks.1.attn.b_K\n",
            "blocks.1.attn.W_V\n",
            "blocks.1.attn.b_V\n",
            "blocks.1.attn.W_O\n",
            "blocks.1.attn.b_O\n",
            "blocks.1.mlp.W_in\n",
            "blocks.1.mlp.b_in\n",
            "blocks.1.mlp.W_out\n",
            "blocks.1.mlp.b_out\n",
            "blocks.1.mlp.ln.w\n",
            "blocks.1.mlp.ln.b\n",
            "blocks.2.norm1.w\n",
            "blocks.2.norm1.b\n",
            "blocks.2.norm2.w\n",
            "blocks.2.norm2.b\n",
            "blocks.2.attn.W_Q\n",
            "blocks.2.attn.b_Q\n",
            "blocks.2.attn.W_K\n",
            "blocks.2.attn.b_K\n",
            "blocks.2.attn.W_V\n",
            "blocks.2.attn.b_V\n",
            "blocks.2.attn.W_O\n",
            "blocks.2.attn.b_O\n",
            "blocks.2.mlp.W_in\n",
            "blocks.2.mlp.b_in\n",
            "blocks.2.mlp.W_out\n",
            "blocks.2.mlp.b_out\n",
            "blocks.2.mlp.ln.w\n",
            "blocks.2.mlp.ln.b\n",
            "blocks.3.norm1.w\n",
            "blocks.3.norm1.b\n",
            "blocks.3.norm2.w\n",
            "blocks.3.norm2.b\n",
            "blocks.3.attn.W_Q\n",
            "blocks.3.attn.b_Q\n",
            "blocks.3.attn.W_K\n",
            "blocks.3.attn.b_K\n",
            "blocks.3.attn.W_V\n",
            "blocks.3.attn.b_V\n",
            "blocks.3.attn.W_O\n",
            "blocks.3.attn.b_O\n",
            "blocks.3.mlp.W_in\n",
            "blocks.3.mlp.b_in\n",
            "blocks.3.mlp.W_out\n",
            "blocks.3.mlp.b_out\n",
            "blocks.3.mlp.ln.w\n",
            "blocks.3.mlp.ln.b\n",
            "blocks.4.norm1.w\n",
            "blocks.4.norm1.b\n",
            "blocks.4.norm2.w\n",
            "blocks.4.norm2.b\n",
            "blocks.4.attn.W_Q\n",
            "blocks.4.attn.b_Q\n",
            "blocks.4.attn.W_K\n",
            "blocks.4.attn.b_K\n",
            "blocks.4.attn.W_V\n",
            "blocks.4.attn.b_V\n",
            "blocks.4.attn.W_O\n",
            "blocks.4.attn.b_O\n",
            "blocks.4.mlp.W_in\n",
            "blocks.4.mlp.b_in\n",
            "blocks.4.mlp.W_out\n",
            "blocks.4.mlp.b_out\n",
            "blocks.4.mlp.ln.w\n",
            "blocks.4.mlp.ln.b\n",
            "blocks.5.norm1.w\n",
            "blocks.5.norm1.b\n",
            "blocks.5.norm2.w\n",
            "blocks.5.norm2.b\n",
            "blocks.5.attn.W_Q\n",
            "blocks.5.attn.b_Q\n",
            "blocks.5.attn.W_K\n",
            "blocks.5.attn.b_K\n",
            "blocks.5.attn.W_V\n",
            "blocks.5.attn.b_V\n",
            "blocks.5.attn.W_O\n",
            "blocks.5.attn.b_O\n",
            "blocks.5.mlp.W_in\n",
            "blocks.5.mlp.b_in\n",
            "blocks.5.mlp.W_out\n",
            "blocks.5.mlp.b_out\n",
            "blocks.5.mlp.ln.w\n",
            "blocks.5.mlp.ln.b\n",
            "blocks.6.norm1.w\n",
            "blocks.6.norm1.b\n",
            "blocks.6.norm2.w\n",
            "blocks.6.norm2.b\n",
            "blocks.6.attn.W_Q\n",
            "blocks.6.attn.b_Q\n",
            "blocks.6.attn.W_K\n",
            "blocks.6.attn.b_K\n",
            "blocks.6.attn.W_V\n",
            "blocks.6.attn.b_V\n",
            "blocks.6.attn.W_O\n",
            "blocks.6.attn.b_O\n",
            "blocks.6.mlp.W_in\n",
            "blocks.6.mlp.b_in\n",
            "blocks.6.mlp.W_out\n",
            "blocks.6.mlp.b_out\n",
            "blocks.6.mlp.ln.w\n",
            "blocks.6.mlp.ln.b\n",
            "blocks.7.norm1.w\n",
            "blocks.7.norm1.b\n",
            "blocks.7.norm2.w\n",
            "blocks.7.norm2.b\n",
            "blocks.7.attn.W_Q\n",
            "blocks.7.attn.b_Q\n",
            "blocks.7.attn.W_K\n",
            "blocks.7.attn.b_K\n",
            "blocks.7.attn.W_V\n",
            "blocks.7.attn.b_V\n",
            "blocks.7.attn.W_O\n",
            "blocks.7.attn.b_O\n",
            "blocks.7.mlp.W_in\n",
            "blocks.7.mlp.b_in\n",
            "blocks.7.mlp.W_out\n",
            "blocks.7.mlp.b_out\n",
            "blocks.7.mlp.ln.w\n",
            "blocks.7.mlp.ln.b\n",
            "blocks.8.norm1.w\n",
            "blocks.8.norm1.b\n",
            "blocks.8.norm2.w\n",
            "blocks.8.norm2.b\n",
            "blocks.8.attn.W_Q\n",
            "blocks.8.attn.b_Q\n",
            "blocks.8.attn.W_K\n",
            "blocks.8.attn.b_K\n",
            "blocks.8.attn.W_V\n",
            "blocks.8.attn.b_V\n",
            "blocks.8.attn.W_O\n",
            "blocks.8.attn.b_O\n",
            "blocks.8.mlp.W_in\n",
            "blocks.8.mlp.b_in\n",
            "blocks.8.mlp.W_out\n",
            "blocks.8.mlp.b_out\n",
            "blocks.8.mlp.ln.w\n",
            "blocks.8.mlp.ln.b\n",
            "blocks.9.norm1.w\n",
            "blocks.9.norm1.b\n",
            "blocks.9.norm2.w\n",
            "blocks.9.norm2.b\n",
            "blocks.9.attn.W_Q\n",
            "blocks.9.attn.b_Q\n",
            "blocks.9.attn.W_K\n",
            "blocks.9.attn.b_K\n",
            "blocks.9.attn.W_V\n",
            "blocks.9.attn.b_V\n",
            "blocks.9.attn.W_O\n",
            "blocks.9.attn.b_O\n",
            "blocks.9.mlp.W_in\n",
            "blocks.9.mlp.b_in\n",
            "blocks.9.mlp.W_out\n",
            "blocks.9.mlp.b_out\n",
            "blocks.9.mlp.ln.w\n",
            "blocks.9.mlp.ln.b\n",
            "unembed.W_U\n",
            "AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    lr: 0.0005\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    lr: 0.0005\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "426d2d52-2d3f-475c-a8ce-fd133b18699b",
      "metadata": {
        "id": "426d2d52-2d3f-475c-a8ce-fd133b18699b"
      },
      "outputs": [],
      "source": [
        "# if cfg['attn_only']:\n",
        "#     model = AttnOnlyTransformer(cfg, tokenizer)\n",
        "# else:\n",
        "#     model = Transformer(cfg, tokenizer)\n",
        "# model.to('cuda')\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), \n",
        "#                               lr=cfg['lr'], \n",
        "#                               betas=cfg['betas'], \n",
        "#                               weight_decay=cfg['weight_decay'])\n",
        "# if cfg['lr_schedule'] is not None:\n",
        "#     # print(\"Using scheduler:\" scheduler)\n",
        "#     scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=cfg['warmup_steps'], num_training_steps=cfg['max_steps'])\n",
        "#     print(\"Using scheduler:\", scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1a5d3412-7e8c-4434-844f-cbfad3bcecf8",
      "metadata": {
        "id": "1a5d3412-7e8c-4434-844f-cbfad3bcecf8"
      },
      "outputs": [],
      "source": [
        "if cfg['debug'] and cfg['debug_batch']:\n",
        "    parallel_model = torch.nn.DataParallel(model, \n",
        "                        device_ids=list(range(torch.cuda.device_count())))\n",
        "    \n",
        "    for batch_size in range(12, 128, 2):\n",
        "    # for batch_size in range(48, 128, 4):\n",
        "    # for batch_size in [64, 64, 64, 64, 64, 64]:\n",
        "        start_time=time.time()\n",
        "        print()\n",
        "        print('New Batch!', batch_size)\n",
        "        cuda_memory()\n",
        "        for i in range(2):\n",
        "            batch = torch.randint(100, 2000, (6*batch_size, cfg['n_ctx']))\n",
        "            loss = parallel_model(batch).mean()\n",
        "            loss.backward()\n",
        "            print('Finished run', i, batch_size, batch.shape)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        cuda_memory()\n",
        "        try:\n",
        "            del loss\n",
        "        except:\n",
        "            print('Deleting loss failed')\n",
        "        torch.cuda.empty_cache()\n",
        "        cuda_memory()\n",
        "        print('Time:', time.time() - start_time)\n",
        "    raise ValueError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "189fd076-c4ff-41b8-b420-e61a650b4f93",
      "metadata": {
        "id": "189fd076-c4ff-41b8-b420-e61a650b4f93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230,
          "referenced_widgets": [
            "3d727eff30df4a41822cd353ec0403c5",
            "d36d21a6877b4d83ac04aa9ca7a6ef9f",
            "d9335316359049cf9610881fcd4c612d",
            "e57a960343e3432c95ac3f2bf022d6d8",
            "4010279517df474aa1c750ee1e7d3e98",
            "5d8f1eaefe924bea971fba060f62dc55",
            "66c6db320d274bf5b9e626e44dc97d8b",
            "1fd9c8a71bf54bbab0501e117a8a7668",
            "0970794e733a4600bde99bbd47af6d86",
            "c32f287b40534880a800c859ca637412",
            "6db466e04b094f1ba2e44f090b89754e"
          ]
        },
        "outputId": "60ef3c09-855e-4b47-f595-b15872fdf7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Permutation of PILE URLs [17  5  6  8  9 25 18 13 14 27 26 20  2 24 10  0  7 12  4  3  1 19 16 23\n",
            " 15 22 11 21]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d727eff30df4a41822cd353ec0403c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default-5e370f4de25e8bde\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded! 0.853412389755249\n",
            "Loaded! 0.0002193450927734375\n",
            "dataset.map 0.00016236305236816406\n",
            "dataset.set_format 0.0033745765686035156\n",
            "dataset.shuffle 0.0008072853088378906\n",
            "train_data_loader = 0.0004334449768066406\n"
          ]
        }
      ],
      "source": [
        "seq_len = cfg['n_ctx']\n",
        "def tokenize(examples):\n",
        "    start_time = time.time()\n",
        "    texts = examples['text']\n",
        "    full_text = tokenizer.eos_token.join(texts)\n",
        "    div = 20\n",
        "    length = len(full_text)//div\n",
        "    text_list = [full_text[i*length:(i+1)*length] for i in range(div)]\n",
        "    tokens = tokenizer(text_list, return_tensors='np', padding=True)['input_ids'].flatten()\n",
        "    tokens = tokens[tokens!=tokenizer.pad_token_id]\n",
        "    # print(len(text_list), len(text_list[0]))\n",
        "    # print(tokens.shape)\n",
        "    n = len(tokens)\n",
        "    curr_batch_size = n//(seq_len-1)\n",
        "    tokens = tokens[:(seq_len-1)*curr_batch_size]\n",
        "    tokens = einops.rearrange(tokens, '(batch_size seq) -> batch_size seq', batch_size=curr_batch_size, seq=seq_len-1)\n",
        "    prefix = np.ones((curr_batch_size, 1), dtype=np.int64)*tokenizer.bos_token_id\n",
        "    # print(tokens.shape, n, curr_batch_size, seq_len)\n",
        "    return {'text': np.concatenate([prefix, tokens], axis=1)}# tiny_owt_orig_2 = load_dataset('stas/openwebtext-10k', cache_dir='./cache', split='train', download_config=datasets.DownloadConfig(resume_download=True, num_proc=4))\n",
        "import time\n",
        "\n",
        "if not cfg['debug']:\n",
        "    start_time = time.time()\n",
        "    if cfg['shuffled_data']:\n",
        "        randperm = np.random.permutation(28)\n",
        "        print('Permutation of PILE URLs', randperm)\n",
        "        pile_urls = [f\"https://mystic.the-eye.eu/public/AI/pile/train/{i:0>2}.jsonl.zst\" for i in randperm]\n",
        "        dataset = load_dataset('json', data_files=pile_urls, streaming=True, split='train')\n",
        "    else:\n",
        "        dataset = load_dataset(cfg['dataset_name'], streaming=True, split='train')\n",
        "    print('Loaded!', time.time()-start_time)\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        dataset = dataset.remove_columns('meta')\n",
        "    except:\n",
        "        print('Meta not in dataset')\n",
        "    print('Loaded!', time.time()-start_time)\n",
        "    start_time = time.time()\n",
        "    dataset = dataset.map(tokenize, batched=True)\n",
        "    print('dataset.map', time.time()-start_time)\n",
        "    start_time = time.time()\n",
        "    dataset = dataset.with_format(type='torch')\n",
        "    print('dataset.set_format', time.time()-start_time)\n",
        "    start_time = time.time()\n",
        "    dataset = dataset.shuffle(seed=cfg['seed'], buffer_size=30000)\n",
        "    print('dataset.shuffle', time.time()-start_time)\n",
        "    start_time = time.time()\n",
        "    train_data_loader = DataLoader(dataset, batch_size=cfg['batch_size'])\n",
        "    print('train_data_loader =', time.time()-start_time)\n",
        "else:\n",
        "    streaming_owt = load_dataset('stas/openwebtext-10k', split='train', cache_dir='cache')\n",
        "    streaming_owt = streaming_owt.map(tokenize, batched=True, num_proc=10)\n",
        "    streaming_owt = streaming_owt.with_format(type='torch')\n",
        "    train_data_loader = DataLoader(streaming_owt, batch_size=cfg['batch_size'], shuffle=True)\n",
        "    start_time = time.time()\n",
        "    for c, i in tqdm.tqdm(enumerate(train_data_loader)):\n",
        "        if c == 0:\n",
        "            print(\"Loaded Initial stream!\")\n",
        "            print(c, time.time() - start_time)\n",
        "            start_time = time.time()\n",
        "        elif c==1:\n",
        "            print('Time for next batch:', time.time() - start_time)\n",
        "            break\n",
        "data_iter = iter(train_data_loader)\n",
        "# tiny_owt_orig_2 = load_dataset('stas/openwebtext-10k', cache_dir='./cache', split='train', download_config=datasets.DownloadConfig(resume_download=True, num_proc=4))\n",
        "# print('Loaded!')\n",
        "# # tokenizer.add_special_tokens({'pad_token':'<PAD>'})\n",
        "# tiny_owt = tiny_owt_orig.map(tokenize, batched=True)\n",
        "# print('Tokenized!')\n",
        "# tiny_owt_2 = tiny_owt_orig_2.map(tokenize, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(model)\n",
        "model_name = f'SoLU_{cfg[\"n_layers\"]}L_v{cfg[\"version\"]}'\n",
        "print(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRk_voMYlkGF",
        "outputId": "9e096a14-7a38-4cc9-acc0-1410061bebb5"
      },
      "id": "KRk_voMYlkGF",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (embed): Embed()\n",
            "  (hook_embed): HookPoint()\n",
            "  (pos_embed): PosEmbed()\n",
            "  (hook_pos_embed): HookPoint()\n",
            "  (norm): LayerNorm(\n",
            "    (hook_scale): HookPoint()\n",
            "  )\n",
            "  (blocks): ModuleList(\n",
            "    (0): TransformerBlock(\n",
            "      (norm1): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (norm2): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_attn): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "        (hook_post_ln): HookPoint()\n",
            "        (ln): LayerNorm(\n",
            "          (hook_scale): HookPoint()\n",
            "        )\n",
            "      )\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (norm1): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (norm2): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_attn): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "        (hook_post_ln): HookPoint()\n",
            "        (ln): LayerNorm(\n",
            "          (hook_scale): HookPoint()\n",
            "        )\n",
            "      )\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (norm1): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (norm2): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_attn): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "        (hook_post_ln): HookPoint()\n",
            "        (ln): LayerNorm(\n",
            "          (hook_scale): HookPoint()\n",
            "        )\n",
            "      )\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "    (3): TransformerBlock(\n",
            "      (norm1): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (norm2): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_attn): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "        (hook_post_ln): HookPoint()\n",
            "        (ln): LayerNorm(\n",
            "          (hook_scale): HookPoint()\n",
            "        )\n",
            "      )\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "    (4): TransformerBlock(\n",
            "      (norm1): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (norm2): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_attn): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "        (hook_post_ln): HookPoint()\n",
            "        (ln): LayerNorm(\n",
            "          (hook_scale): HookPoint()\n",
            "        )\n",
            "      )\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "    (5): TransformerBlock(\n",
            "      (norm1): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (norm2): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_attn): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "        (hook_post_ln): HookPoint()\n",
            "        (ln): LayerNorm(\n",
            "          (hook_scale): HookPoint()\n",
            "        )\n",
            "      )\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "    (6): TransformerBlock(\n",
            "      (norm1): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (norm2): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_attn): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "        (hook_post_ln): HookPoint()\n",
            "        (ln): LayerNorm(\n",
            "          (hook_scale): HookPoint()\n",
            "        )\n",
            "      )\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "    (7): TransformerBlock(\n",
            "      (norm1): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (norm2): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_attn): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "        (hook_post_ln): HookPoint()\n",
            "        (ln): LayerNorm(\n",
            "          (hook_scale): HookPoint()\n",
            "        )\n",
            "      )\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "    (8): TransformerBlock(\n",
            "      (norm1): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (norm2): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_attn): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "        (hook_post_ln): HookPoint()\n",
            "        (ln): LayerNorm(\n",
            "          (hook_scale): HookPoint()\n",
            "        )\n",
            "      )\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "    (9): TransformerBlock(\n",
            "      (norm1): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (norm2): LayerNorm(\n",
            "        (hook_scale): HookPoint()\n",
            "      )\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_attn): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (hook_pre): HookPoint()\n",
            "        (hook_post): HookPoint()\n",
            "        (hook_post_ln): HookPoint()\n",
            "        (ln): LayerNorm(\n",
            "          (hook_scale): HookPoint()\n",
            "        )\n",
            "      )\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_mid): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "  )\n",
            "  (unembed): Unembed()\n",
            ")\n",
            "SoLU_10L_v22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f01bf331-e796-4bea-96d1-92231ab47e91",
      "metadata": {
        "id": "f01bf331-e796-4bea-96d1-92231ab47e91"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "02f2cf44-da54-4fc5-a366-31f6a3c28bfe",
      "metadata": {
        "id": "02f2cf44-da54-4fc5-a366-31f6a3c28bfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73900005-2dbc-4a6e-bd5e-456282f6a731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.340122112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "parallel_model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
        "cuda_memory()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SaveSchedule():\n",
        "    def __init__(self, max_tokens, tokens_per_step, schedule=None):\n",
        "        if schedule is None:\n",
        "            self.schedule = np.concatenate([np.arange(10)/10*1e-3, np.arange(2, 20)/20*1e-2, np.arange(5, 50)/50*1e-1, np.arange(10, 101)/100])\n",
        "        else:\n",
        "            self.schedule = schedule\n",
        "        self.max_tokens = max_tokens\n",
        "        self.tokens_per_step = tokens_per_step\n",
        "        self.counter = 0\n",
        "        self.next_save_point = 0\n",
        "        px.line(self.schedule * max_tokens, log_y=True, title='Save Schedule', labels={\"y\":\"Tokens\", \"x\":\"Checkpoint Index\"}).show()\n",
        "    \n",
        "    def step(self):\n",
        "        value = self.counter * self.tokens_per_step / self.max_tokens\n",
        "        threshold = self.schedule[self.next_save_point]\n",
        "        if value >= threshold:\n",
        "            self.next_save_point+=1\n",
        "            self.counter+=1\n",
        "            return True\n",
        "        else:\n",
        "            self.counter+=1\n",
        "            return False\n",
        "schedule = SaveSchedule(cfg['max_tokens'], cfg['tokens_per_step'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "98orZsBGgt0d",
        "outputId": "8f86cf69-b9d5-41af-972a-599fa727845a"
      },
      "id": "98orZsBGgt0d",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.14.0.min.js\"></script>                <div id=\"bc9565a3-8bd8-45b2-8d53-337f3c982a8a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bc9565a3-8bd8-45b2-8d53-337f3c982a8a\")) {                    Plotly.newPlot(                        \"bc9565a3-8bd8-45b2-8d53-337f3c982a8a\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163],\"xaxis\":\"x\",\"y\":[0.0,1500000.0,3000000.0,4500000.0,6000000.0,7500000.0,9000000.0,10500000.0,12000000.0,13500000.000000002,15000000.0,22500000.0,30000000.0,37500000.0,45000000.0,52499999.99999999,60000000.0,67500000.00000001,75000000.0,82500000.00000001,90000000.0,97500000.00000001,104999999.99999999,112500000.0,120000000.0,127500000.00000001,135000000.00000003,142500000.0,150000000.00000003,180000000.0,210000000.00000003,240000000.0,270000000.0,300000000.00000006,330000000.00000006,360000000.0,390000000.00000006,420000000.00000006,450000000.0,480000000.0,510000000.00000006,540000000.0,570000000.0000001,600000000.0000001,630000000.0,660000000.0000001,690000000.0000001,720000000.0,750000000.0,780000000.0000001,810000000.0000001,840000000.0000001,869999999.9999999,900000000.0,930000000.0,960000000.0,990000000.0,1020000000.0000001,1049999999.9999999,1080000000.0,1110000000.0,1140000000.0000002,1170000000.0000002,1200000000.0000002,1230000000.0,1260000000.0,1290000000.0,1320000000.0000002,1350000000.0000002,1380000000.0000002,1410000000.0,1440000000.0,1470000000.0,1500000000.0,1650000000.0,1800000000.0,1950000000.0,2100000000.0000002,2250000000.0,2400000000.0,2550000000.0,2700000000.0,2850000000.0,3000000000.0,3150000000.0,3300000000.0,3450000000.0,3600000000.0,3750000000.0,3900000000.0,4050000000.0000005,4200000000.0000005,4350000000.0,4500000000.0,4650000000.0,4800000000.0,4950000000.0,5100000000.0,5250000000.0,5400000000.0,5550000000.0,5700000000.0,5850000000.0,6000000000.0,6150000000.0,6300000000.0,6450000000.0,6600000000.0,6750000000.0,6900000000.0,7050000000.0,7200000000.0,7350000000.0,7500000000.0,7650000000.0,7800000000.0,7950000000.0,8100000000.000001,8250000000.000001,8400000000.000001,8549999999.999999,8700000000.0,8850000000.0,9000000000.0,9150000000.0,9300000000.0,9450000000.0,9600000000.0,9750000000.0,9900000000.0,10050000000.0,10200000000.0,10350000000.0,10500000000.0,10650000000.0,10800000000.0,10950000000.0,11100000000.0,11250000000.0,11400000000.0,11550000000.0,11700000000.0,11850000000.0,12000000000.0,12150000000.0,12300000000.0,12450000000.0,12600000000.0,12750000000.0,12900000000.0,13050000000.0,13200000000.0,13350000000.0,13500000000.0,13650000000.0,13800000000.0,13950000000.0,14100000000.0,14250000000.0,14400000000.0,14550000000.0,14700000000.0,14850000000.0,15000000000.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"},\"type\":\"log\"},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Save Schedule\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bc9565a3-8bd8-45b2-8d53-337f3c982a8a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2161ac50-fa20-4ea3-8628-e71d89544a11",
      "metadata": {
        "id": "2161ac50-fa20-4ea3-8628-e71d89544a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "dce3cfea-7bc5-4ec0-a716-d5a4723f8268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mneelnanda-io\u001b[0m (\u001b[33mmechanistic-interpretability\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.13.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/workspace/wandb/run-20220913_204442-ve2e5pjs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/mechanistic-interpretability/solu/runs/ve2e5pjs\" target=\"_blank\">jumping-sky-91</a></strong> to <a href=\"https://wandb.ai/mechanistic-interpretability/solu\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/mechanistic-interpretability/solu/runs/ve2e5pjs?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f245b6d5a90>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "wandb.init(project=\"solu\", entity=\"mechanistic-interpretability\", config=cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3253ca00-b2ee-4883-844d-561d3fbbcb97",
      "metadata": {
        "id": "3253ca00-b2ee-4883-844d-561d3fbbcb97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "be21190537f344af8445642cde0dce84",
            "209b37e6fec246f4b27bc5cfa279301e",
            "d3ab6deb92674d02b58ce4035e321662",
            "7db02bba95ad46b9b6a145c88eb9bf74",
            "16afb996974d463b923d6843d883036f",
            "08cd8519be0a40eea30318f13c6802fa",
            "ac3bf66810dd4a448144bff89ef210ef",
            "65d4268c8e1e4c00b6bc2915f368381d",
            "7569faa473bc49c7babb8ab8532034d3",
            "49e5e2dbd3844a649af6918fb727d2e8",
            "47ee13fcd2b5450abacad9e38a43d407"
          ]
        },
        "outputId": "df947bbf-77eb-470f-df5d-8487a6f1f4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'act_fn': 'SoLU',\n",
            " 'attn_only': False,\n",
            " 'batch_size': 104,\n",
            " 'batches_per_step': 3,\n",
            " 'betas': (0.9, 0.99),\n",
            " 'd_head': 64,\n",
            " 'd_mlp': 5120,\n",
            " 'd_model': 1280,\n",
            " 'd_vocab': 50278,\n",
            " 'dataset_name': 'the_pile',\n",
            " 'debug': False,\n",
            " 'debug_batch': False,\n",
            " 'debug_overfit': False,\n",
            " 'factored_embed': False,\n",
            " 'grad_norm_clip': 1.0,\n",
            " 'ln_eps': 1e-05,\n",
            " 'lr': 0.0005,\n",
            " 'lr_schedule': 'cosine_warmup',\n",
            " 'max_steps': 46950,\n",
            " 'max_tokens': 15000000000,\n",
            " 'n_ctx': 1024,\n",
            " 'n_devices': 8,\n",
            " 'n_heads': 20,\n",
            " 'n_layers': 10,\n",
            " 'normalization': 'LN',\n",
            " 'right_multiply_matrices': True,\n",
            " 'save_checkpoints_to_bfloat16': True,\n",
            " 'seed': 14916,\n",
            " 'shuffled_data': True,\n",
            " 'tokens_per_step': 319488,\n",
            " 'train_loss_ewma_beta': 0.99,\n",
            " 'use_attn_result': False,\n",
            " 'use_bfloat16': False,\n",
            " 'use_bfloat16_matmul': True,\n",
            " 'use_checkpoint_schedule': True,\n",
            " 'use_pos_resid': True,\n",
            " 'version': 22,\n",
            " 'warmup_steps': 782,\n",
            " 'warmup_tokens': 250000000,\n",
            " 'weight_decay': 0.01}\n",
            "Training begins!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be21190537f344af8445642cde0dce84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n",
            "\n",
            "Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.649573888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 0 90.26232886314392\n",
            "[2.649573888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 1 2.3085339069366455\n",
            "Saved the model! Step: 0. Frac of way through training: 0.0\n",
            "Saved model as bfloat16 to SoLU_10L_v22_000000.pth\n",
            "2 0 319488 10.982126871744791 9.019821268717449\n",
            "[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 2 8.920480012893677\n",
            "[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 3 2.3054752349853516\n",
            "[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 4 2.3048253059387207\n",
            "[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 5 2.3655030727386475\n",
            "[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 6 2.2996890544891357\n",
            "[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 7 2.301769256591797\n",
            "[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 8 2.362489938735962\n",
            "[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 9 2.303229808807373\n",
            "[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 10 2.3018174171447754\n",
            "[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Early iteration complete! 11 9.724807262420654\n"
          ]
        }
      ],
      "source": [
        "pprint(cfg)\n",
        "# DataLoader(full_owt_test['text'], batch_size=cfg['batch_size'], shuffle=False, pin_memory=False)\n",
        "print('Training begins!')\n",
        "losses = []\n",
        "loss_ewmas=[]\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "loss_ewma = 9\n",
        "# loss_beta = 0.95\n",
        "total_tokens = 0\n",
        "running_loss = 0\n",
        "prev_time=time.time()\n",
        "epoch=0\n",
        "# for epoch in range(100):\n",
        "for c, batch in tqdm.tqdm(enumerate(data_iter)):\n",
        "    batch = batch['text']\n",
        "    if cfg['debug'] and epoch==0 and c<3:\n",
        "        print(batch[0])\n",
        "        print(tokenizer.decode(batch[0]))\n",
        "    batch = batch.cuda()\n",
        "    loss = parallel_model(batch).mean()\n",
        "    loss.backward()\n",
        "    running_loss+=loss.item()\n",
        "    total_tokens += batch.numel()\n",
        "    if (c+1)%cfg['batches_per_step'] == 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['grad_norm_clip'])\n",
        "        optimizer.step()\n",
        "        if cfg['lr_schedule'] is not None:\n",
        "            scheduler.step()\n",
        "            wandb.log({'scheduled_lr':scheduler.get_last_lr()[0]}, step=step)\n",
        "        optimizer.zero_grad()\n",
        "        if schedule.step() and cfg['use_checkpoint_schedule']:\n",
        "            print(f'Saved the model! Step: {step}. Frac of way through training: {schedule.schedule[schedule.next_save_point-1]}')\n",
        "            if not cfg['debug']:\n",
        "                if cfg['save_checkpoints_to_bfloat16']:\n",
        "                    save_to_bfloat16(model, f'{model_name}_{step:0>6}.pth')\n",
        "                else:\n",
        "                    torch.save(model.state_dict(), f'{model_name}_{step:0>6}.pth')\n",
        "                torch.save(optimizer.state_dict(), f'{model_name}_opt_checkpoint.pth')\n",
        "                if cfg['lr_schedule'] is not None:\n",
        "                    torch.save(scheduler.state_dict(), f'{model_name}_scheduler_checkpoint.pth')\n",
        "                wandb.save(f'{model_name}_{step:0>6}.pth')\n",
        "        running_loss = running_loss / cfg['batches_per_step']\n",
        "        losses.append(running_loss)\n",
        "\n",
        "        loss_ewma = loss_ewma * cfg['train_loss_ewma_beta'] + running_loss * (1 - cfg['train_loss_ewma_beta'])\n",
        "        loss_ewmas.append(loss_ewma)\n",
        "        wandb.log({\"loss\": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c}, step=step)\n",
        "        # print('Just logged')\n",
        "        # print({\"loss\": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c})\n",
        "        running_loss = 0\n",
        "        if step % 30 == 0:\n",
        "            print(c, step, total_tokens, losses[-1], loss_ewmas[-1])\n",
        "        step+=1\n",
        "        if step>=cfg['max_steps']:\n",
        "            break\n",
        "    if c<=12 and epoch==0:\n",
        "        cuda_memory()\n",
        "        print('Early iteration complete!', c, time.time()-prev_time)\n",
        "        prev_time=time.time()\n",
        "    del loss\n",
        "    # print(batch.shape, logits.shape, running_loss, loss, step, total_tokens)\n",
        "    # if not cfg['debug_overfit']:\n",
        "    #     break\n",
        "\n",
        "print(f'Finished training! Train Loss EWMA: {loss_ewma}')\n",
        "\n",
        "if not cfg['debug']:\n",
        "    torch.save(model.state_dict(), f'{model_name}_final.pth')\n",
        "    wandb.save(f'{model_name}_final.pth')\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pprint(cfg)\n",
        "# # DataLoader(full_owt_test['text'], batch_size=cfg['batch_size'], shuffle=False, pin_memory=False)\n",
        "# print('Training begins!')\n",
        "# losses = []\n",
        "# loss_ewmas=[]\n",
        "# step = 0\n",
        "# start_time = time.time()\n",
        "# loss_ewma = 9\n",
        "# loss_beta = 0.95\n",
        "# total_tokens = 0\n",
        "# running_loss = 0\n",
        "# prev_time=time.time()\n",
        "# epoch=0\n",
        "# # for epoch in range(100):\n",
        "# for epoch in range(100):\n",
        "#     data_iter = iter(train_data_loader)\n",
        "#     for c, batch in tqdm.tqdm(enumerate(data_iter)):\n",
        "#         batch = batch['text']\n",
        "#         if cfg['debug'] and epoch==0 and c<3:\n",
        "#             print(batch[0])\n",
        "#             print(tokenizer.decode(batch[0]))\n",
        "#         batch = batch.cuda()\n",
        "#         loss = parallel_model(batch).mean()\n",
        "#         loss.backward()\n",
        "#         running_loss+=loss.item()\n",
        "#         total_tokens += batch.numel()\n",
        "#         if (c+1)%cfg['batches_per_step'] == 0:\n",
        "#             torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['grad_norm_clip'])\n",
        "#             optimizer.step()\n",
        "#             if cfg['lr_schedule'] is not None:\n",
        "#                 scheduler.step()\n",
        "#                 wandb.log({'scheduled_lr':scheduler.get_last_lr()[0]}, step=step)\n",
        "#             optimizer.zero_grad()\n",
        "#             if step % cfg['checkpoint_every'] == 0:\n",
        "#                 print(f'Saved the model! Step: {step}')\n",
        "#                 if not cfg['debug']:\n",
        "#                     torch.save(model.state_dict(), f'{model_name}_{step}.pth')\n",
        "#                     torch.save(optimizer.state_dict(), f'{model_name}_opt_checkpoint.pth')\n",
        "#                     if cfg['lr_schedule'] is not None:\n",
        "#                         torch.save(scheduler.state_dict(), f'{model_name}_scheduler_checkpoint.pth')\n",
        "#                     wandb.save(f'{model_name}_{step}.pth')\n",
        "#             running_loss = running_loss / cfg['batches_per_step']\n",
        "#             losses.append(running_loss)\n",
        "\n",
        "#             loss_ewma = loss_ewma * loss_beta + running_loss * (1 - loss_beta)\n",
        "#             loss_ewmas.append(loss_ewma)\n",
        "#             wandb.log({\"loss\": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c}, step=step)\n",
        "#             # print('Just logged')\n",
        "#             # print({\"loss\": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c})\n",
        "#             running_loss = 0\n",
        "#             if step % 30 == 0:\n",
        "#                 print(c, step, total_tokens, losses[-1], loss_ewmas[-1])\n",
        "#             step+=1\n",
        "#             # if step>=cfg['max_steps']:\n",
        "#             #     break\n",
        "#         if c<=12 and epoch==0:\n",
        "#             cuda_memory()\n",
        "#             print('Early iteration complete!', c, time.time()-prev_time)\n",
        "#             prev_time=time.time()\n",
        "#         # print(batch.shape, logits.shape, running_loss, loss, step, total_tokens)\n",
        "#         # if not cfg['debug_overfit']:\n",
        "#         #     break\n",
        "\n",
        "# print(f'Finished training! Train Loss EWMA: {loss_ewma}')\n",
        "\n",
        "# if not cfg['debug']:\n",
        "#     torch.save(model.state_dict(), f'{model_name}_final.pth')\n",
        "#     wandb.save(f'{model_name}_final.pth')\n",
        "# wandb.finish()"
      ],
      "metadata": {
        "id": "BzVeLq94pK70"
      },
      "id": "BzVeLq94pK70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model)\n",
        "# print(parallel_model)"
      ],
      "metadata": {
        "id": "hOkyrmfHXhUd"
      },
      "id": "hOkyrmfHXhUd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer_2 = torch.optim.AdamW(model.parameters(), \n",
        "#                               lr=3e-4, \n",
        "#                               betas=cfg['betas'], \n",
        "#                               weight_decay=cfg['weight_decay'])\n",
        "# optimizer_2.load_state_dict(optimizer.state_dict())"
      ],
      "metadata": {
        "id": "1CkLr8_4XoDw"
      },
      "id": "1CkLr8_4XoDw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(optimizer_2)"
      ],
      "metadata": {
        "id": "Vd3CZgEgXx_O"
      },
      "id": "Vd3CZgEgXx_O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(optimizer_2.state_dict(), \"_\"+model_name+\"_opt_midflight_checkpoint.pth\")\n",
        "# torch.save(model.state_dict(), \"_\"+model_name+\"_model_midflight_checkpoint.pth\")\n",
        "# wandb.save(\"_\"+model_name+\"_model_midflight_checkpoint.pth\")\n",
        "# wandb.save(\"_\"+model_name+\"_opt_midflight_checkpoint.pth\")"
      ],
      "metadata": {
        "id": "ezWmZwY8X0NQ"
      },
      "id": "ezWmZwY8X0NQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sd = optimizer_2.state_dict()"
      ],
      "metadata": {
        "id": "mfptRtZRX1lk"
      },
      "id": "mfptRtZRX1lk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sd['param_groups'][0]['lr']=2e-4"
      ],
      "metadata": {
        "id": "EFx3WBQCY4f_"
      },
      "id": "EFx3WBQCY4f_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _ = optimizer_2.load_state_dict(sd)"
      ],
      "metadata": {
        "id": "5PVO7pz3Y5D8"
      },
      "id": "5PVO7pz3Y5D8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(optimizer_2)"
      ],
      "metadata": {
        "id": "s6s00j_wZBOC"
      },
      "id": "s6s00j_wZBOC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dA8XpsUEZCSu"
      },
      "id": "dA8XpsUEZCSu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import datasets\n",
        "\n",
        "# files = '29.jsonl.zst'\n",
        "# pile_dataset = datasets.load_dataset('json', data_files=files, split='train', cache_dir='cache')\n",
        "# print(pile_dataset)\n",
        "# pile_dataset = pile_dataset.remove_columns('meta')"
      ],
      "metadata": {
        "id": "_2y9Q4IzcdGd"
      },
      "id": "_2y9Q4IzcdGd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pile_dataset = pile_dataset.map(tokenize, batched=True, num_proc=30)\n",
        "# pile_dataset = pile_dataset.with_format(type='torch')\n"
      ],
      "metadata": {
        "id": "1qJ3nZ0LeyqN"
      },
      "id": "1qJ3nZ0LeyqN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data_loader = DataLoader(pile_dataset, batch_size=cfg['batch_size'], shuffle=True, num_workers=10)"
      ],
      "metadata": {
        "id": "KDzhgYByjcwQ"
      },
      "id": "KDzhgYByjcwQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_old = Transformer(cfg, tokenizer)\n",
        "# model_old.to('cuda')"
      ],
      "metadata": {
        "id": "SiFLiFW3qOvj"
      },
      "id": "SiFLiFW3qOvj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Old transformer, left mult\n",
        "# # Define network architecture\n",
        "\n",
        "# # Embed & Unembed\n",
        "# class Embed(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.W_E = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_vocab']))\n",
        "#         nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5))\n",
        "    \n",
        "#     def forward(self, tokens):\n",
        "#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n",
        "#         # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n",
        "#         return einops.rearrange(self.W_E[:, tokens], 'd_model batch pos -> batch pos d_model')\n",
        "\n",
        "# class FactoredEmbed(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.W_E = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_vocab']))\n",
        "#         self.W_E_factor = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['factor_size']))\n",
        "#         nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5))\n",
        "#         nn.init.kaiming_uniform_(self.W_E_factor, a=np.sqrt(5))\n",
        "    \n",
        "#     def forward(self, tokens):\n",
        "#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n",
        "#         # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n",
        "#         return einops.rearrange(self.W_E[:, tokens], 'factor batch pos -> batch pos factor') @ self.W_E_factor.T\n",
        "\n",
        "\n",
        "# class Unembed(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.W_U = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['d_model']))\n",
        "#         nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5))\n",
        "    \n",
        "#     def forward(self, residual):\n",
        "#         return amp_einsum('vm,bpm->bpv', self.W_U, residual) # [batch, pos, d_vocab]\n",
        "\n",
        "# class FactoredUnembed(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.W_U = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['factor_size']))\n",
        "#         self.W_U_factor = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_model']))\n",
        "#         nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5))\n",
        "#         nn.init.kaiming_uniform_(self.W_U_factor, a=np.sqrt(5))\n",
        "    \n",
        "#     def forward(self, residual):\n",
        "#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n",
        "#         # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n",
        "#         return amp_einsum('fm,vf,bpm->bpv', self.W_U_factor, self.W_U, residual) # [batch, pos, d_vocab]\n",
        "\n",
        "# # Positional Embeddings\n",
        "# class PosEmbed(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.W_pos = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['n_ctx'])) \n",
        "#         nn.init.kaiming_uniform_(self.W_pos, a=np.sqrt(5))\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         # Output shape [pos, d_model] - will be broadcast along batch dim\n",
        "#         return self.W_pos[:, :x.size(-1)].T # [pos, d_model]\n",
        "\n",
        "# class LayerNormPre(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.eps = self.cfg['ln_eps']\n",
        "\n",
        "#         # Adds a hook point for the normalization scale factor\n",
        "#         self.hook_scale = HookPoint() # [batch, pos]\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]\n",
        "#         scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n",
        "#                                  self.eps).sqrt()) # [batch, pos, 1]\n",
        "#         return x / scale\n",
        "\n",
        "# class LayerNorm(nn.Module):\n",
        "#     def __init__(self, cfg, length):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.eps = self.cfg['ln_eps']\n",
        "#         self.length = length\n",
        "#         self.w = nn.Parameter(torch.ones(length))\n",
        "#         self.b = nn.Parameter(torch.zeros(length))\n",
        "\n",
        "#         # Adds a hook point for the normalization scale factor\n",
        "#         self.hook_scale = HookPoint() # [batch, pos]\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]\n",
        "#         scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n",
        "#                                  self.eps).sqrt()) # [batch, pos, 1]\n",
        "#         out = (x / scale) * self.w + self.b\n",
        "#         return out\n",
        "\n",
        "# class RMSNorm(nn.Module):\n",
        "#     def __init__(self, cfg, length):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.eps = self.cfg['ln_eps']\n",
        "#         self.length = length\n",
        "#         self.w = nn.Parameter(torch.ones(length))\n",
        "\n",
        "#         # Adds a hook point for the normalization scale factor\n",
        "#         self.hook_scale = HookPoint() # [batch, pos]\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n",
        "#                                  self.eps).sqrt()) # [batch, pos, 1]\n",
        "#         out = (x / scale) * self.w\n",
        "#         return out\n",
        "\n",
        "# # Attention\n",
        "# class Attention(nn.Module):\n",
        "#     def __init__(self, cfg, attn_type='global'):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.W_Q = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n",
        "#         self.b_Q = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n",
        "#         nn.init.kaiming_uniform_(self.W_Q, a=np.sqrt(5))\n",
        "#         self.W_K = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n",
        "#         self.b_K = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n",
        "#         nn.init.kaiming_uniform_(self.W_K, a=np.sqrt(5))\n",
        "#         self.W_V = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n",
        "#         self.b_V = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n",
        "#         nn.init.kaiming_uniform_(self.W_V, a=np.sqrt(5))\n",
        "#         self.W_O = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_model'], self.cfg['d_head']))\n",
        "#         self.b_O = nn.Parameter(torch.zeros(self.cfg['d_model']))\n",
        "#         nn.init.kaiming_uniform_(self.W_O, a=np.sqrt(5))\n",
        "#         # if cfg['W_O_init_scale']:\n",
        "#         #     self.W_O/=np.sqrt(2*self.cfg['n_layers'])\n",
        "        \n",
        "#         self.attn_type = attn_type\n",
        "#         # Create a query_pos x key_pos mask, with True iff that query position \n",
        "#         # can attend to that key position\n",
        "#         causal_mask = torch.tril(torch.ones((self.cfg['n_ctx'], self.cfg['n_ctx'])).bool())\n",
        "#         self.register_buffer('mask', causal_mask)\n",
        "        \n",
        "#         self.register_buffer('IGNORE', torch.tensor(-1e5))\n",
        "#         self.attn_scale = np.sqrt(self.cfg['d_head'])\n",
        "        \n",
        "#         self.hook_k = HookPoint() # [batch, pos, head_index, d_head]\n",
        "#         self.hook_q = HookPoint() # [batch, pos, head_index, d_head]\n",
        "#         self.hook_v = HookPoint() # [batch, pos, head_index, d_head]\n",
        "#         self.hook_z = HookPoint() # [batch, pos, head_index, d_head]\n",
        "#         self.hook_attn_scores = HookPoint() # [batch, head_index, query_pos, key_pos]\n",
        "#         self.hook_attn = HookPoint() # [batch, head_index, query_pos, key_pos]\n",
        "#         self.hook_result = HookPoint() # [batch, head_index, head_index, d_model]\n",
        "#         if not cfg['use_pos_resid']:\n",
        "#             self.hook_attn_input = HookPoint()\n",
        "\n",
        "#     def forward(self, x, pos_embed):\n",
        "#         if not cfg['use_pos_resid']:\n",
        "#             attn_input = self.hook_attn_input(x+pos_embed)\n",
        "#             q = self.hook_q(amp_einsum('ihm,bpm->bpih', self.W_Q, attn_input)+self.b_Q) # [batch, pos, head_index, d_head]\n",
        "#             k = self.hook_k(amp_einsum('ihm,bpm->bpih', self.W_K, attn_input)+self.b_K) # [batch, pos, head_index, d_head]\n",
        "#         else:\n",
        "#             q = self.hook_q(amp_einsum('ihm,bpm->bpih', self.W_Q, x)+self.b_Q) # [batch, pos, head_index, d_head]\n",
        "#             k = self.hook_k(amp_einsum('ihm,bpm->bpih', self.W_K, x)+self.b_K) # [batch, pos, head_index, d_head]\n",
        "\n",
        "#         v = self.hook_v(amp_einsum('ihm,bpm->bpih', self.W_V, x)+self.b_V) # [batch, pos, head_index, d_head]\n",
        "#         attn_scores = amp_einsum('bpih,bqih->bipq', q, k)/self.attn_scale # [batch, head_index, query_pos, key_pos]\n",
        "#         attn_scores = self.hook_attn_scores(self.apply_causal_mask(attn_scores)) # [batch, head_index, query_pos, key_pos]\n",
        "#         attn_matrix = self.hook_attn(F.softmax(attn_scores, dim=-1)) # [batch, head_index, query_pos, key_pos]\n",
        "#         z = self.hook_z(amp_einsum('bpih,biqp->bqih', v, attn_matrix)) # [batch, pos, head_index, d_head]\n",
        "        \n",
        "#         if cfg['use_attn_result']:\n",
        "#             result = self.hook_result(amp_einsum('imh,bqih->bqim', self.W_O, z)) # [batch, pos, head_index, d_model]\n",
        "#             out = einops.reduce(result, \n",
        "#                             'batch position index model->batch position model', \n",
        "#                             'sum')+self.b_O  # [batch, pos, d_model]\n",
        "#         else:\n",
        "#             out = (amp_einsum('imh,bqih->bqm', self.W_O, z)+self.b_O) # [batch, pos, head_index, d_model]\n",
        "#         return out\n",
        "    \n",
        "#     def apply_causal_mask(self, attn_scores):\n",
        "#         return torch.where(self.mask[:attn_scores.size(-2), :attn_scores.size(-1)], attn_scores, self.IGNORE)\n",
        "\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         self.W_in = nn.Parameter(torch.empty(self.cfg['d_mlp'], self.cfg['d_model']))\n",
        "#         nn.init.kaiming_uniform_(self.W_in, a=np.sqrt(5))\n",
        "#         self.b_in = nn.Parameter(torch.zeros(self.cfg['d_mlp']))\n",
        "#         self.W_out = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_mlp']))\n",
        "#         nn.init.kaiming_uniform_(self.W_out, a=np.sqrt(5))\n",
        "#         self.b_out = nn.Parameter(torch.zeros(self.cfg['d_model']))\n",
        "\n",
        "#         self.hook_pre = HookPoint() # [batch, pos, d_mlp]\n",
        "#         self.hook_post = HookPoint() # [batch, pos, d_mlp]\n",
        "\n",
        "#         if self.cfg['act_fn'].lower()=='relu':\n",
        "#             self.act_fn = F.relu\n",
        "#         elif self.cfg['act_fn'].lower()=='gelu_new':\n",
        "#             self.act_fn = gelu_new\n",
        "#         elif self.cfg['act_fn'].lower()=='solu':\n",
        "#             self.act_fn = lambda x: F.softmax(x, dim=-1)*x\n",
        "#             self.hook_post_ln = HookPoint() # [batch, pos, d_mlp]\n",
        "#             self.ln = LayerNorm(self.cfg, self.cfg['d_mlp'])\n",
        "#         else:\n",
        "#             raise ValueError(f\"Invalid activation function name: {self.cfg['act_fn']}\")\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.hook_pre(amp_einsum('md,bpd->bpm', self.W_in, x) + self.b_in) # [batch, pos, d_mlp]\n",
        "#         x = self.hook_post(self.act_fn(x)) # [batch, pos, d_mlp]\n",
        "#         if self.cfg['act_fn'].lower()=='solu':\n",
        "#             x = self.hook_post_ln(self.ln(x))\n",
        "#         x = amp_einsum('dm,bpm->bpd', self.W_out, x) + self.b_out # [batch, pos, d_model]\n",
        "#         return x\n",
        "\n",
        "# # Transformer Block\n",
        "# class TransformerBlock(nn.Module):\n",
        "#     def __init__(self, cfg, block_index):\n",
        "#         super().__init__()\n",
        "#         self.cfg = cfg\n",
        "#         if self.cfg['normalization']=='RMS':\n",
        "#             self.norm1 = LayerNorm(self.cfg, self.cfg['d_model'])\n",
        "#             self.norm2 = LayerNorm(self.cfg, self.cfg['d_model'])\n",
        "#         elif self.cfg['normalization']=='LN':\n",
        "#             self.norm1 = LayerNorm(self.cfg, self.cfg['d_model'])\n",
        "#             self.norm2 = LayerNorm(self.cfg, self.cfg['d_model'])\n",
        "#         self.attn = Attention(self.cfg)\n",
        "#         self.mlp = MLP(self.cfg)\n",
        "\n",
        "#         self.hook_attn_out = HookPoint() # [batch, pos, d_model]\n",
        "#         self.hook_mlp_out = HookPoint() # [batch, pos, d_model]\n",
        "#         # Note that resid_pre of layer k+1 is resid_post of layer k - given for convenience\n",
        "#         self.hook_resid_pre = HookPoint() # [batch, pos, d_model]\n",
        "#         self.hook_resid_mid = HookPoint() # [batch, pos, d_model]\n",
        "#         self.hook_resid_post = HookPoint() # [batch, pos, d_model]\n",
        "    \n",
        "#     def forward(self, x, pos_embed):\n",
        "#         resid_pre = self.hook_resid_pre(x) # [batch, pos, d_model]\n",
        "#         if self.cfg['normalization'] is not None:\n",
        "#             attn_out = self.hook_attn_out(self.attn(self.norm1(resid_pre), pos_embed)) # [batch, pos, d_model]\n",
        "#         else:\n",
        "#             attn_out = self.hook_attn_out(self.attn(resid_pre, pos_embed)) # [batch, pos, d_model]\n",
        "#         resid_mid = self.hook_resid_mid(resid_pre + attn_out) # [batch, pos, d_model]\n",
        "#         if self.cfg['normalization'] is not None:\n",
        "#             mlp_out = self.hook_mlp_out(self.mlp(self.norm2(resid_mid))) # [batch, pos, d_model]\n",
        "#         else:\n",
        "#             mlp_out = self.hook_mlp_out(self.mlp(resid_mid)) # [batch, pos, d_model]\n",
        "#         resid_post = self.hook_resid_post(resid_mid + mlp_out) # [batch, pos, d_model]\n",
        "#         return resid_post\n",
        "\n",
        "# # Full transformer\n",
        "# class Transformer(HookedRootModule):\n",
        "#     def __init__(self, cfg, tokenizer):\n",
        "#         super().__init__()\n",
        "        \n",
        "#         self.cfg = cfg\n",
        "#         self.tokenizer = tokenizer\n",
        "        \n",
        "#         if self.cfg['factored_embed']:\n",
        "#             self.embed = FactoredEmbed(self.cfg)\n",
        "#         else:\n",
        "#             self.embed = Embed(self.cfg)\n",
        "#         self.hook_embed = HookPoint() # [batch, pos, d_model]\n",
        "        \n",
        "#         self.pos_embed = PosEmbed(self.cfg)\n",
        "#         self.hook_pos_embed = HookPoint() # [batch, pos, d_model]\n",
        "\n",
        "#         if cfg['normalization']=='RMS':\n",
        "#             self.norm = RMSNorm(self.cfg, self.cfg['d_model'])\n",
        "#         elif cfg['normalization']=='LN':\n",
        "#             self.norm = LayerNorm(self.cfg, self.cfg['d_model'])\n",
        "            \n",
        "#         self.blocks = nn.ModuleList([TransformerBlock(self.cfg, block_index) for block_index in range(self.cfg['n_layers'])])\n",
        "\n",
        "#         if self.cfg['factored_embed']:\n",
        "#             self.unembed = FactoredUnembed(self.cfg)\n",
        "#         else:\n",
        "#             self.unembed = Unembed(self.cfg)\n",
        "\n",
        "#         # Gives each module a parameter with its name (relative to this root module)\n",
        "#         # Needed for HookPoints to work\n",
        "#         self.setup_hooks()\n",
        "            \n",
        "#     def forward(self, tokens, return_loss=True):\n",
        "#         # Input x is either a batch of tokens ([batch, pos]) or a text string\n",
        "#         # if type(x)==str:\n",
        "#         #     # If text, convert to tokens (batch_size=1)\n",
        "#         #     x = self.to_tokens(x)\n",
        "#         embed = self.hook_embed(self.embed(tokens)) # [batch, pos, d_model]\n",
        "#         pos_embed = self.hook_pos_embed(self.pos_embed(tokens)) # [batch, pos, d_model]\n",
        "#         if cfg['use_pos_resid']:\n",
        "#             residual = embed + pos_embed # [batch, pos, d_model]\n",
        "#         else:\n",
        "#             residual = embed # [batch, pos, d_model]\n",
        "#         for block in self.blocks:\n",
        "#             # Note that each block includes skip connections, so we don't need\n",
        "#             # residual + block(residual)\n",
        "#             residual = block(residual, pos_embed) # [batch, pos, d_model]\n",
        "#         if self.cfg['normalization'] is not None:\n",
        "#             residual = self.norm(residual)\n",
        "#         logits = self.unembed(residual) # [batch, pos, d_vocab]\n",
        "#         if return_loss:\n",
        "#             return loss_fn(logits, tokens)\n",
        "#         else:\n",
        "#             return logits\n",
        "    \n",
        "#     def to_tokens(self, text):\n",
        "#         return self.tokenizer(text, return_tensors='pt')['input_ids']"
      ],
      "metadata": {
        "id": "AC7gV4ieifTR"
      },
      "id": "AC7gV4ieifTR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7xC7JTXv8mkF"
      },
      "id": "7xC7JTXv8mkF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_new = NewTransformer(cfg, tokenizer)\n",
        "# model_new.to('cuda:0')"
      ],
      "metadata": {
        "id": "25OWrctG8pfr"
      },
      "id": "25OWrctG8pfr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_old.to('cuda:7').to(torch.float32)\n",
        "# model_new.to('cuda:7').to(torch.float32)\n",
        "# tokens = torch.randint(100, 1000, (10, 1024)).to('cuda:7')\n",
        "# s = time.time()\n",
        "# loss = model_old(tokens)\n",
        "# print(loss.item())\n",
        "# print(time.time() - s)\n",
        "# s = time.time()\n",
        "# loss = model_new(tokens)\n",
        "# print(loss.item())\n",
        "# print(time.time() - s)\n",
        "# %timeit model_old(tokens)\n",
        "# %timeit model_new(tokens)"
      ],
      "metadata": {
        "id": "Nx4TTgiJ8n52"
      },
      "id": "Nx4TTgiJ8n52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %timeit model_old(tokens)\n",
        "# %timeit model_new(tokens)"
      ],
      "metadata": {
        "id": "lwJKu14SqeWw"
      },
      "id": "lwJKu14SqeWw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_old.to('cuda:5').to(torch.float16)\n",
        "# model_new.to('cuda:5').to(torch.float16)\n",
        "# tokens = torch.randint(100, 1000, (10, 1024)).to('cuda:5')\n",
        "# s = time.time()\n",
        "# loss = model_old(tokens)\n",
        "# print(loss.item())\n",
        "# print(time.time() - s)\n",
        "# s = time.time()\n",
        "# loss = model_new(tokens)\n",
        "# print(loss.item())\n",
        "# print(time.time() - s)\n",
        "# %timeit model_old(tokens)\n",
        "# %timeit model_new(tokens)"
      ],
      "metadata": {
        "id": "B61Qr7Xp88_a"
      },
      "id": "B61Qr7Xp88_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_old.to('cuda:7')\n",
        "# model_new_2.to('cuda:7')\n",
        "# tokens = torch.randint(100, 1000, (10, 1024)).to('cuda:7')\n",
        "# s = time.time()\n",
        "# loss = model_old(tokens)\n",
        "# print(loss.item())\n",
        "# print(time.time() - s)\n",
        "# s = time.time()\n",
        "# loss = model_new_2(tokens)\n",
        "# print(loss.item())\n",
        "# print(time.time() - s)\n",
        "# %timeit model_old(tokens)\n",
        "# %timeit model_new_2(tokens)"
      ],
      "metadata": {
        "id": "D6CCKsQD_EvT"
      },
      "id": "D6CCKsQD_EvT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if False:\n",
        "    # Debugging code:\n",
        "    lis = []\n",
        "    for i in range(cfg['max_steps']):\n",
        "        if schedule.step():\n",
        "            lis.append(i)\n",
        "    px.line(lis, log_y=True).show()"
      ],
      "metadata": {
        "id": "bbOO2evHApIN"
      },
      "id": "bbOO2evHApIN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y9OczhRxgGhj"
      },
      "id": "Y9OczhRxgGhj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d727eff30df4a41822cd353ec0403c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d36d21a6877b4d83ac04aa9ca7a6ef9f",
              "IPY_MODEL_d9335316359049cf9610881fcd4c612d",
              "IPY_MODEL_e57a960343e3432c95ac3f2bf022d6d8"
            ],
            "layout": "IPY_MODEL_4010279517df474aa1c750ee1e7d3e98"
          }
        },
        "d36d21a6877b4d83ac04aa9ca7a6ef9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d8f1eaefe924bea971fba060f62dc55",
            "placeholder": "​",
            "style": "IPY_MODEL_66c6db320d274bf5b9e626e44dc97d8b",
            "value": "Resolving data files: 100%"
          }
        },
        "d9335316359049cf9610881fcd4c612d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fd9c8a71bf54bbab0501e117a8a7668",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0970794e733a4600bde99bbd47af6d86",
            "value": 28
          }
        },
        "e57a960343e3432c95ac3f2bf022d6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c32f287b40534880a800c859ca637412",
            "placeholder": "​",
            "style": "IPY_MODEL_6db466e04b094f1ba2e44f090b89754e",
            "value": " 28/28 [00:00&lt;00:00,  2.18it/s]"
          }
        },
        "4010279517df474aa1c750ee1e7d3e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d8f1eaefe924bea971fba060f62dc55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66c6db320d274bf5b9e626e44dc97d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fd9c8a71bf54bbab0501e117a8a7668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0970794e733a4600bde99bbd47af6d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c32f287b40534880a800c859ca637412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6db466e04b094f1ba2e44f090b89754e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be21190537f344af8445642cde0dce84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_209b37e6fec246f4b27bc5cfa279301e",
              "IPY_MODEL_d3ab6deb92674d02b58ce4035e321662",
              "IPY_MODEL_7db02bba95ad46b9b6a145c88eb9bf74"
            ],
            "layout": "IPY_MODEL_16afb996974d463b923d6843d883036f"
          }
        },
        "209b37e6fec246f4b27bc5cfa279301e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08cd8519be0a40eea30318f13c6802fa",
            "placeholder": "​",
            "style": "IPY_MODEL_ac3bf66810dd4a448144bff89ef210ef",
            "value": ""
          }
        },
        "d3ab6deb92674d02b58ce4035e321662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65d4268c8e1e4c00b6bc2915f368381d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7569faa473bc49c7babb8ab8532034d3",
            "value": 1
          }
        },
        "7db02bba95ad46b9b6a145c88eb9bf74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49e5e2dbd3844a649af6918fb727d2e8",
            "placeholder": "​",
            "style": "IPY_MODEL_47ee13fcd2b5450abacad9e38a43d407",
            "value": " 12/? [02:09&lt;00:00,  5.18s/it]"
          }
        },
        "16afb996974d463b923d6843d883036f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08cd8519be0a40eea30318f13c6802fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac3bf66810dd4a448144bff89ef210ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65d4268c8e1e4c00b6bc2915f368381d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7569faa473bc49c7babb8ab8532034d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49e5e2dbd3844a649af6918fb727d2e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47ee13fcd2b5450abacad9e38a43d407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}