[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SoLU-Notebook-Test",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "SoLU-Notebook-Test",
    "section": "Install",
    "text": "Install\npip install SoLU_Notebook_Test"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "SoLU-Notebook-Test",
    "section": "How to use",
    "text": "How to use\nFill me in please! Donâ€™t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "SoLU_Transformer_v3 (1).html",
    "href": "SoLU_Transformer_v3 (1).html",
    "title": "SoLU-Notebook-Test",
    "section": "",
    "text": "# # !pip install wandb\n\n# # !apt-get update\n# # !su -\n# # !apt-get install sudo -y\n# # !apt-get install tmux -y\n# !pip install einops\n# !pip install pyyaml==5.4.1\n# !pip install transformers\n# !pip install datasets\n# !pip install matplotlib\n# !pip install plotly\n# !pip install zstandard\n\n# # import wandb\n# # wandb.login()\n# # # !wandb login\n\n\n# Import stuff\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport einops\nimport tqdm.notebook as tqdm\n\nimport random\nimport time\n\n# from google.colab import drive\nfrom pathlib import Path\nimport pickle\nimport os\n\nimport matplotlib.pyplot as plt\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"colab\"\nimport plotly.graph_objects as go\n\nfrom torch.utils.data import DataLoader\n\nfrom functools import *\nimport pandas as pd\nimport gc\nimport collections\nimport copy\n\n# import comet_ml\nimport itertools\nfrom transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\nimport transformers\nfrom datasets import load_dataset\nimport json\nfrom transformers import AutoTokenizer\nimport transformers\nimport datasets\nimport time\nimport wandb\n\nfrom pprint import pprint\n\n\ncfg = {\n    'd_model':1280,\n    'n_layers':10,\n    'lr':5e-4,\n    'batch_size':13 * torch.cuda.device_count(),\n    'batches_per_step':3,\n    'seed':14916,\n    # 'checkpoint_every_tokens':5*10**7,\n    'use_checkpoint_schedule':True,\n    'debug':False,\n    'debug_batch':False,\n    'debug_overfit':False,\n    'normalization':'LN', # 'LN' 'RMS' or None\n    'max_tokens':15*10**9,\n    'version':22,\n    'use_bfloat16':False,\n    'save_checkpoints_to_bfloat16':True,\n    'use_bfloat16_matmul':True,\n    'right_multiply_matrices':True,\n    # 'n_heads':8,\n    'd_head':64,\n    'n_ctx':1024,\n    'd_vocab':50278,\n    # 'factor_size':256,\n    'betas':(0.9, 0.99),\n    'weight_decay':0.01,\n    'dataset_name':'the_pile',\n    'grad_norm_clip':1.0,\n    'use_attn_result':False,\n    'n_devices':torch.cuda.device_count(),\n    'act_fn':'SoLU',\n    'use_pos_resid':True,\n    'attn_only':False,\n    'ln_eps':1e-5,\n    'lr_schedule': 'cosine_warmup',\n    'warmup_tokens':25*10**7,\n    'factored_embed':False,\n    'train_loss_ewma_beta':0.99,\n    'shuffled_data':True,\n    # 'W_O_init_scale':True,\n}\nprint('Old')\npprint(cfg)\nprint()\ncfg['n_heads'] = cfg['d_model']//cfg['d_head']\ncfg['d_mlp'] = 4 * cfg['d_model']\ncfg['tokens_per_step'] = (cfg['batch_size']*cfg['n_ctx']*cfg['batches_per_step'])\ncfg['max_steps'] = cfg['max_tokens']//cfg['tokens_per_step']\ncfg['warmup_steps'] = cfg['warmup_tokens']//cfg['tokens_per_step']\n# cfg['checkpoint_every'] = cfg['checkpoint_every_tokens']//cfg['tokens_per_step']\nif cfg['debug'] and not cfg['debug_overfit']:\n    print('Old max steps:', cfg['max_steps'])\n    cfg['max_steps']=20\n# cfg['warmup_steps']=cfg['warmup_tokens']//cfg['tokens_per_step']\npprint(cfg)\ntorch.manual_seed(cfg['seed'])\nnp.random.seed(cfg['seed'])\nrandom.seed(cfg['seed'])\n\nOld\n{'act_fn': 'SoLU',\n 'attn_only': False,\n 'batch_size': 104,\n 'batches_per_step': 3,\n 'betas': (0.9, 0.99),\n 'd_head': 64,\n 'd_model': 1280,\n 'd_vocab': 50278,\n 'dataset_name': 'the_pile',\n 'debug': False,\n 'debug_batch': False,\n 'debug_overfit': False,\n 'factored_embed': False,\n 'grad_norm_clip': 1.0,\n 'ln_eps': 1e-05,\n 'lr': 0.0005,\n 'lr_schedule': 'cosine_warmup',\n 'max_tokens': 15000000000,\n 'n_ctx': 1024,\n 'n_devices': 8,\n 'n_layers': 10,\n 'normalization': 'LN',\n 'right_multiply_matrices': True,\n 'save_checkpoints_to_bfloat16': True,\n 'seed': 14916,\n 'shuffled_data': True,\n 'train_loss_ewma_beta': 0.99,\n 'use_attn_result': False,\n 'use_bfloat16': False,\n 'use_bfloat16_matmul': True,\n 'use_checkpoint_schedule': True,\n 'use_pos_resid': True,\n 'version': 22,\n 'warmup_tokens': 250000000,\n 'weight_decay': 0.01}\n\n{'act_fn': 'SoLU',\n 'attn_only': False,\n 'batch_size': 104,\n 'batches_per_step': 3,\n 'betas': (0.9, 0.99),\n 'd_head': 64,\n 'd_mlp': 5120,\n 'd_model': 1280,\n 'd_vocab': 50278,\n 'dataset_name': 'the_pile',\n 'debug': False,\n 'debug_batch': False,\n 'debug_overfit': False,\n 'factored_embed': False,\n 'grad_norm_clip': 1.0,\n 'ln_eps': 1e-05,\n 'lr': 0.0005,\n 'lr_schedule': 'cosine_warmup',\n 'max_steps': 46950,\n 'max_tokens': 15000000000,\n 'n_ctx': 1024,\n 'n_devices': 8,\n 'n_heads': 20,\n 'n_layers': 10,\n 'normalization': 'LN',\n 'right_multiply_matrices': True,\n 'save_checkpoints_to_bfloat16': True,\n 'seed': 14916,\n 'shuffled_data': True,\n 'tokens_per_step': 319488,\n 'train_loss_ewma_beta': 0.99,\n 'use_attn_result': False,\n 'use_bfloat16': False,\n 'use_bfloat16_matmul': True,\n 'use_checkpoint_schedule': True,\n 'use_pos_resid': True,\n 'version': 22,\n 'warmup_steps': 782,\n 'warmup_tokens': 250000000,\n 'weight_decay': 0.01}\n\n\n\nprint(f\"Num params: {12*cfg['n_layers']*cfg['d_model']**2:,}\")\n\nNum params: 196,608,000\n\n\n\ndef cuda_memory():\n    print([torch.cuda.memory_allocated(f\"cuda:{i}\")/1e9 for i in range(torch.cuda.device_count())])\n\n\ndef get_corner(tensor, n=2):\n    # Prints the top left corner of the tensor\n    if len(tensor.shape)==0:\n        return tensor\n    elif len(tensor.shape)==1:\n        return tensor[:n]\n    elif len(tensor.shape)==2:\n        return tensor[:n, :n]\n    elif len(tensor.shape)==3:\n        return tensor[:n, :n, :n]\n    elif len(tensor.shape)==4:\n        return tensor[:n, :n, :n, :n]\n    elif len(tensor.shape)==5:\n        return tensor[:n, :n, :n, :n, :n]\n    elif len(tensor.shape)==6:\n        return tensor[:n, :n, :n, :n, :n, :n]\n    else:\n        # I never need tensors of rank > 6\n        raise ValueError(f'Tensor of shape {tensor.shape} is too big')\n\ndef to_numpy(tensor, flat=False):\n    if (type(tensor)!=torch.Tensor) and (type(tensor)!=torch.nn.parameter.Parameter):\n        return tensor\n    if flat:\n        return tensor.flatten().detach().cpu().numpy()\n    else:\n        return tensor.detach().cpu().numpy()\n\ndef save_to_bfloat16(model, file_name):\n    sd = model.state_dict()\n    torch.save({k:v.to(torch.bfloat16) for k, v in sd.items()}, file_name)\n    print(\"Saved model as bfloat16 to\", file_name)\n# save_to_bfloat16(model, 'SoLU_3L_testing.pth')\n\n\n# A helper class to get access to intermediate activations (inspired by Garcon)\n# It's a dummy module that is the identity function by default\n# I can wrap any intermediate activation in a HookPoint and get a convenient \n# way to add PyTorch hooks\nclass HookPoint(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fwd_hooks = []\n        self.bwd_hooks = []\n        self.ctx = {}\n        \n        # A variable giving the hook's name (from the perspective of the root \n        # module) - this is set by the root module at setup.\n        self.name = None\n    \n    def add_hook(self, hook, dir='fwd'):\n        # Hook format is fn(activation, hook_name)\n        # Change it into PyTorch hook format (this includes input and output, \n        # which are the same for a HookPoint)\n        def full_hook(module, module_input, module_output):\n            return hook(module_output, hook=self)\n        if dir=='fwd':\n            handle = self.register_forward_hook(full_hook)\n            self.fwd_hooks.append(handle)\n        elif dir=='bwd':\n            handle = self.register_full_backward_hook(full_hook)\n            self.bwd_hooks.append(handle)\n        else:\n            raise ValueError(f\"Invalid direction {dir}\")\n    \n    def remove_hooks(self, dir='fwd'):\n        if (dir=='fwd') or (dir=='both'):\n            for hook in self.fwd_hooks:\n                hook.remove()\n            self.fwd_hooks = []\n        if (dir=='bwd') or (dir=='both'):\n            for hook in self.bwd_hooks:\n                hook.remove()\n            self.bwd_hooks = []\n        if dir not in ['fwd', 'bwd', 'both']:\n            raise ValueError(f\"Invalid direction {dir}\")\n    \n    def clear_context(self):\n        del self.ctx\n        self.ctx = {}\n\n    def forward(self, x):\n        return x\n\n    def layer(self):\n        # Returns the layer index if the name has the form 'blocks.{layer}.{...}'\n        # Helper function that's mainly useful on EasyTransformer\n        # If it doesn't have this form, raises an error - \n        split_name = self.name.split('.')\n        return int(split_name[1])\n\nclass HookedRootModule(nn.Module):\n    # A class building on nn.Module to interface nicely with HookPoints\n    # Allows you to name each hook, remove hooks, cache every activation/gradient, etc\n    def __init__(self, *args):\n        super().__init__()\n    \n    def setup_hooks(self):\n        # Setup function - this needs to be run in __init__ AFTER defining all \n        # layers\n        # Add a parameter to each module giving its name\n        # Build a dictionary mapping a module name to the module\n        self.mod_dict = {}\n        self.hook_dict = {}\n        for name, module in self.named_modules():\n            module.name = name\n            self.mod_dict[name] = module\n            if type(module)==HookPoint:\n                self.hook_dict[name] = module\n        \n    def hook_points(self):\n        return (self.hook_dict.values())\n\n    def remove_all_hook_fns(self, direction='both'):\n        for hp in self.hook_points():\n            hp.remove_hooks(direction)\n    \n    def clear_contexts(self):\n        for hp in self.hook_points():\n            hp.clear_context()\n    \n    def reset_hooks(self, clear_contexts=True, direction='both'):\n        if clear_contexts: self.clear_contexts()\n        self.remove_all_hook_fns(direction)\n    \n    def cache_all(self, cache, incl_bwd=False, device='cuda'):\n        # Caches all activations wrapped in a HookPoint\n        def save_hook(tensor, hook):\n            cache[hook.name] = tensor.detach().to(device)\n        def save_hook_back(tensor, hook):\n            cache[hook.name+'_grad'] = tensor[0].detach().to(device)\n        for hp in self.hook_points():\n            hp.add_hook(save_hook, 'fwd')\n            if incl_bwd:\n                hp.add_hook(save_hook_back, 'bwd')\n    \n    def run_with_hooks(self, \n                       *args, \n                       fwd_hooks=[], \n                       bwd_hooks=[], \n                       reset_hooks_start=True, \n                       reset_hooks_end=True, \n                       clear_contexts=False):\n        '''\n        fwd_hooks: A list of (name, hook), where name is either the name of \n        a hook point or a Boolean function on hook names and hook is the \n        function to add to that hook point, or the hook whose names evaluate \n        to True respectively. Ditto bwd_hooks\n        reset_hooks_start (bool): If True, all prior hooks are removed at the start\n        reset_hooks_end (bool): If True, all hooks are removed at the end (ie, \n        including those added in this run)\n        clear_contexts (bool): If True, clears hook contexts whenever hooks are reset\n        \n        Note that if we want to use backward hooks, we need to set \n        reset_hooks_end to be False, so the backward hooks are still there - this function only runs a forward pass.\n        '''\n        if reset_hooks_start:\n            self.reset_hooks(clear_contexts)\n        for name, hook in fwd_hooks:\n            if type(name)==str:\n                self.mod_dict[name].add_hook(hook, dir='fwd')\n            else:\n                # Otherwise, name is a Boolean function on names\n                for hook_name, hp in self.hook_dict.items():\n                    if name(hook_name):\n                        hp.add_hook(hook, dir='fwd')\n        for name, hook in bwd_hooks:\n            if type(name)==str:\n                self.mod_dict[name].add_hook(hook, dir='fwd')\n            else:\n                # Otherwise, name is a Boolean function on names\n                for hook_name, hp in self.hook_dict:\n                    if name(hook_name):\n                        hp.add_hook(hook, dir='bwd')\n        out = self.forward(*args)\n        if reset_hooks_end:\n            if len(bwd_hooks)>0:\n                print(\"WARNING: Hooks were reset at the end of run_with_hooks while backward hooks were set.\")\n                print(\"This removes the backward hooks before a backward pass can occur\")\n            self.reset_hooks(clear_contexts)\n        return out\n\n\ndef loss_fn(logits, batch):\n    log_probs = F.log_softmax(logits[:, :-1], dim=-1)\n    pred_log_probs = torch.gather(log_probs, -1, batch[:, 1:, None])[..., 0]\n    return -pred_log_probs.mean()\n\n\ndef amp_einsum(einsum_str, mat1, mat2):\n    # return torch.einsum(einsum_str, mat1, mat2)\n    # return torch.einsum(einsum_str, mat1.to(torch.bfloat16), mat2.to(torch.bfloat16)).to(torch.float32)\n    if cfg['use_bfloat16_matmul']:\n        return torch.einsum(einsum_str, mat1.to(torch.bfloat16), mat2.to(torch.bfloat16)).to(torch.float32)\n    else:\n        return torch.einsum(einsum_str, mat1, mat2)\n\n\n# Define network architecture\n\n# Embed & Unembed\nclass Embed(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.W_E = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['d_model']))\n        nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5), mode='fan_out')\n    \n    def forward(self, tokens):\n        # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n        # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n        # return einops.rearrange(self.W_E[tokens, :], 'd_model batch pos -> batch pos d_model')\n        return self.W_E[tokens, :]\n\n# class FactoredEmbed(nn.Module):\n#     def __init__(self, cfg):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.W_E = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_vocab']))\n#         self.W_E_factor = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['factor_size']))\n#         nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5), mode='fan_out')\n#         nn.init.kaiming_uniform_(self.W_E_factor, a=np.sqrt(5), mode='fan_out')\n    \n#     def forward(self, tokens):\n#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n#         # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n#         return einops.rearrange(self.W_E[:, tokens], 'factor batch pos -> batch pos factor') @ self.W_E_factor.T\n\n\nclass Unembed(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.W_U = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_vocab']))\n        nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5), mode='fan_out')\n    \n    def forward(self, residual):\n        return amp_einsum('bpm,mv->bpv', residual, self.W_U) # [batch, pos, d_vocab]\n\n# class FactoredUnembed(nn.Module):\n#     def __init__(self, cfg):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.W_U = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['factor_size']))\n#         self.W_U_factor = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_model']))\n#         nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5), mode='fan_out')\n#         nn.init.kaiming_uniform_(self.W_U_factor, a=np.sqrt(5), mode='fan_out')\n    \n#     def forward(self, residual):\n#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n#         # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n#         return amp_einsum('fm,vf,bpm->bpv', self.W_U_factor, self.W_U, residual) # [batch, pos, d_vocab]\n\n# Positional Embeddings\nclass PosEmbed(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.W_pos = nn.Parameter(torch.empty(self.cfg['n_ctx'], self.cfg['d_model'])) \n        nn.init.kaiming_uniform_(self.W_pos, a=np.sqrt(5), mode='fan_out')\n    \n    def forward(self, x):\n        # Output shape [pos, d_model] - will be broadcast along batch dim\n        return self.W_pos[:x.size(-1), :] # [pos, d_model]\n\nclass LayerNormPre(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.eps = self.cfg['ln_eps']\n\n        # Adds a hook point for the normalization scale factor\n        self.hook_scale = HookPoint() # [batch, pos]\n    \n    def forward(self, x):\n        x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]\n        scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n                                 self.eps).sqrt()) # [batch, pos, 1]\n        return x / scale\n\nclass LayerNorm(nn.Module):\n    def __init__(self, cfg, length):\n        super().__init__()\n        self.cfg = cfg\n        self.eps = self.cfg['ln_eps']\n        self.length = length\n        self.w = nn.Parameter(torch.ones(length))\n        self.b = nn.Parameter(torch.zeros(length))\n\n        # Adds a hook point for the normalization scale factor\n        self.hook_scale = HookPoint() # [batch, pos]\n    \n    def forward(self, x):\n        x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]\n        scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n                                 self.eps).sqrt()) # [batch, pos, 1]\n        out = (x / scale) * self.w + self.b\n        return out\n\nclass RMSNorm(nn.Module):\n    def __init__(self, cfg, length):\n        super().__init__()\n        self.cfg = cfg\n        self.eps = self.cfg['ln_eps']\n        self.length = length\n        self.w = nn.Parameter(torch.ones(length))\n\n        # Adds a hook point for the normalization scale factor\n        self.hook_scale = HookPoint() # [batch, pos]\n    \n    def forward(self, x):\n        scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n                                 self.eps).sqrt()) # [batch, pos, 1]\n        out = (x / scale) * self.w\n        return out\n\n# Attention\nclass Attention(nn.Module):\n    def __init__(self, cfg, attn_type='global'):\n        super().__init__()\n        self.cfg = cfg\n        self.W_Q = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_model'], self.cfg['d_head']))\n        self.b_Q = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n        nn.init.kaiming_uniform_(self.W_Q, a=np.sqrt(5), mode='fan_out')\n        self.W_K = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_model'], self.cfg['d_head']))\n        self.b_K = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n        nn.init.kaiming_uniform_(self.W_K, a=np.sqrt(5), mode='fan_out')\n        self.W_V = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_model'], self.cfg['d_head']))\n        self.b_V = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n        nn.init.kaiming_uniform_(self.W_V, a=np.sqrt(5), mode='fan_out')\n        self.W_O = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n        self.b_O = nn.Parameter(torch.zeros(self.cfg['d_model']))\n        nn.init.kaiming_uniform_(self.W_O, a=np.sqrt(5), mode='fan_out')\n        # if cfg['W_O_init_scale']:\n        #     self.W_O/=np.sqrt(2*self.cfg['n_layers'])\n        \n        self.attn_type = attn_type\n        # Create a query_pos x key_pos mask, with True iff that query position \n        # can attend to that key position\n        causal_mask = torch.tril(torch.ones((self.cfg['n_ctx'], self.cfg['n_ctx'])).bool())\n        self.register_buffer('mask', causal_mask)\n        \n        self.register_buffer('IGNORE', torch.tensor(-1e5))\n        self.attn_scale = np.sqrt(self.cfg['d_head'])\n        \n        self.hook_k = HookPoint() # [batch, pos, head_index, d_head]\n        self.hook_q = HookPoint() # [batch, pos, head_index, d_head]\n        self.hook_v = HookPoint() # [batch, pos, head_index, d_head]\n        self.hook_z = HookPoint() # [batch, pos, head_index, d_head]\n        self.hook_attn_scores = HookPoint() # [batch, head_index, query_pos, key_pos]\n        self.hook_attn = HookPoint() # [batch, head_index, query_pos, key_pos]\n        self.hook_result = HookPoint() # [batch, head_index, head_index, d_model]\n        if not cfg['use_pos_resid']:\n            self.hook_attn_input = HookPoint()\n\n    def forward(self, x, pos_embed):\n        if not cfg['use_pos_resid']:\n            attn_input = self.hook_attn_input(x+pos_embed)\n            q = self.hook_q(amp_einsum('bpm,imh->bpih', attn_input, self.W_Q)+self.b_Q) # [batch, pos, head_index, d_head]\n            k = self.hook_k(amp_einsum('bpm,imh->bpih', attn_input, self.W_K)+self.b_K) # [batch, pos, head_index, d_head]\n        else:\n            q = self.hook_q(amp_einsum('bpm,imh->bpih', x, self.W_Q)+self.b_Q) # [batch, pos, head_index, d_head]\n            k = self.hook_k(amp_einsum('bpm,imh->bpih', x, self.W_K)+self.b_K) # [batch, pos, head_index, d_head]\n\n        v = self.hook_v(amp_einsum('bpm,imh->bpih', x, self.W_V)+self.b_V) # [batch, pos, head_index, d_head]\n        attn_scores = amp_einsum('bpih,bqih->bipq', q, k)/self.attn_scale # [batch, head_index, query_pos, key_pos]\n        attn_scores = self.hook_attn_scores(self.apply_causal_mask(attn_scores)) # [batch, head_index, query_pos, key_pos]\n        attn_matrix = self.hook_attn(F.softmax(attn_scores, dim=-1)) # [batch, head_index, query_pos, key_pos]\n        z = self.hook_z(amp_einsum('bpih,biqp->bqih', v, attn_matrix)) # [batch, pos, head_index, d_head]\n        \n        if cfg['use_attn_result']:\n            result = self.hook_result(amp_einsum('bqih,ihm->bqim', z, self.W_O)) # [batch, pos, head_index, d_model]\n            out = einops.reduce(result, \n                            'batch position index model->batch position model', \n                            'sum')+self.b_O  # [batch, pos, d_model]\n        else:\n            out = (amp_einsum('bqih,ihm->bqm', z, self.W_O)+self.b_O) # [batch, pos, head_index, d_model]\n        return out\n    \n    def apply_causal_mask(self, attn_scores):\n        return torch.where(self.mask[:attn_scores.size(-2), :attn_scores.size(-1)], attn_scores, self.IGNORE)\n\nclass MLP(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.W_in = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_mlp']))\n        nn.init.kaiming_uniform_(self.W_in, a=np.sqrt(5), mode='fan_out')\n        self.b_in = nn.Parameter(torch.zeros(self.cfg['d_mlp']))\n        self.W_out = nn.Parameter(torch.empty(self.cfg['d_mlp'], self.cfg['d_model']))\n        nn.init.kaiming_uniform_(self.W_out, a=np.sqrt(5), mode='fan_out')\n        self.b_out = nn.Parameter(torch.zeros(self.cfg['d_model']))\n\n        self.hook_pre = HookPoint() # [batch, pos, d_mlp]\n        self.hook_post = HookPoint() # [batch, pos, d_mlp]\n\n        if self.cfg['act_fn'].lower()=='relu':\n            self.act_fn = F.relu\n        elif self.cfg['act_fn'].lower()=='gelu_new':\n            self.act_fn = gelu_new\n        elif self.cfg['act_fn'].lower()=='solu':\n            self.act_fn = lambda x: F.softmax(x, dim=-1)*x\n            self.hook_post_ln = HookPoint() # [batch, pos, d_mlp]\n            self.ln = LayerNorm(self.cfg, self.cfg['d_mlp'])\n        else:\n            raise ValueError(f\"Invalid activation function name: {self.cfg['act_fn']}\")\n\n    def forward(self, x):\n        x = self.hook_pre(amp_einsum('bpd,dm->bpm', x, self.W_in) + self.b_in) # [batch, pos, d_mlp]\n        x = self.hook_post(self.act_fn(x)) # [batch, pos, d_mlp]\n        if self.cfg['act_fn'].lower()=='solu':\n            x = self.hook_post_ln(self.ln(x))\n        x = amp_einsum('bpm,md->bpd', x, self.W_out) + self.b_out # [batch, pos, d_model]\n        return x\n\n# Transformer Block\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg, block_index):\n        super().__init__()\n        self.cfg = cfg\n        if self.cfg['normalization']=='RMS':\n            self.norm1 = LayerNorm(self.cfg, self.cfg['d_model'])\n            self.norm2 = LayerNorm(self.cfg, self.cfg['d_model'])\n        elif self.cfg['normalization']=='LN':\n            self.norm1 = LayerNorm(self.cfg, self.cfg['d_model'])\n            self.norm2 = LayerNorm(self.cfg, self.cfg['d_model'])\n        self.attn = Attention(self.cfg)\n        self.mlp = MLP(self.cfg)\n\n        self.hook_attn_out = HookPoint() # [batch, pos, d_model]\n        self.hook_mlp_out = HookPoint() # [batch, pos, d_model]\n        # Note that resid_pre of layer k+1 is resid_post of layer k - given for convenience\n        self.hook_resid_pre = HookPoint() # [batch, pos, d_model]\n        self.hook_resid_mid = HookPoint() # [batch, pos, d_model]\n        self.hook_resid_post = HookPoint() # [batch, pos, d_model]\n    \n    def forward(self, x, pos_embed):\n        resid_pre = self.hook_resid_pre(x) # [batch, pos, d_model]\n        if self.cfg['normalization'] is not None:\n            attn_out = self.hook_attn_out(self.attn(self.norm1(resid_pre), pos_embed)) # [batch, pos, d_model]\n        else:\n            attn_out = self.hook_attn_out(self.attn(resid_pre, pos_embed)) # [batch, pos, d_model]\n        resid_mid = self.hook_resid_mid(resid_pre + attn_out) # [batch, pos, d_model]\n        if self.cfg['normalization'] is not None:\n            mlp_out = self.hook_mlp_out(self.mlp(self.norm2(resid_mid))) # [batch, pos, d_model]\n        else:\n            mlp_out = self.hook_mlp_out(self.mlp(resid_mid)) # [batch, pos, d_model]\n        resid_post = self.hook_resid_post(resid_mid + mlp_out) # [batch, pos, d_model]\n        return resid_post\n\n# Full transformer\nclass Transformer(HookedRootModule):\n    def __init__(self, cfg, tokenizer):\n        super().__init__()\n        \n        self.cfg = cfg\n        self.tokenizer = tokenizer\n        \n        if self.cfg['factored_embed']:\n            self.embed = FactoredEmbed(self.cfg)\n        else:\n            self.embed = Embed(self.cfg)\n        self.hook_embed = HookPoint() # [batch, pos, d_model]\n        \n        self.pos_embed = PosEmbed(self.cfg)\n        self.hook_pos_embed = HookPoint() # [batch, pos, d_model]\n\n        if cfg['normalization']=='RMS':\n            self.norm = RMSNorm(self.cfg, self.cfg['d_model'])\n        elif cfg['normalization']=='LN':\n            self.norm = LayerNorm(self.cfg, self.cfg['d_model'])\n            \n        self.blocks = nn.ModuleList([TransformerBlock(self.cfg, block_index) for block_index in range(self.cfg['n_layers'])])\n\n        if self.cfg['factored_embed']:\n            self.unembed = FactoredUnembed(self.cfg)\n        else:\n            self.unembed = Unembed(self.cfg)\n\n        # Gives each module a parameter with its name (relative to this root module)\n        # Needed for HookPoints to work\n        self.setup_hooks()\n            \n    def forward(self, tokens, return_loss=True):\n        # Input x is either a batch of tokens ([batch, pos]) or a text string\n        # if type(x)==str:\n        #     # If text, convert to tokens (batch_size=1)\n        #     x = self.to_tokens(x)\n        embed = self.hook_embed(self.embed(tokens)) # [batch, pos, d_model]\n        pos_embed = self.hook_pos_embed(self.pos_embed(tokens)) # [batch, pos, d_model]\n        if cfg['use_pos_resid']:\n            residual = embed + pos_embed # [batch, pos, d_model]\n        else:\n            residual = embed # [batch, pos, d_model]\n        for block in self.blocks:\n            # Note that each block includes skip connections, so we don't need\n            # residual + block(residual)\n            residual = block(residual, pos_embed) # [batch, pos, d_model]\n        if self.cfg['normalization'] is not None:\n            residual = self.norm(residual)\n        logits = self.unembed(residual) # [batch, pos, d_vocab]\n        if return_loss:\n            return loss_fn(logits, tokens)\n        else:\n            return logits\n    \n    def to_tokens(self, text):\n        return self.tokenizer(text, return_tensors='pt')['input_ids']\n\n\n# Transformer Block\nclass AttnOnlyBlock(nn.Module):\n    def __init__(self, cfg, block_index):\n        super().__init__()\n        self.cfg = cfg\n        self.attn = Attention(cfg)\n\n        self.hook_attn_out = HookPoint() # [batch, pos, d_model]\n        # Note that resid_pre of layer k+1 is resid_post of layer k - given for convenience\n        self.hook_resid_pre = HookPoint() # [batch, pos, d_model]\n        self.hook_resid_post = HookPoint() # [batch, pos, d_model]\n    \n    def forward(self, x, pos_embed):\n        resid_pre = self.hook_resid_pre(x) # [batch, pos, d_model]\n        attn_out = self.hook_attn_out(self.attn(x, pos_embed)) # [batch, pos, d_model]\n        resid_post = self.hook_resid_post(resid_pre + attn_out) # [batch, pos, d_model]\n        return resid_post\n        \n# Full transformer\nclass AttnOnlyTransformer(HookedRootModule):\n    def __init__(self, cfg, tokenizer):\n        raise NotImplementedError(\"Need to add LN support etc\")\n        super().__init__()\n        \n        self.cfg = cfg\n        self.tokenizer = tokenizer\n        \n        self.embed = Embed(self.cfg)\n        self.hook_embed = HookPoint() # [batch, pos, d_model]\n        \n        self.pos_embed = PosEmbed(self.cfg)\n        self.hook_pos_embed = HookPoint() # [batch, pos, d_model]\n        \n        self.blocks = nn.ModuleList([AttnOnlyBlock(self.cfg, block_index) for block_index in range(self.cfg['n_layers'])])\n        self.unembed = Unembed(self.cfg)\n\n        # Gives each module a parameter with its name (relative to this root module)\n        # Needed for HookPoints to work\n        self.setup_hooks()\n            \n    def forward(self, tokens, return_loss=True):\n        # Input x is either a batch of tokens ([batch, pos]) or a text string\n        # if type(x)==str:\n        #     # If text, convert to tokens (batch_size=1)\n        #     x = self.to_tokens(x)\n        embed = self.hook_embed(self.embed(tokens)) # [batch, pos, d_model]\n        pos_embed = self.hook_pos_embed(self.pos_embed(tokens)) # [batch, pos, d_model]\n        residual = embed # [batch, pos, d_model]\n        for block in self.blocks:\n            # Note that each block includes skip connections, so we don't need\n            # residual + block(residual)\n            residual = block(residual, pos_embed) # [batch, pos, d_model]\n        logits = self.unembed(residual) # [batch, pos, d_vocab]\n        if return_loss:\n            return loss_fn(logits, tokens)\n        else:\n            return logits\n    \n    def to_tokens(self, text):\n        return self.tokenizer(text, return_tensors='pt')['input_ids']\n\n\ntokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\npad_token = '<PAD>'\ntokenizer.add_special_tokens({'pad_token':pad_token})\nprint(tokenizer)\n\nPreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-neox-20b', vocab_size=50254, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<PAD>'})\n\n\n\nif cfg['attn_only']:\n    model = AttnOnlyTransformer(cfg, tokenizer)\nelse:\n    model = Transformer(cfg, tokenizer)\nmodel.to('cuda')\nif cfg['use_bfloat16']:\n    model.to(torch.bfloat16)\noptimizer = torch.optim.AdamW(model.parameters(), \n                              lr=cfg['lr'], \n                              betas=cfg['betas'], \n                              weight_decay=cfg['weight_decay'])\nif cfg['lr_schedule'] is not None:\n    def lr_schedule(step):\n        if step<cfg['warmup_steps']:\n            return (1e-7+(cfg['lr']-1e-7)*step/cfg['warmup_steps'])/cfg['lr']\n        else:\n            return (0.55 + 0.9*0.5*np.cos(np.pi*(step-cfg['warmup_steps'])/(cfg['max_steps'] - cfg['warmup_steps'])))\n    param_groups = {'decay':[], 'no_decay':[]}\n    for name, param in model.named_parameters():\n        print(name)\n        if 'W_' in name and name not in ['W_E', 'W_U']:\n            param_groups['decay'].append(param)\n        else:\n            param_groups['no_decay'].append(param)\n    optim_groups = [\n                {\"params\": param_groups['decay'], \"weight_decay\": cfg['weight_decay']},\n                {\"params\": param_groups['no_decay'], \"weight_decay\": 0.0},\n            ]\n    optimizer = torch.optim.AdamW(optim_groups, lr=cfg['lr'])\n    print(optimizer)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_schedule)\n    # px.line(y=[lr_schedule(i) for i in range(cfg['max_steps'])]).show()\n\nembed.W_E\npos_embed.W_pos\nnorm.w\nnorm.b\nblocks.0.norm1.w\nblocks.0.norm1.b\nblocks.0.norm2.w\nblocks.0.norm2.b\nblocks.0.attn.W_Q\nblocks.0.attn.b_Q\nblocks.0.attn.W_K\nblocks.0.attn.b_K\nblocks.0.attn.W_V\nblocks.0.attn.b_V\nblocks.0.attn.W_O\nblocks.0.attn.b_O\nblocks.0.mlp.W_in\nblocks.0.mlp.b_in\nblocks.0.mlp.W_out\nblocks.0.mlp.b_out\nblocks.0.mlp.ln.w\nblocks.0.mlp.ln.b\nblocks.1.norm1.w\nblocks.1.norm1.b\nblocks.1.norm2.w\nblocks.1.norm2.b\nblocks.1.attn.W_Q\nblocks.1.attn.b_Q\nblocks.1.attn.W_K\nblocks.1.attn.b_K\nblocks.1.attn.W_V\nblocks.1.attn.b_V\nblocks.1.attn.W_O\nblocks.1.attn.b_O\nblocks.1.mlp.W_in\nblocks.1.mlp.b_in\nblocks.1.mlp.W_out\nblocks.1.mlp.b_out\nblocks.1.mlp.ln.w\nblocks.1.mlp.ln.b\nblocks.2.norm1.w\nblocks.2.norm1.b\nblocks.2.norm2.w\nblocks.2.norm2.b\nblocks.2.attn.W_Q\nblocks.2.attn.b_Q\nblocks.2.attn.W_K\nblocks.2.attn.b_K\nblocks.2.attn.W_V\nblocks.2.attn.b_V\nblocks.2.attn.W_O\nblocks.2.attn.b_O\nblocks.2.mlp.W_in\nblocks.2.mlp.b_in\nblocks.2.mlp.W_out\nblocks.2.mlp.b_out\nblocks.2.mlp.ln.w\nblocks.2.mlp.ln.b\nblocks.3.norm1.w\nblocks.3.norm1.b\nblocks.3.norm2.w\nblocks.3.norm2.b\nblocks.3.attn.W_Q\nblocks.3.attn.b_Q\nblocks.3.attn.W_K\nblocks.3.attn.b_K\nblocks.3.attn.W_V\nblocks.3.attn.b_V\nblocks.3.attn.W_O\nblocks.3.attn.b_O\nblocks.3.mlp.W_in\nblocks.3.mlp.b_in\nblocks.3.mlp.W_out\nblocks.3.mlp.b_out\nblocks.3.mlp.ln.w\nblocks.3.mlp.ln.b\nblocks.4.norm1.w\nblocks.4.norm1.b\nblocks.4.norm2.w\nblocks.4.norm2.b\nblocks.4.attn.W_Q\nblocks.4.attn.b_Q\nblocks.4.attn.W_K\nblocks.4.attn.b_K\nblocks.4.attn.W_V\nblocks.4.attn.b_V\nblocks.4.attn.W_O\nblocks.4.attn.b_O\nblocks.4.mlp.W_in\nblocks.4.mlp.b_in\nblocks.4.mlp.W_out\nblocks.4.mlp.b_out\nblocks.4.mlp.ln.w\nblocks.4.mlp.ln.b\nblocks.5.norm1.w\nblocks.5.norm1.b\nblocks.5.norm2.w\nblocks.5.norm2.b\nblocks.5.attn.W_Q\nblocks.5.attn.b_Q\nblocks.5.attn.W_K\nblocks.5.attn.b_K\nblocks.5.attn.W_V\nblocks.5.attn.b_V\nblocks.5.attn.W_O\nblocks.5.attn.b_O\nblocks.5.mlp.W_in\nblocks.5.mlp.b_in\nblocks.5.mlp.W_out\nblocks.5.mlp.b_out\nblocks.5.mlp.ln.w\nblocks.5.mlp.ln.b\nblocks.6.norm1.w\nblocks.6.norm1.b\nblocks.6.norm2.w\nblocks.6.norm2.b\nblocks.6.attn.W_Q\nblocks.6.attn.b_Q\nblocks.6.attn.W_K\nblocks.6.attn.b_K\nblocks.6.attn.W_V\nblocks.6.attn.b_V\nblocks.6.attn.W_O\nblocks.6.attn.b_O\nblocks.6.mlp.W_in\nblocks.6.mlp.b_in\nblocks.6.mlp.W_out\nblocks.6.mlp.b_out\nblocks.6.mlp.ln.w\nblocks.6.mlp.ln.b\nblocks.7.norm1.w\nblocks.7.norm1.b\nblocks.7.norm2.w\nblocks.7.norm2.b\nblocks.7.attn.W_Q\nblocks.7.attn.b_Q\nblocks.7.attn.W_K\nblocks.7.attn.b_K\nblocks.7.attn.W_V\nblocks.7.attn.b_V\nblocks.7.attn.W_O\nblocks.7.attn.b_O\nblocks.7.mlp.W_in\nblocks.7.mlp.b_in\nblocks.7.mlp.W_out\nblocks.7.mlp.b_out\nblocks.7.mlp.ln.w\nblocks.7.mlp.ln.b\nblocks.8.norm1.w\nblocks.8.norm1.b\nblocks.8.norm2.w\nblocks.8.norm2.b\nblocks.8.attn.W_Q\nblocks.8.attn.b_Q\nblocks.8.attn.W_K\nblocks.8.attn.b_K\nblocks.8.attn.W_V\nblocks.8.attn.b_V\nblocks.8.attn.W_O\nblocks.8.attn.b_O\nblocks.8.mlp.W_in\nblocks.8.mlp.b_in\nblocks.8.mlp.W_out\nblocks.8.mlp.b_out\nblocks.8.mlp.ln.w\nblocks.8.mlp.ln.b\nblocks.9.norm1.w\nblocks.9.norm1.b\nblocks.9.norm2.w\nblocks.9.norm2.b\nblocks.9.attn.W_Q\nblocks.9.attn.b_Q\nblocks.9.attn.W_K\nblocks.9.attn.b_K\nblocks.9.attn.W_V\nblocks.9.attn.b_V\nblocks.9.attn.W_O\nblocks.9.attn.b_O\nblocks.9.mlp.W_in\nblocks.9.mlp.b_in\nblocks.9.mlp.W_out\nblocks.9.mlp.b_out\nblocks.9.mlp.ln.w\nblocks.9.mlp.ln.b\nunembed.W_U\nAdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0.01\n\nParameter Group 1\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0.0\n)\n\n\n\n# if cfg['attn_only']:\n#     model = AttnOnlyTransformer(cfg, tokenizer)\n# else:\n#     model = Transformer(cfg, tokenizer)\n# model.to('cuda')\n# optimizer = torch.optim.AdamW(model.parameters(), \n#                               lr=cfg['lr'], \n#                               betas=cfg['betas'], \n#                               weight_decay=cfg['weight_decay'])\n# if cfg['lr_schedule'] is not None:\n#     # print(\"Using scheduler:\" scheduler)\n#     scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=cfg['warmup_steps'], num_training_steps=cfg['max_steps'])\n#     print(\"Using scheduler:\", scheduler)\n\n\nif cfg['debug'] and cfg['debug_batch']:\n    parallel_model = torch.nn.DataParallel(model, \n                        device_ids=list(range(torch.cuda.device_count())))\n    \n    for batch_size in range(12, 128, 2):\n    # for batch_size in range(48, 128, 4):\n    # for batch_size in [64, 64, 64, 64, 64, 64]:\n        start_time=time.time()\n        print()\n        print('New Batch!', batch_size)\n        cuda_memory()\n        for i in range(2):\n            batch = torch.randint(100, 2000, (6*batch_size, cfg['n_ctx']))\n            loss = parallel_model(batch).mean()\n            loss.backward()\n            print('Finished run', i, batch_size, batch.shape)\n        optimizer.step()\n        optimizer.zero_grad()\n        cuda_memory()\n        try:\n            del loss\n        except:\n            print('Deleting loss failed')\n        torch.cuda.empty_cache()\n        cuda_memory()\n        print('Time:', time.time() - start_time)\n    raise ValueError\n\n\nseq_len = cfg['n_ctx']\ndef tokenize(examples):\n    start_time = time.time()\n    texts = examples['text']\n    full_text = tokenizer.eos_token.join(texts)\n    div = 20\n    length = len(full_text)//div\n    text_list = [full_text[i*length:(i+1)*length] for i in range(div)]\n    tokens = tokenizer(text_list, return_tensors='np', padding=True)['input_ids'].flatten()\n    tokens = tokens[tokens!=tokenizer.pad_token_id]\n    # print(len(text_list), len(text_list[0]))\n    # print(tokens.shape)\n    n = len(tokens)\n    curr_batch_size = n//(seq_len-1)\n    tokens = tokens[:(seq_len-1)*curr_batch_size]\n    tokens = einops.rearrange(tokens, '(batch_size seq) -> batch_size seq', batch_size=curr_batch_size, seq=seq_len-1)\n    prefix = np.ones((curr_batch_size, 1), dtype=np.int64)*tokenizer.bos_token_id\n    # print(tokens.shape, n, curr_batch_size, seq_len)\n    return {'text': np.concatenate([prefix, tokens], axis=1)}# tiny_owt_orig_2 = load_dataset('stas/openwebtext-10k', cache_dir='./cache', split='train', download_config=datasets.DownloadConfig(resume_download=True, num_proc=4))\nimport time\n\nif not cfg['debug']:\n    start_time = time.time()\n    if cfg['shuffled_data']:\n        randperm = np.random.permutation(28)\n        print('Permutation of PILE URLs', randperm)\n        pile_urls = [f\"https://mystic.the-eye.eu/public/AI/pile/train/{i:0>2}.jsonl.zst\" for i in randperm]\n        dataset = load_dataset('json', data_files=pile_urls, streaming=True, split='train')\n    else:\n        dataset = load_dataset(cfg['dataset_name'], streaming=True, split='train')\n    print('Loaded!', time.time()-start_time)\n    start_time = time.time()\n    try:\n        dataset = dataset.remove_columns('meta')\n    except:\n        print('Meta not in dataset')\n    print('Loaded!', time.time()-start_time)\n    start_time = time.time()\n    dataset = dataset.map(tokenize, batched=True)\n    print('dataset.map', time.time()-start_time)\n    start_time = time.time()\n    dataset = dataset.with_format(type='torch')\n    print('dataset.set_format', time.time()-start_time)\n    start_time = time.time()\n    dataset = dataset.shuffle(seed=cfg['seed'], buffer_size=30000)\n    print('dataset.shuffle', time.time()-start_time)\n    start_time = time.time()\n    train_data_loader = DataLoader(dataset, batch_size=cfg['batch_size'])\n    print('train_data_loader =', time.time()-start_time)\nelse:\n    streaming_owt = load_dataset('stas/openwebtext-10k', split='train', cache_dir='cache')\n    streaming_owt = streaming_owt.map(tokenize, batched=True, num_proc=10)\n    streaming_owt = streaming_owt.with_format(type='torch')\n    train_data_loader = DataLoader(streaming_owt, batch_size=cfg['batch_size'], shuffle=True)\n    start_time = time.time()\n    for c, i in tqdm.tqdm(enumerate(train_data_loader)):\n        if c == 0:\n            print(\"Loaded Initial stream!\")\n            print(c, time.time() - start_time)\n            start_time = time.time()\n        elif c==1:\n            print('Time for next batch:', time.time() - start_time)\n            break\ndata_iter = iter(train_data_loader)\n# tiny_owt_orig_2 = load_dataset('stas/openwebtext-10k', cache_dir='./cache', split='train', download_config=datasets.DownloadConfig(resume_download=True, num_proc=4))\n# print('Loaded!')\n# # tokenizer.add_special_tokens({'pad_token':'<PAD>'})\n# tiny_owt = tiny_owt_orig.map(tokenize, batched=True)\n# print('Tokenized!')\n# tiny_owt_2 = tiny_owt_orig_2.map(tokenize, batched=True)\n\nPermutation of PILE URLs [17  5  6  8  9 25 18 13 14 27 26 20  2 24 10  0  7 12  4  3  1 19 16 23\n 15 22 11 21]\n\n\n\n\n\nUsing custom data configuration default-5e370f4de25e8bde\n\n\nLoaded! 0.853412389755249\nLoaded! 0.0002193450927734375\ndataset.map 0.00016236305236816406\ndataset.set_format 0.0033745765686035156\ndataset.shuffle 0.0008072853088378906\ntrain_data_loader = 0.0004334449768066406\n\n\n\ntorch.cuda.empty_cache()\nprint(model)\nmodel_name = f'SoLU_{cfg[\"n_layers\"]}L_v{cfg[\"version\"]}'\nprint(model_name)\n\nTransformer(\n  (embed): Embed()\n  (hook_embed): HookPoint()\n  (pos_embed): PosEmbed()\n  (hook_pos_embed): HookPoint()\n  (norm): LayerNorm(\n    (hook_scale): HookPoint()\n  )\n  (blocks): ModuleList(\n    (0): TransformerBlock(\n      (norm1): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (norm2): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_attn): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n        (hook_post_ln): HookPoint()\n        (ln): LayerNorm(\n          (hook_scale): HookPoint()\n        )\n      )\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n    (1): TransformerBlock(\n      (norm1): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (norm2): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_attn): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n        (hook_post_ln): HookPoint()\n        (ln): LayerNorm(\n          (hook_scale): HookPoint()\n        )\n      )\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n    (2): TransformerBlock(\n      (norm1): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (norm2): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_attn): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n        (hook_post_ln): HookPoint()\n        (ln): LayerNorm(\n          (hook_scale): HookPoint()\n        )\n      )\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n    (3): TransformerBlock(\n      (norm1): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (norm2): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_attn): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n        (hook_post_ln): HookPoint()\n        (ln): LayerNorm(\n          (hook_scale): HookPoint()\n        )\n      )\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n    (4): TransformerBlock(\n      (norm1): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (norm2): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_attn): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n        (hook_post_ln): HookPoint()\n        (ln): LayerNorm(\n          (hook_scale): HookPoint()\n        )\n      )\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n    (5): TransformerBlock(\n      (norm1): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (norm2): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_attn): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n        (hook_post_ln): HookPoint()\n        (ln): LayerNorm(\n          (hook_scale): HookPoint()\n        )\n      )\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n    (6): TransformerBlock(\n      (norm1): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (norm2): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_attn): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n        (hook_post_ln): HookPoint()\n        (ln): LayerNorm(\n          (hook_scale): HookPoint()\n        )\n      )\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n    (7): TransformerBlock(\n      (norm1): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (norm2): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_attn): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n        (hook_post_ln): HookPoint()\n        (ln): LayerNorm(\n          (hook_scale): HookPoint()\n        )\n      )\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n    (8): TransformerBlock(\n      (norm1): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (norm2): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_attn): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n        (hook_post_ln): HookPoint()\n        (ln): LayerNorm(\n          (hook_scale): HookPoint()\n        )\n      )\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n    (9): TransformerBlock(\n      (norm1): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (norm2): LayerNorm(\n        (hook_scale): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_attn): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n        (hook_post_ln): HookPoint()\n        (ln): LayerNorm(\n          (hook_scale): HookPoint()\n        )\n      )\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n  )\n  (unembed): Unembed()\n)\nSoLU_10L_v22\n\n\n\nparallel_model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\ncuda_memory()\n\n[1.340122112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\n\n\nclass SaveSchedule():\n    def __init__(self, max_tokens, tokens_per_step, schedule=None):\n        if schedule is None:\n            self.schedule = np.concatenate([np.arange(10)/10*1e-3, np.arange(2, 20)/20*1e-2, np.arange(5, 50)/50*1e-1, np.arange(10, 101)/100])\n        else:\n            self.schedule = schedule\n        self.max_tokens = max_tokens\n        self.tokens_per_step = tokens_per_step\n        self.counter = 0\n        self.next_save_point = 0\n        px.line(self.schedule * max_tokens, log_y=True, title='Save Schedule', labels={\"y\":\"Tokens\", \"x\":\"Checkpoint Index\"}).show()\n    \n    def step(self):\n        value = self.counter * self.tokens_per_step / self.max_tokens\n        threshold = self.schedule[self.next_save_point]\n        if value >= threshold:\n            self.next_save_point+=1\n            self.counter+=1\n            return True\n        else:\n            self.counter+=1\n            return False\nschedule = SaveSchedule(cfg['max_tokens'], cfg['tokens_per_step'])\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\nwandb.init(project=\"solu\", entity=\"mechanistic-interpretability\", config=cfg)\n\nwandb: Currently logged in as: neelnanda-io (mechanistic-interpretability). Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.13.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.2\n\n\nRun data is saved locally in /workspace/wandb/run-20220913_204442-ve2e5pjs\n\n\nSyncing run jumping-sky-91 to Weights & Biases (docs)\n\n\nDisplay W&B run\n\n\n\npprint(cfg)\n# DataLoader(full_owt_test['text'], batch_size=cfg['batch_size'], shuffle=False, pin_memory=False)\nprint('Training begins!')\nlosses = []\nloss_ewmas=[]\nstep = 0\nstart_time = time.time()\nloss_ewma = 9\n# loss_beta = 0.95\ntotal_tokens = 0\nrunning_loss = 0\nprev_time=time.time()\nepoch=0\n# for epoch in range(100):\nfor c, batch in tqdm.tqdm(enumerate(data_iter)):\n    batch = batch['text']\n    if cfg['debug'] and epoch==0 and c<3:\n        print(batch[0])\n        print(tokenizer.decode(batch[0]))\n    batch = batch.cuda()\n    loss = parallel_model(batch).mean()\n    loss.backward()\n    running_loss+=loss.item()\n    total_tokens += batch.numel()\n    if (c+1)%cfg['batches_per_step'] == 0:\n        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['grad_norm_clip'])\n        optimizer.step()\n        if cfg['lr_schedule'] is not None:\n            scheduler.step()\n            wandb.log({'scheduled_lr':scheduler.get_last_lr()[0]}, step=step)\n        optimizer.zero_grad()\n        if schedule.step() and cfg['use_checkpoint_schedule']:\n            print(f'Saved the model! Step: {step}. Frac of way through training: {schedule.schedule[schedule.next_save_point-1]}')\n            if not cfg['debug']:\n                if cfg['save_checkpoints_to_bfloat16']:\n                    save_to_bfloat16(model, f'{model_name}_{step:0>6}.pth')\n                else:\n                    torch.save(model.state_dict(), f'{model_name}_{step:0>6}.pth')\n                torch.save(optimizer.state_dict(), f'{model_name}_opt_checkpoint.pth')\n                if cfg['lr_schedule'] is not None:\n                    torch.save(scheduler.state_dict(), f'{model_name}_scheduler_checkpoint.pth')\n                wandb.save(f'{model_name}_{step:0>6}.pth')\n        running_loss = running_loss / cfg['batches_per_step']\n        losses.append(running_loss)\n\n        loss_ewma = loss_ewma * cfg['train_loss_ewma_beta'] + running_loss * (1 - cfg['train_loss_ewma_beta'])\n        loss_ewmas.append(loss_ewma)\n        wandb.log({\"loss\": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c}, step=step)\n        # print('Just logged')\n        # print({\"loss\": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c})\n        running_loss = 0\n        if step % 30 == 0:\n            print(c, step, total_tokens, losses[-1], loss_ewmas[-1])\n        step+=1\n        if step>=cfg['max_steps']:\n            break\n    if c<=12 and epoch==0:\n        cuda_memory()\n        print('Early iteration complete!', c, time.time()-prev_time)\n        prev_time=time.time()\n    del loss\n    # print(batch.shape, logits.shape, running_loss, loss, step, total_tokens)\n    # if not cfg['debug_overfit']:\n    #     break\n\nprint(f'Finished training! Train Loss EWMA: {loss_ewma}')\n\nif not cfg['debug']:\n    torch.save(model.state_dict(), f'{model_name}_final.pth')\n    wandb.save(f'{model_name}_final.pth')\nwandb.finish()\n\n{'act_fn': 'SoLU',\n 'attn_only': False,\n 'batch_size': 104,\n 'batches_per_step': 3,\n 'betas': (0.9, 0.99),\n 'd_head': 64,\n 'd_mlp': 5120,\n 'd_model': 1280,\n 'd_vocab': 50278,\n 'dataset_name': 'the_pile',\n 'debug': False,\n 'debug_batch': False,\n 'debug_overfit': False,\n 'factored_embed': False,\n 'grad_norm_clip': 1.0,\n 'ln_eps': 1e-05,\n 'lr': 0.0005,\n 'lr_schedule': 'cosine_warmup',\n 'max_steps': 46950,\n 'max_tokens': 15000000000,\n 'n_ctx': 1024,\n 'n_devices': 8,\n 'n_heads': 20,\n 'n_layers': 10,\n 'normalization': 'LN',\n 'right_multiply_matrices': True,\n 'save_checkpoints_to_bfloat16': True,\n 'seed': 14916,\n 'shuffled_data': True,\n 'tokens_per_step': 319488,\n 'train_loss_ewma_beta': 0.99,\n 'use_attn_result': False,\n 'use_bfloat16': False,\n 'use_bfloat16_matmul': True,\n 'use_checkpoint_schedule': True,\n 'use_pos_resid': True,\n 'version': 22,\n 'warmup_steps': 782,\n 'warmup_tokens': 250000000,\n 'weight_decay': 0.01}\nTraining begins!\n\n\n\n\n\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n\n\n[2.649573888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 0 90.26232886314392\n[2.649573888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 1 2.3085339069366455\nSaved the model! Step: 0. Frac of way through training: 0.0\nSaved model as bfloat16 to SoLU_10L_v22_000000.pth\n2 0 319488 10.982126871744791 9.019821268717449\n[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 2 8.920480012893677\n[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 3 2.3054752349853516\n[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 4 2.3048253059387207\n[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 5 2.3655030727386475\n[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 6 2.2996890544891357\n[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 7 2.301769256591797\n[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 8 2.362489938735962\n[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 9 2.303229808807373\n[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 10 2.3018174171447754\n[5.2661376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nEarly iteration complete! 11 9.724807262420654\n\n\n\n# pprint(cfg)\n# # DataLoader(full_owt_test['text'], batch_size=cfg['batch_size'], shuffle=False, pin_memory=False)\n# print('Training begins!')\n# losses = []\n# loss_ewmas=[]\n# step = 0\n# start_time = time.time()\n# loss_ewma = 9\n# loss_beta = 0.95\n# total_tokens = 0\n# running_loss = 0\n# prev_time=time.time()\n# epoch=0\n# # for epoch in range(100):\n# for epoch in range(100):\n#     data_iter = iter(train_data_loader)\n#     for c, batch in tqdm.tqdm(enumerate(data_iter)):\n#         batch = batch['text']\n#         if cfg['debug'] and epoch==0 and c<3:\n#             print(batch[0])\n#             print(tokenizer.decode(batch[0]))\n#         batch = batch.cuda()\n#         loss = parallel_model(batch).mean()\n#         loss.backward()\n#         running_loss+=loss.item()\n#         total_tokens += batch.numel()\n#         if (c+1)%cfg['batches_per_step'] == 0:\n#             torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['grad_norm_clip'])\n#             optimizer.step()\n#             if cfg['lr_schedule'] is not None:\n#                 scheduler.step()\n#                 wandb.log({'scheduled_lr':scheduler.get_last_lr()[0]}, step=step)\n#             optimizer.zero_grad()\n#             if step % cfg['checkpoint_every'] == 0:\n#                 print(f'Saved the model! Step: {step}')\n#                 if not cfg['debug']:\n#                     torch.save(model.state_dict(), f'{model_name}_{step}.pth')\n#                     torch.save(optimizer.state_dict(), f'{model_name}_opt_checkpoint.pth')\n#                     if cfg['lr_schedule'] is not None:\n#                         torch.save(scheduler.state_dict(), f'{model_name}_scheduler_checkpoint.pth')\n#                     wandb.save(f'{model_name}_{step}.pth')\n#             running_loss = running_loss / cfg['batches_per_step']\n#             losses.append(running_loss)\n\n#             loss_ewma = loss_ewma * loss_beta + running_loss * (1 - loss_beta)\n#             loss_ewmas.append(loss_ewma)\n#             wandb.log({\"loss\": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c}, step=step)\n#             # print('Just logged')\n#             # print({\"loss\": loss.item(), 'loss_ewma':loss_ewma, 'elapsed':time.time()-start_time, 'total_tokens':total_tokens, 'c':c})\n#             running_loss = 0\n#             if step % 30 == 0:\n#                 print(c, step, total_tokens, losses[-1], loss_ewmas[-1])\n#             step+=1\n#             # if step>=cfg['max_steps']:\n#             #     break\n#         if c<=12 and epoch==0:\n#             cuda_memory()\n#             print('Early iteration complete!', c, time.time()-prev_time)\n#             prev_time=time.time()\n#         # print(batch.shape, logits.shape, running_loss, loss, step, total_tokens)\n#         # if not cfg['debug_overfit']:\n#         #     break\n\n# print(f'Finished training! Train Loss EWMA: {loss_ewma}')\n\n# if not cfg['debug']:\n#     torch.save(model.state_dict(), f'{model_name}_final.pth')\n#     wandb.save(f'{model_name}_final.pth')\n# wandb.finish()\n\n\n# print(model)\n# print(parallel_model)\n\n\n# optimizer_2 = torch.optim.AdamW(model.parameters(), \n#                               lr=3e-4, \n#                               betas=cfg['betas'], \n#                               weight_decay=cfg['weight_decay'])\n# optimizer_2.load_state_dict(optimizer.state_dict())\n\n\n# print(optimizer_2)\n\n\n# torch.save(optimizer_2.state_dict(), \"_\"+model_name+\"_opt_midflight_checkpoint.pth\")\n# torch.save(model.state_dict(), \"_\"+model_name+\"_model_midflight_checkpoint.pth\")\n# wandb.save(\"_\"+model_name+\"_model_midflight_checkpoint.pth\")\n# wandb.save(\"_\"+model_name+\"_opt_midflight_checkpoint.pth\")\n\n\n# sd = optimizer_2.state_dict()\n\n\n# sd['param_groups'][0]['lr']=2e-4\n\n\n# _ = optimizer_2.load_state_dict(sd)\n\n\n# print(optimizer_2)\n\n\n# import datasets\n\n# files = '29.jsonl.zst'\n# pile_dataset = datasets.load_dataset('json', data_files=files, split='train', cache_dir='cache')\n# print(pile_dataset)\n# pile_dataset = pile_dataset.remove_columns('meta')\n\n\n# pile_dataset = pile_dataset.map(tokenize, batched=True, num_proc=30)\n# pile_dataset = pile_dataset.with_format(type='torch')\n\n\n# train_data_loader = DataLoader(pile_dataset, batch_size=cfg['batch_size'], shuffle=True, num_workers=10)\n\n\n# model_old = Transformer(cfg, tokenizer)\n# model_old.to('cuda')\n\n\n# Old transformer, left mult\n# # Define network architecture\n\n# # Embed & Unembed\n# class Embed(nn.Module):\n#     def __init__(self, cfg):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.W_E = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_vocab']))\n#         nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5))\n    \n#     def forward(self, tokens):\n#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n#         # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n#         return einops.rearrange(self.W_E[:, tokens], 'd_model batch pos -> batch pos d_model')\n\n# class FactoredEmbed(nn.Module):\n#     def __init__(self, cfg):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.W_E = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_vocab']))\n#         self.W_E_factor = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['factor_size']))\n#         nn.init.kaiming_uniform_(self.W_E, a=np.sqrt(5))\n#         nn.init.kaiming_uniform_(self.W_E_factor, a=np.sqrt(5))\n    \n#     def forward(self, tokens):\n#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n#         # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n#         return einops.rearrange(self.W_E[:, tokens], 'factor batch pos -> batch pos factor') @ self.W_E_factor.T\n\n\n# class Unembed(nn.Module):\n#     def __init__(self, cfg):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.W_U = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['d_model']))\n#         nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5))\n    \n#     def forward(self, residual):\n#         return amp_einsum('vm,bpm->bpv', self.W_U, residual) # [batch, pos, d_vocab]\n\n# class FactoredUnembed(nn.Module):\n#     def __init__(self, cfg):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.W_U = nn.Parameter(torch.empty(self.cfg['d_vocab'], self.cfg['factor_size']))\n#         self.W_U_factor = nn.Parameter(torch.empty(self.cfg['factor_size'], self.cfg['d_model']))\n#         nn.init.kaiming_uniform_(self.W_U, a=np.sqrt(5))\n#         nn.init.kaiming_uniform_(self.W_U_factor, a=np.sqrt(5))\n    \n#     def forward(self, residual):\n#         # If A has shape [a, b] and B has shape [c, d], then A[:, B] has shape [a, c, d]\n#         # B acts as a tensor of indices into the second dimension (so >=0 and <b)\n#         return amp_einsum('fm,vf,bpm->bpv', self.W_U_factor, self.W_U, residual) # [batch, pos, d_vocab]\n\n# # Positional Embeddings\n# class PosEmbed(nn.Module):\n#     def __init__(self, cfg):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.W_pos = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['n_ctx'])) \n#         nn.init.kaiming_uniform_(self.W_pos, a=np.sqrt(5))\n    \n#     def forward(self, x):\n#         # Output shape [pos, d_model] - will be broadcast along batch dim\n#         return self.W_pos[:, :x.size(-1)].T # [pos, d_model]\n\n# class LayerNormPre(nn.Module):\n#     def __init__(self, cfg):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.eps = self.cfg['ln_eps']\n\n#         # Adds a hook point for the normalization scale factor\n#         self.hook_scale = HookPoint() # [batch, pos]\n    \n#     def forward(self, x):\n#         x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]\n#         scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n#                                  self.eps).sqrt()) # [batch, pos, 1]\n#         return x / scale\n\n# class LayerNorm(nn.Module):\n#     def __init__(self, cfg, length):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.eps = self.cfg['ln_eps']\n#         self.length = length\n#         self.w = nn.Parameter(torch.ones(length))\n#         self.b = nn.Parameter(torch.zeros(length))\n\n#         # Adds a hook point for the normalization scale factor\n#         self.hook_scale = HookPoint() # [batch, pos]\n    \n#     def forward(self, x):\n#         x = x - x.mean(axis=-1, keepdim=True) # [batch, pos, d_model]\n#         scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n#                                  self.eps).sqrt()) # [batch, pos, 1]\n#         out = (x / scale) * self.w + self.b\n#         return out\n\n# class RMSNorm(nn.Module):\n#     def __init__(self, cfg, length):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.eps = self.cfg['ln_eps']\n#         self.length = length\n#         self.w = nn.Parameter(torch.ones(length))\n\n#         # Adds a hook point for the normalization scale factor\n#         self.hook_scale = HookPoint() # [batch, pos]\n    \n#     def forward(self, x):\n#         scale = self.hook_scale((x.pow(2).mean(-1, keepdim=True) + \n#                                  self.eps).sqrt()) # [batch, pos, 1]\n#         out = (x / scale) * self.w\n#         return out\n\n# # Attention\n# class Attention(nn.Module):\n#     def __init__(self, cfg, attn_type='global'):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.W_Q = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n#         self.b_Q = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n#         nn.init.kaiming_uniform_(self.W_Q, a=np.sqrt(5))\n#         self.W_K = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n#         self.b_K = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n#         nn.init.kaiming_uniform_(self.W_K, a=np.sqrt(5))\n#         self.W_V = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_head'], self.cfg['d_model']))\n#         self.b_V = nn.Parameter(torch.zeros(self.cfg['n_heads'], self.cfg['d_head']))\n#         nn.init.kaiming_uniform_(self.W_V, a=np.sqrt(5))\n#         self.W_O = nn.Parameter(torch.empty(self.cfg['n_heads'], self.cfg['d_model'], self.cfg['d_head']))\n#         self.b_O = nn.Parameter(torch.zeros(self.cfg['d_model']))\n#         nn.init.kaiming_uniform_(self.W_O, a=np.sqrt(5))\n#         # if cfg['W_O_init_scale']:\n#         #     self.W_O/=np.sqrt(2*self.cfg['n_layers'])\n        \n#         self.attn_type = attn_type\n#         # Create a query_pos x key_pos mask, with True iff that query position \n#         # can attend to that key position\n#         causal_mask = torch.tril(torch.ones((self.cfg['n_ctx'], self.cfg['n_ctx'])).bool())\n#         self.register_buffer('mask', causal_mask)\n        \n#         self.register_buffer('IGNORE', torch.tensor(-1e5))\n#         self.attn_scale = np.sqrt(self.cfg['d_head'])\n        \n#         self.hook_k = HookPoint() # [batch, pos, head_index, d_head]\n#         self.hook_q = HookPoint() # [batch, pos, head_index, d_head]\n#         self.hook_v = HookPoint() # [batch, pos, head_index, d_head]\n#         self.hook_z = HookPoint() # [batch, pos, head_index, d_head]\n#         self.hook_attn_scores = HookPoint() # [batch, head_index, query_pos, key_pos]\n#         self.hook_attn = HookPoint() # [batch, head_index, query_pos, key_pos]\n#         self.hook_result = HookPoint() # [batch, head_index, head_index, d_model]\n#         if not cfg['use_pos_resid']:\n#             self.hook_attn_input = HookPoint()\n\n#     def forward(self, x, pos_embed):\n#         if not cfg['use_pos_resid']:\n#             attn_input = self.hook_attn_input(x+pos_embed)\n#             q = self.hook_q(amp_einsum('ihm,bpm->bpih', self.W_Q, attn_input)+self.b_Q) # [batch, pos, head_index, d_head]\n#             k = self.hook_k(amp_einsum('ihm,bpm->bpih', self.W_K, attn_input)+self.b_K) # [batch, pos, head_index, d_head]\n#         else:\n#             q = self.hook_q(amp_einsum('ihm,bpm->bpih', self.W_Q, x)+self.b_Q) # [batch, pos, head_index, d_head]\n#             k = self.hook_k(amp_einsum('ihm,bpm->bpih', self.W_K, x)+self.b_K) # [batch, pos, head_index, d_head]\n\n#         v = self.hook_v(amp_einsum('ihm,bpm->bpih', self.W_V, x)+self.b_V) # [batch, pos, head_index, d_head]\n#         attn_scores = amp_einsum('bpih,bqih->bipq', q, k)/self.attn_scale # [batch, head_index, query_pos, key_pos]\n#         attn_scores = self.hook_attn_scores(self.apply_causal_mask(attn_scores)) # [batch, head_index, query_pos, key_pos]\n#         attn_matrix = self.hook_attn(F.softmax(attn_scores, dim=-1)) # [batch, head_index, query_pos, key_pos]\n#         z = self.hook_z(amp_einsum('bpih,biqp->bqih', v, attn_matrix)) # [batch, pos, head_index, d_head]\n        \n#         if cfg['use_attn_result']:\n#             result = self.hook_result(amp_einsum('imh,bqih->bqim', self.W_O, z)) # [batch, pos, head_index, d_model]\n#             out = einops.reduce(result, \n#                             'batch position index model->batch position model', \n#                             'sum')+self.b_O  # [batch, pos, d_model]\n#         else:\n#             out = (amp_einsum('imh,bqih->bqm', self.W_O, z)+self.b_O) # [batch, pos, head_index, d_model]\n#         return out\n    \n#     def apply_causal_mask(self, attn_scores):\n#         return torch.where(self.mask[:attn_scores.size(-2), :attn_scores.size(-1)], attn_scores, self.IGNORE)\n\n# class MLP(nn.Module):\n#     def __init__(self, cfg):\n#         super().__init__()\n#         self.cfg = cfg\n#         self.W_in = nn.Parameter(torch.empty(self.cfg['d_mlp'], self.cfg['d_model']))\n#         nn.init.kaiming_uniform_(self.W_in, a=np.sqrt(5))\n#         self.b_in = nn.Parameter(torch.zeros(self.cfg['d_mlp']))\n#         self.W_out = nn.Parameter(torch.empty(self.cfg['d_model'], self.cfg['d_mlp']))\n#         nn.init.kaiming_uniform_(self.W_out, a=np.sqrt(5))\n#         self.b_out = nn.Parameter(torch.zeros(self.cfg['d_model']))\n\n#         self.hook_pre = HookPoint() # [batch, pos, d_mlp]\n#         self.hook_post = HookPoint() # [batch, pos, d_mlp]\n\n#         if self.cfg['act_fn'].lower()=='relu':\n#             self.act_fn = F.relu\n#         elif self.cfg['act_fn'].lower()=='gelu_new':\n#             self.act_fn = gelu_new\n#         elif self.cfg['act_fn'].lower()=='solu':\n#             self.act_fn = lambda x: F.softmax(x, dim=-1)*x\n#             self.hook_post_ln = HookPoint() # [batch, pos, d_mlp]\n#             self.ln = LayerNorm(self.cfg, self.cfg['d_mlp'])\n#         else:\n#             raise ValueError(f\"Invalid activation function name: {self.cfg['act_fn']}\")\n\n#     def forward(self, x):\n#         x = self.hook_pre(amp_einsum('md,bpd->bpm', self.W_in, x) + self.b_in) # [batch, pos, d_mlp]\n#         x = self.hook_post(self.act_fn(x)) # [batch, pos, d_mlp]\n#         if self.cfg['act_fn'].lower()=='solu':\n#             x = self.hook_post_ln(self.ln(x))\n#         x = amp_einsum('dm,bpm->bpd', self.W_out, x) + self.b_out # [batch, pos, d_model]\n#         return x\n\n# # Transformer Block\n# class TransformerBlock(nn.Module):\n#     def __init__(self, cfg, block_index):\n#         super().__init__()\n#         self.cfg = cfg\n#         if self.cfg['normalization']=='RMS':\n#             self.norm1 = LayerNorm(self.cfg, self.cfg['d_model'])\n#             self.norm2 = LayerNorm(self.cfg, self.cfg['d_model'])\n#         elif self.cfg['normalization']=='LN':\n#             self.norm1 = LayerNorm(self.cfg, self.cfg['d_model'])\n#             self.norm2 = LayerNorm(self.cfg, self.cfg['d_model'])\n#         self.attn = Attention(self.cfg)\n#         self.mlp = MLP(self.cfg)\n\n#         self.hook_attn_out = HookPoint() # [batch, pos, d_model]\n#         self.hook_mlp_out = HookPoint() # [batch, pos, d_model]\n#         # Note that resid_pre of layer k+1 is resid_post of layer k - given for convenience\n#         self.hook_resid_pre = HookPoint() # [batch, pos, d_model]\n#         self.hook_resid_mid = HookPoint() # [batch, pos, d_model]\n#         self.hook_resid_post = HookPoint() # [batch, pos, d_model]\n    \n#     def forward(self, x, pos_embed):\n#         resid_pre = self.hook_resid_pre(x) # [batch, pos, d_model]\n#         if self.cfg['normalization'] is not None:\n#             attn_out = self.hook_attn_out(self.attn(self.norm1(resid_pre), pos_embed)) # [batch, pos, d_model]\n#         else:\n#             attn_out = self.hook_attn_out(self.attn(resid_pre, pos_embed)) # [batch, pos, d_model]\n#         resid_mid = self.hook_resid_mid(resid_pre + attn_out) # [batch, pos, d_model]\n#         if self.cfg['normalization'] is not None:\n#             mlp_out = self.hook_mlp_out(self.mlp(self.norm2(resid_mid))) # [batch, pos, d_model]\n#         else:\n#             mlp_out = self.hook_mlp_out(self.mlp(resid_mid)) # [batch, pos, d_model]\n#         resid_post = self.hook_resid_post(resid_mid + mlp_out) # [batch, pos, d_model]\n#         return resid_post\n\n# # Full transformer\n# class Transformer(HookedRootModule):\n#     def __init__(self, cfg, tokenizer):\n#         super().__init__()\n        \n#         self.cfg = cfg\n#         self.tokenizer = tokenizer\n        \n#         if self.cfg['factored_embed']:\n#             self.embed = FactoredEmbed(self.cfg)\n#         else:\n#             self.embed = Embed(self.cfg)\n#         self.hook_embed = HookPoint() # [batch, pos, d_model]\n        \n#         self.pos_embed = PosEmbed(self.cfg)\n#         self.hook_pos_embed = HookPoint() # [batch, pos, d_model]\n\n#         if cfg['normalization']=='RMS':\n#             self.norm = RMSNorm(self.cfg, self.cfg['d_model'])\n#         elif cfg['normalization']=='LN':\n#             self.norm = LayerNorm(self.cfg, self.cfg['d_model'])\n            \n#         self.blocks = nn.ModuleList([TransformerBlock(self.cfg, block_index) for block_index in range(self.cfg['n_layers'])])\n\n#         if self.cfg['factored_embed']:\n#             self.unembed = FactoredUnembed(self.cfg)\n#         else:\n#             self.unembed = Unembed(self.cfg)\n\n#         # Gives each module a parameter with its name (relative to this root module)\n#         # Needed for HookPoints to work\n#         self.setup_hooks()\n            \n#     def forward(self, tokens, return_loss=True):\n#         # Input x is either a batch of tokens ([batch, pos]) or a text string\n#         # if type(x)==str:\n#         #     # If text, convert to tokens (batch_size=1)\n#         #     x = self.to_tokens(x)\n#         embed = self.hook_embed(self.embed(tokens)) # [batch, pos, d_model]\n#         pos_embed = self.hook_pos_embed(self.pos_embed(tokens)) # [batch, pos, d_model]\n#         if cfg['use_pos_resid']:\n#             residual = embed + pos_embed # [batch, pos, d_model]\n#         else:\n#             residual = embed # [batch, pos, d_model]\n#         for block in self.blocks:\n#             # Note that each block includes skip connections, so we don't need\n#             # residual + block(residual)\n#             residual = block(residual, pos_embed) # [batch, pos, d_model]\n#         if self.cfg['normalization'] is not None:\n#             residual = self.norm(residual)\n#         logits = self.unembed(residual) # [batch, pos, d_vocab]\n#         if return_loss:\n#             return loss_fn(logits, tokens)\n#         else:\n#             return logits\n    \n#     def to_tokens(self, text):\n#         return self.tokenizer(text, return_tensors='pt')['input_ids']\n\n\n# model_new = NewTransformer(cfg, tokenizer)\n# model_new.to('cuda:0')\n\n\n# model_old.to('cuda:7').to(torch.float32)\n# model_new.to('cuda:7').to(torch.float32)\n# tokens = torch.randint(100, 1000, (10, 1024)).to('cuda:7')\n# s = time.time()\n# loss = model_old(tokens)\n# print(loss.item())\n# print(time.time() - s)\n# s = time.time()\n# loss = model_new(tokens)\n# print(loss.item())\n# print(time.time() - s)\n# %timeit model_old(tokens)\n# %timeit model_new(tokens)\n\n\n# %timeit model_old(tokens)\n# %timeit model_new(tokens)\n\n\n# model_old.to('cuda:5').to(torch.float16)\n# model_new.to('cuda:5').to(torch.float16)\n# tokens = torch.randint(100, 1000, (10, 1024)).to('cuda:5')\n# s = time.time()\n# loss = model_old(tokens)\n# print(loss.item())\n# print(time.time() - s)\n# s = time.time()\n# loss = model_new(tokens)\n# print(loss.item())\n# print(time.time() - s)\n# %timeit model_old(tokens)\n# %timeit model_new(tokens)\n\n\n# model_old.to('cuda:7')\n# model_new_2.to('cuda:7')\n# tokens = torch.randint(100, 1000, (10, 1024)).to('cuda:7')\n# s = time.time()\n# loss = model_old(tokens)\n# print(loss.item())\n# print(time.time() - s)\n# s = time.time()\n# loss = model_new_2(tokens)\n# print(loss.item())\n# print(time.time() - s)\n# %timeit model_old(tokens)\n# %timeit model_new_2(tokens)\n\n\nif False:\n    # Debugging code:\n    lis = []\n    for i in range(cfg['max_steps']):\n        if schedule.step():\n            lis.append(i)\n    px.line(lis, log_y=True).show()"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  }
]